{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b9d96c2",
   "metadata": {},
   "source": [
    "# Implementation of Multilayer Perceptrons\n",
    ":label:`sec_mlp-implementation`\n",
    "\n",
    "Multilayer perceptrons (MLPs) are not much more complex to implement than simple linear models. The key conceptual\n",
    "difference is that we now concatenate multiple layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81f206bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Flux,Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d0dd6f",
   "metadata": {},
   "source": [
    "## Implementation from Scratch\n",
    "\n",
    "Let's begin again by implementing such a network from scratch.\n",
    "\n",
    "### Initializing Model Parameters\n",
    "\n",
    "Recall that Fashion-MNIST contains 10 classes,\n",
    "and that each image consists of a $28 \\times 28 = 784$\n",
    "grid of grayscale pixel values.\n",
    "As before we will disregard the spatial structure\n",
    "among the pixels for now,\n",
    "so we can think of this as a classification dataset\n",
    "with 784 input features and 10 classes.\n",
    "To begin, we will [**implement an MLP\n",
    "with one hidden layer and 256 hidden units.**]\n",
    "Both the number of layers and their width are adjustable\n",
    "(they are considered hyperparameters).\n",
    "Typically, we choose the layer widths to be divisible by larger powers of 2.\n",
    "This is computationally efficient due to the way\n",
    "memory is allocated and addressed in hardware.\n",
    "\n",
    "Again, we will represent our parameters with several tensors.\n",
    "Note that *for every layer*, we must keep track of\n",
    "one weight matrix and one bias vector.\n",
    "As always, we allocate memory\n",
    "for the gradients of the loss with respect to these parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d595e12",
   "metadata": {},
   "source": [
    "In the code below we use [`nn.Parameter`](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html)\n",
    "to automatically register\n",
    "a class attribute as a parameter to be tracked by `autograd` (:numref:`sec_autograd`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e067075f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Dense(784 => 40, relu),               \u001b[90m# 31_400 parameters\u001b[39m\n",
       "  Dense(40 => 10),                      \u001b[90m# 410 parameters\u001b[39m\n",
       "  NNlib.softmax,\n",
       ") \u001b[90m                  # Total: 4 arrays, \u001b[39m31_810 parameters, 124.508 KiB."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Flux\n",
    "model = Chain(\n",
    "  Dense(28*28,40, relu),\n",
    "  Dense(40, 10),\n",
    "  softmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8535f083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15-element Vector{Float64}:\n",
       " 5.978637810056745e-13\n",
       " 4.417649017369718e-12\n",
       " 3.264225641473071e-11\n",
       " 2.411954638441241e-10\n",
       " 1.7822068131518322e-9\n",
       " 1.3168826122275303e-8\n",
       " 9.730519497455558e-8\n",
       " 7.18993544384376e-7\n",
       " 5.312683634225139e-6\n",
       " 3.925571740916031e-5\n",
       " 0.0002900626981400541\n",
       " 0.0021432895487640473\n",
       " 0.015836886712069304\n",
       " 0.11701964434788946\n",
       " 0.8646647167634682"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(range(-14,14,length=15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1c6fa6",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2dfda5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: MNIST.traindata() is deprecated, use `MNIST(split=:train)[:]` instead.\n",
      "└ @ MLDatasets C:\\Users\\Szasz\\.julia\\packages\\MLDatasets\\bg0uc\\src\\datasets\\vision\\mnist.jl:187\n",
      "┌ Warning: MNIST.testdata() is deprecated, use `MNIST(split=:test)[:]` instead.\n",
      "└ @ MLDatasets C:\\Users\\Szasz\\.julia\\packages\\MLDatasets\\bg0uc\\src\\datasets\\vision\\mnist.jl:195\n"
     ]
    }
   ],
   "source": [
    "using MLDatasets\n",
    "\n",
    "# load training set\n",
    "train_x, train_y = MNIST.traindata();\n",
    "\n",
    "# load test set\n",
    "test_x,  test_y  = MNIST.testdata();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b7b0c1",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "- convert train data from grayscale to float <br>\n",
    "- use a function from Flux to one-hot encode our labels <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3af6c3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = Float32.(train_x)\n",
    "y_train = Flux.onehotbatch(train_y, 0:9);\n",
    "\n",
    "x_test = Float32.(test_x)\n",
    "y_test = Flux.onehotbatch(test_y, 0:9);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed644ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×10000 OneHotMatrix(::Vector{UInt32}) with eltype Bool:\n",
       " ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  …  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  1  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  1  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  …  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅     ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1\n",
       " 1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  1  ⋅  ⋅  1     ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a36e01",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Fortunately, [**the training loop for MLPs\n",
    "is exactly the same as for softmax regression.**] We define the model, data, trainer and finally invoke the `fit` method on model and data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed2fabd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam(0.001, (0.9, 0.999), 1.0e-8, IdDict{Any, Any}())"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(X, y) = Flux.crossentropy(model(X), y)\n",
    "\n",
    "#adaptative moment estimation  (combines loss function and parameters)\n",
    "opt = Flux.ADAM()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff77f89",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e094a94c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "calculate_accuracy_test (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#with this function we'll calculate the accuracy in every epoch\n",
    "using Statistics\n",
    "\n",
    "function calculate_accuracy_test()\n",
    "    test_data = [(Flux.flatten(x_test), test_y)]\n",
    "    accuracy = 0\n",
    "    err=0;\n",
    "    nr_of_errors=0\n",
    "    for i in 1:length(test_y)\n",
    "        if findmax(model(test_data[1][1][:, i]))[2] - 1  == test_y[i]\n",
    "            accuracy = accuracy + 1\n",
    "        else\n",
    "            nr_of_errors=nr_of_errors+1\n",
    "            A=abs.(model(test_data[1][1][:, i]) .- y_test[:,i])\n",
    "            aux=0\n",
    "            for a in A\n",
    "                aux=aux+a\n",
    "            end\n",
    "            err=err+aux;\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    m = accuracy / length(test_y)\n",
    "    n = err/ length(test_y)\n",
    "    s = nr_of_errors \n",
    "    println(\"The val_accuracy is: \",m)\n",
    "    println(\"Mean absolute error (TEST): \",n)\n",
    "    println(\"Number of errors: \",s)\n",
    "    return m,n,s\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "456c7382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "calculate_accuracy_train (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#with this function we'll calculate the accuracy in every epoch\n",
    "using Statistics\n",
    "\n",
    "function calculate_accuracy_train()\n",
    "    train_data = [(Flux.flatten(x_train), train_y)]\n",
    "    err=0;\n",
    "    for i in 1:length(train_y)\n",
    "        if findmax(model(train_data[1][1][:, i]))[2] - 1  != train_y[i]\n",
    "            A=abs.(model(train_data[1][1][:, i]) .- y_train[:,i])\n",
    "            aux=0\n",
    "            for a in A\n",
    "                aux=aux+a\n",
    "            end\n",
    "            err=err+aux;\n",
    "        end\n",
    "    end    \n",
    "    n = err/ length(train_y)\n",
    "    println(\"Mean absolute error (TRAIN): \",n)\n",
    "    return n\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f93b1c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "Mean absolute error (TRAIN): 1.4910815\n",
      "The val_accuracy is: 0.1772\n",
      "Mean absolute error (TEST): 1.4757332\n",
      "Number of errors: 8228\n",
      "epoch 2\n",
      "Mean absolute error (TRAIN): 1.3756839\n",
      "The val_accuracy is: 0.231\n",
      "Mean absolute error (TEST): 1.372346\n",
      "Number of errors: 7690\n",
      "epoch 3\n",
      "Mean absolute error (TRAIN): 1.2747517\n",
      "The val_accuracy is: 0.292\n",
      "Mean absolute error (TEST): 1.2582064\n",
      "Number of errors: 7080\n",
      "epoch 4\n",
      "Mean absolute error (TRAIN): 1.1764238\n",
      "The val_accuracy is: 0.3436\n",
      "Mean absolute error (TEST): 1.1610466\n",
      "Number of errors: 6564\n",
      "epoch 5\n",
      "Mean absolute error (TRAIN): 1.0804728\n",
      "The val_accuracy is: 0.3997\n",
      "Mean absolute error (TEST): 1.0570196\n",
      "Number of errors: 6003\n",
      "epoch 6\n",
      "Mean absolute error (TRAIN): 0.9901274\n",
      "The val_accuracy is: 0.4497\n",
      "Mean absolute error (TEST): 0.9644249\n",
      "Number of errors: 5503\n",
      "epoch 7\n",
      "Mean absolute error (TRAIN): 0.9072635\n",
      "The val_accuracy is: 0.4933\n",
      "Mean absolute error (TEST): 0.8838929\n",
      "Number of errors: 5067\n",
      "epoch 8\n",
      "Mean absolute error (TRAIN): 0.8363687\n",
      "The val_accuracy is: 0.5322\n",
      "Mean absolute error (TEST): 0.81235534\n",
      "Number of errors: 4678\n"
     ]
    }
   ],
   "source": [
    "using Flux\n",
    "\n",
    "parameters = Flux.params(model)\n",
    "# flatten() function converts array 28x28x60000 into 784x60000 \n",
    "train_data = [(Flux.flatten(x_train), Flux.flatten(y_train))]\n",
    "\n",
    "val_acc=zeros(0)\n",
    "mean_err_test=zeros(0)\n",
    "nr_err=zeros(0)\n",
    "mean_err_train=zeros(0)\n",
    "nr_of_epoch=8\n",
    "\n",
    "for i in 1:nr_of_epoch\n",
    "    println(\"epoch \",i)\n",
    "    Flux.train!(loss, parameters, train_data, opt)\n",
    "    \n",
    "    n = calculate_accuracy_train()\n",
    "    append!(mean_err_train,n)\n",
    "    m, n, s = calculate_accuracy_test()\n",
    "    append!( val_acc, m )\n",
    "    append!( mean_err_test, n )\n",
    "    append!( nr_err, s )\n",
    "    \n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31e9dca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"500\" height=\"250\" viewBox=\"0 0 2000 1000\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip280\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2000\" height=\"1000\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip280)\" d=\"\n",
       "M0 1000 L2000 1000 L2000 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip281\">\n",
       "    <rect x=\"400\" y=\"0\" width=\"1401\" height=\"1000\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip280)\" d=\"\n",
       "M152.598 901.088 L1952.76 901.088 L1952.76 47.2441 L152.598 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip282\">\n",
       "    <rect x=\"152\" y=\"47\" width=\"1801\" height=\"855\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip282)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  446.155,901.088 446.155,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip282)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  931.373,901.088 931.373,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip282)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1416.59,901.088 1416.59,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip282)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1901.81,901.088 1901.81,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip280)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  152.598,901.088 1952.76,901.088 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip280)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  446.155,901.088 446.155,882.19 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip280)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  931.373,901.088 931.373,882.19 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip280)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1416.59,901.088 1416.59,882.19 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip280)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1901.81,901.088 1901.81,882.19 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip280)\" d=\"M440.808 956.353 L457.127 956.353 L457.127 960.288 L435.183 960.288 L435.183 956.353 Q437.845 953.598 442.428 948.968 Q447.035 944.316 448.215 942.973 Q450.46 940.45 451.34 938.714 Q452.243 936.955 452.243 935.265 Q452.243 932.51 450.298 930.774 Q448.377 929.038 445.275 929.038 Q443.076 929.038 440.623 929.802 Q438.192 930.566 435.414 932.117 L435.414 927.395 Q438.238 926.26 440.692 925.682 Q443.146 925.103 445.183 925.103 Q450.553 925.103 453.747 927.788 Q456.942 930.473 456.942 934.964 Q456.942 937.094 456.132 939.015 Q455.345 940.913 453.238 943.506 Q452.66 944.177 449.558 947.394 Q446.456 950.589 440.808 956.353 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M934.382 929.802 L922.576 948.251 L934.382 948.251 L934.382 929.802 M933.155 925.728 L939.035 925.728 L939.035 948.251 L943.965 948.251 L943.965 952.14 L939.035 952.14 L939.035 960.288 L934.382 960.288 L934.382 952.14 L918.78 952.14 L918.78 947.626 L933.155 925.728 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M1417 941.144 Q1413.85 941.144 1412 943.297 Q1410.17 945.45 1410.17 949.2 Q1410.17 952.927 1412 955.103 Q1413.85 957.255 1417 957.255 Q1420.14 957.255 1421.97 955.103 Q1423.82 952.927 1423.82 949.2 Q1423.82 945.45 1421.97 943.297 Q1420.14 941.144 1417 941.144 M1426.28 926.492 L1426.28 930.751 Q1424.52 929.918 1422.71 929.478 Q1420.93 929.038 1419.17 929.038 Q1414.54 929.038 1412.09 932.163 Q1409.66 935.288 1409.31 941.607 Q1410.68 939.594 1412.74 938.529 Q1414.8 937.441 1417.27 937.441 Q1422.48 937.441 1425.49 940.612 Q1428.52 943.76 1428.52 949.2 Q1428.52 954.524 1425.38 957.742 Q1422.23 960.959 1417 960.959 Q1411 960.959 1407.83 956.376 Q1404.66 951.769 1404.66 943.043 Q1404.66 934.848 1408.55 929.987 Q1412.44 925.103 1418.99 925.103 Q1420.75 925.103 1422.53 925.45 Q1424.33 925.797 1426.28 926.492 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M1901.81 943.876 Q1898.47 943.876 1896.55 945.658 Q1894.66 947.441 1894.66 950.566 Q1894.66 953.691 1896.55 955.473 Q1898.47 957.255 1901.81 957.255 Q1905.14 957.255 1907.06 955.473 Q1908.98 953.668 1908.98 950.566 Q1908.98 947.441 1907.06 945.658 Q1905.16 943.876 1901.81 943.876 M1897.13 941.885 Q1894.12 941.144 1892.43 939.084 Q1890.77 937.024 1890.77 934.061 Q1890.77 929.918 1893.71 927.51 Q1896.67 925.103 1901.81 925.103 Q1906.97 925.103 1909.91 927.51 Q1912.85 929.918 1912.85 934.061 Q1912.85 937.024 1911.16 939.084 Q1909.49 941.144 1906.51 941.885 Q1909.89 942.672 1911.76 944.964 Q1913.66 947.256 1913.66 950.566 Q1913.66 955.589 1910.58 958.274 Q1907.53 960.959 1901.81 960.959 Q1896.09 960.959 1893.01 958.274 Q1889.96 955.589 1889.96 950.566 Q1889.96 947.256 1891.85 944.964 Q1893.75 942.672 1897.13 941.885 M1895.42 934.501 Q1895.42 937.186 1897.09 938.691 Q1898.78 940.195 1901.81 940.195 Q1904.82 940.195 1906.51 938.691 Q1908.22 937.186 1908.22 934.501 Q1908.22 931.816 1906.51 930.311 Q1904.82 928.807 1901.81 928.807 Q1898.78 928.807 1897.09 930.311 Q1895.42 931.816 1895.42 934.501 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip282)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  152.598,862.944 1952.76,862.944 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip282)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  152.598,740.329 1952.76,740.329 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip282)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  152.598,617.713 1952.76,617.713 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip282)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  152.598,495.097 1952.76,495.097 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip282)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  152.598,372.481 1952.76,372.481 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip282)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  152.598,249.865 1952.76,249.865 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip282)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  152.598,127.25 1952.76,127.25 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip280)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  152.598,901.088 152.598,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip280)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  152.598,862.944 171.496,862.944 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip280)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  152.598,740.329 171.496,740.329 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip280)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  152.598,617.713 171.496,617.713 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip280)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  152.598,495.097 171.496,495.097 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip280)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  152.598,372.481 171.496,372.481 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip280)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  152.598,249.865 171.496,249.865 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip280)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  152.598,127.25 171.496,127.25 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip280)\" d=\"M67.0198 848.743 Q63.4087 848.743 61.58 852.308 Q59.7745 855.849 59.7745 862.979 Q59.7745 870.085 61.58 873.65 Q63.4087 877.192 67.0198 877.192 Q70.6541 877.192 72.4596 873.65 Q74.2883 870.085 74.2883 862.979 Q74.2883 855.849 72.4596 852.308 Q70.6541 848.743 67.0198 848.743 M67.0198 845.039 Q72.83 845.039 75.8855 849.646 Q78.9642 854.229 78.9642 862.979 Q78.9642 871.706 75.8855 876.312 Q72.83 880.896 67.0198 880.896 Q61.2097 880.896 58.131 876.312 Q55.0754 871.706 55.0754 862.979 Q55.0754 854.229 58.131 849.646 Q61.2097 845.039 67.0198 845.039 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M87.1818 874.345 L92.066 874.345 L92.066 880.224 L87.1818 880.224 L87.1818 874.345 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M106.279 876.289 L122.598 876.289 L122.598 880.224 L100.654 880.224 L100.654 876.289 Q103.316 873.535 107.899 868.905 Q112.506 864.252 113.686 862.91 Q115.932 860.386 116.811 858.65 Q117.714 856.891 117.714 855.201 Q117.714 852.447 115.77 850.711 Q113.848 848.974 110.746 848.974 Q108.547 848.974 106.094 849.738 Q103.663 850.502 100.885 852.053 L100.885 847.331 Q103.709 846.197 106.163 845.618 Q108.617 845.039 110.654 845.039 Q116.024 845.039 119.219 847.724 Q122.413 850.41 122.413 854.9 Q122.413 857.03 121.603 858.951 Q120.816 860.849 118.709 863.442 Q118.131 864.113 115.029 867.331 Q111.927 870.525 106.279 876.289 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M64.9365 726.127 Q61.3254 726.127 59.4967 729.692 Q57.6912 733.234 57.6912 740.363 Q57.6912 747.47 59.4967 751.034 Q61.3254 754.576 64.9365 754.576 Q68.5707 754.576 70.3763 751.034 Q72.205 747.47 72.205 740.363 Q72.205 733.234 70.3763 729.692 Q68.5707 726.127 64.9365 726.127 M64.9365 722.424 Q70.7467 722.424 73.8022 727.03 Q76.8809 731.613 76.8809 740.363 Q76.8809 749.09 73.8022 753.696 Q70.7467 758.28 64.9365 758.28 Q59.1264 758.28 56.0477 753.696 Q52.9921 749.09 52.9921 740.363 Q52.9921 731.613 56.0477 727.03 Q59.1264 722.424 64.9365 722.424 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M85.0984 751.729 L89.9827 751.729 L89.9827 757.609 L85.0984 757.609 L85.0984 751.729 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M113.015 727.123 L101.209 745.572 L113.015 745.572 L113.015 727.123 M111.788 723.049 L117.668 723.049 L117.668 745.572 L122.598 745.572 L122.598 749.46 L117.668 749.46 L117.668 757.609 L113.015 757.609 L113.015 749.46 L97.4132 749.46 L97.4132 744.947 L111.788 723.049 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M65.2606 603.511 Q61.6495 603.511 59.8208 607.076 Q58.0152 610.618 58.0152 617.747 Q58.0152 624.854 59.8208 628.419 Q61.6495 631.96 65.2606 631.96 Q68.8948 631.96 70.7004 628.419 Q72.5291 624.854 72.5291 617.747 Q72.5291 610.618 70.7004 607.076 Q68.8948 603.511 65.2606 603.511 M65.2606 599.808 Q71.0707 599.808 74.1263 604.414 Q77.205 608.997 77.205 617.747 Q77.205 626.474 74.1263 631.081 Q71.0707 635.664 65.2606 635.664 Q59.4504 635.664 56.3717 631.081 Q53.3162 626.474 53.3162 617.747 Q53.3162 608.997 56.3717 604.414 Q59.4504 599.808 65.2606 599.808 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M85.4225 629.113 L90.3067 629.113 L90.3067 634.993 L85.4225 634.993 L85.4225 629.113 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M111.071 615.849 Q107.922 615.849 106.071 618.002 Q104.242 620.155 104.242 623.905 Q104.242 627.632 106.071 629.808 Q107.922 631.96 111.071 631.96 Q114.219 631.96 116.047 629.808 Q117.899 627.632 117.899 623.905 Q117.899 620.155 116.047 618.002 Q114.219 615.849 111.071 615.849 M120.353 601.197 L120.353 605.456 Q118.594 604.623 116.788 604.183 Q115.006 603.743 113.246 603.743 Q108.617 603.743 106.163 606.868 Q103.733 609.993 103.385 616.312 Q104.751 614.298 106.811 613.234 Q108.871 612.146 111.348 612.146 Q116.557 612.146 119.566 615.317 Q122.598 618.465 122.598 623.905 Q122.598 629.229 119.45 632.446 Q116.302 635.664 111.071 635.664 Q105.075 635.664 101.904 631.081 Q98.7326 626.474 98.7326 617.747 Q98.7326 609.553 102.621 604.692 Q106.51 599.808 113.061 599.808 Q114.82 599.808 116.603 600.155 Q118.408 600.502 120.353 601.197 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M65.5152 480.896 Q61.9041 480.896 60.0754 484.46 Q58.2699 488.002 58.2699 495.132 Q58.2699 502.238 60.0754 505.803 Q61.9041 509.345 65.5152 509.345 Q69.1494 509.345 70.955 505.803 Q72.7837 502.238 72.7837 495.132 Q72.7837 488.002 70.955 484.46 Q69.1494 480.896 65.5152 480.896 M65.5152 477.192 Q71.3254 477.192 74.3809 481.798 Q77.4596 486.382 77.4596 495.132 Q77.4596 503.858 74.3809 508.465 Q71.3254 513.048 65.5152 513.048 Q59.7051 513.048 56.6264 508.465 Q53.5708 503.858 53.5708 495.132 Q53.5708 486.382 56.6264 481.798 Q59.7051 477.192 65.5152 477.192 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M85.6771 506.497 L90.5614 506.497 L90.5614 512.377 L85.6771 512.377 L85.6771 506.497 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M110.746 495.965 Q107.413 495.965 105.492 497.747 Q103.594 499.53 103.594 502.655 Q103.594 505.78 105.492 507.562 Q107.413 509.345 110.746 509.345 Q114.08 509.345 116.001 507.562 Q117.922 505.757 117.922 502.655 Q117.922 499.53 116.001 497.747 Q114.103 495.965 110.746 495.965 M106.071 493.974 Q103.061 493.234 101.371 491.173 Q99.7048 489.113 99.7048 486.15 Q99.7048 482.007 102.645 479.599 Q105.608 477.192 110.746 477.192 Q115.908 477.192 118.848 479.599 Q121.788 482.007 121.788 486.15 Q121.788 489.113 120.098 491.173 Q118.432 493.234 115.445 493.974 Q118.825 494.761 120.7 497.053 Q122.598 499.345 122.598 502.655 Q122.598 507.678 119.52 510.363 Q116.464 513.048 110.746 513.048 Q105.029 513.048 101.95 510.363 Q98.8947 507.678 98.8947 502.655 Q98.8947 499.345 100.793 497.053 Q102.691 494.761 106.071 493.974 M104.358 486.59 Q104.358 489.275 106.024 490.78 Q107.714 492.284 110.746 492.284 Q113.756 492.284 115.445 490.78 Q117.158 489.275 117.158 486.59 Q117.158 483.905 115.445 482.4 Q113.756 480.896 110.746 480.896 Q107.714 480.896 106.024 482.4 Q104.358 483.905 104.358 486.59 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M56.2328 385.826 L63.8717 385.826 L63.8717 359.46 L55.5616 361.127 L55.5616 356.868 L63.8254 355.201 L68.5013 355.201 L68.5013 385.826 L76.1402 385.826 L76.1402 389.761 L56.2328 389.761 L56.2328 385.826 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M85.5845 383.882 L90.4688 383.882 L90.4688 389.761 L85.5845 389.761 L85.5845 383.882 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M110.654 358.28 Q107.043 358.28 105.214 361.845 Q103.409 365.386 103.409 372.516 Q103.409 379.622 105.214 383.187 Q107.043 386.729 110.654 386.729 Q114.288 386.729 116.094 383.187 Q117.922 379.622 117.922 372.516 Q117.922 365.386 116.094 361.845 Q114.288 358.28 110.654 358.28 M110.654 354.576 Q116.464 354.576 119.52 359.183 Q122.598 363.766 122.598 372.516 Q122.598 381.243 119.52 385.849 Q116.464 390.432 110.654 390.432 Q104.844 390.432 101.765 385.849 Q98.7095 381.243 98.7095 372.516 Q98.7095 363.766 101.765 359.183 Q104.844 354.576 110.654 354.576 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M57.8301 263.21 L65.4689 263.21 L65.4689 236.845 L57.1588 238.511 L57.1588 234.252 L65.4226 232.585 L70.0985 232.585 L70.0985 263.21 L77.7374 263.21 L77.7374 267.145 L57.8301 267.145 L57.8301 263.21 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M87.1818 261.266 L92.066 261.266 L92.066 267.145 L87.1818 267.145 L87.1818 261.266 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M106.279 263.21 L122.598 263.21 L122.598 267.145 L100.654 267.145 L100.654 263.21 Q103.316 260.456 107.899 255.826 Q112.506 251.173 113.686 249.831 Q115.932 247.308 116.811 245.571 Q117.714 243.812 117.714 242.122 Q117.714 239.368 115.77 237.632 Q113.848 235.896 110.746 235.896 Q108.547 235.896 106.094 236.659 Q103.663 237.423 100.885 238.974 L100.885 234.252 Q103.709 233.118 106.163 232.539 Q108.617 231.96 110.654 231.96 Q116.024 231.96 119.219 234.646 Q122.413 237.331 122.413 241.821 Q122.413 243.951 121.603 245.872 Q120.816 247.77 118.709 250.363 Q118.131 251.034 115.029 254.252 Q111.927 257.446 106.279 263.21 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M55.7467 140.594 L63.3856 140.594 L63.3856 114.229 L55.0754 115.896 L55.0754 111.636 L63.3393 109.97 L68.0152 109.97 L68.0152 140.594 L75.654 140.594 L75.654 144.53 L55.7467 144.53 L55.7467 140.594 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M85.0984 138.65 L89.9827 138.65 L89.9827 144.53 L85.0984 144.53 L85.0984 138.65 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M113.015 114.044 L101.209 132.493 L113.015 132.493 L113.015 114.044 M111.788 109.97 L117.668 109.97 L117.668 132.493 L122.598 132.493 L122.598 136.382 L117.668 136.382 L117.668 144.53 L113.015 144.53 L113.015 136.382 L97.4132 136.382 L97.4132 131.868 L111.788 109.97 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip282)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  203.546,876.922 446.155,843.939 688.764,806.541 931.373,774.906 1173.98,740.512 1416.59,709.858 1659.2,683.128 1901.81,659.279 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip282)\" style=\"stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  203.546,80.8192 446.155,144.204 688.764,214.18 931.373,273.747 1173.98,337.524 1416.59,394.292 1659.2,443.664 1901.81,487.522 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip282)\" style=\"stroke:#3da44d; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  203.546,71.4095 446.155,142.157 688.764,204.037 931.373,264.319 1173.98,323.145 1416.59,378.534 1659.2,429.336 1901.81,472.8 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip280)\" d=\"\n",
       "M1341.55 872.626 L1892.75 872.626 L1892.75 665.266 L1341.55 665.266  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip280)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1341.55,872.626 1892.75,872.626 1892.75,665.266 1341.55,665.266 1341.55,872.626 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip280)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1361.56,717.106 1481.57,717.106 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip280)\" d=\"M1501.57 708.461 L1506.08 708.461 L1514.18 730.22 L1522.29 708.461 L1526.8 708.461 L1517.08 734.386 L1511.29 734.386 L1501.57 708.461 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M1544.46 721.354 Q1539.3 721.354 1537.31 722.535 Q1535.32 723.715 1535.32 726.562 Q1535.32 728.831 1536.8 730.173 Q1538.3 731.493 1540.87 731.493 Q1544.42 731.493 1546.55 728.993 Q1548.7 726.47 1548.7 722.303 L1548.7 721.354 L1544.46 721.354 M1552.96 719.595 L1552.96 734.386 L1548.7 734.386 L1548.7 730.451 Q1547.24 732.812 1545.06 733.947 Q1542.89 735.058 1539.74 735.058 Q1535.76 735.058 1533.4 732.835 Q1531.06 730.59 1531.06 726.84 Q1531.06 722.465 1533.98 720.243 Q1536.92 718.021 1542.73 718.021 L1548.7 718.021 L1548.7 717.604 Q1548.7 714.664 1546.75 713.067 Q1544.83 711.447 1541.34 711.447 Q1539.11 711.447 1537.01 711.979 Q1534.9 712.512 1532.96 713.576 L1532.96 709.641 Q1535.3 708.738 1537.49 708.299 Q1539.69 707.836 1541.78 707.836 Q1547.4 707.836 1550.18 710.752 Q1552.96 713.669 1552.96 719.595 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M1561.73 698.368 L1565.99 698.368 L1565.99 734.386 L1561.73 734.386 L1561.73 698.368 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M1594.6 742.257 L1594.6 745.567 L1569.97 745.567 L1569.97 742.257 L1594.6 742.257 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M1610.39 721.354 Q1605.23 721.354 1603.23 722.535 Q1601.24 723.715 1601.24 726.562 Q1601.24 728.831 1602.73 730.173 Q1604.23 731.493 1606.8 731.493 Q1610.34 731.493 1612.47 728.993 Q1614.62 726.47 1614.62 722.303 L1614.62 721.354 L1610.39 721.354 M1618.88 719.595 L1618.88 734.386 L1614.62 734.386 L1614.62 730.451 Q1613.17 732.812 1610.99 733.947 Q1608.81 735.058 1605.67 735.058 Q1601.68 735.058 1599.32 732.835 Q1596.98 730.59 1596.98 726.84 Q1596.98 722.465 1599.9 720.243 Q1602.84 718.021 1608.65 718.021 L1614.62 718.021 L1614.62 717.604 Q1614.62 714.664 1612.68 713.067 Q1610.76 711.447 1607.26 711.447 Q1605.04 711.447 1602.93 711.979 Q1600.83 712.512 1598.88 713.576 L1598.88 709.641 Q1601.22 708.738 1603.42 708.299 Q1605.62 707.836 1607.7 707.836 Q1613.33 707.836 1616.1 710.752 Q1618.88 713.669 1618.88 719.595 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M1646.31 709.456 L1646.31 713.437 Q1644.51 712.442 1642.68 711.956 Q1640.87 711.447 1639.02 711.447 Q1634.88 711.447 1632.59 714.086 Q1630.29 716.701 1630.29 721.447 Q1630.29 726.192 1632.59 728.831 Q1634.88 731.447 1639.02 731.447 Q1640.87 731.447 1642.68 730.961 Q1644.51 730.451 1646.31 729.456 L1646.31 733.391 Q1644.53 734.224 1642.61 734.641 Q1640.71 735.058 1638.56 735.058 Q1632.7 735.058 1629.25 731.377 Q1625.8 727.697 1625.8 721.447 Q1625.8 715.104 1629.28 711.47 Q1632.77 707.836 1638.84 707.836 Q1640.8 707.836 1642.68 708.252 Q1644.55 708.646 1646.31 709.456 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M1672.38 709.456 L1672.38 713.437 Q1670.57 712.442 1668.74 711.956 Q1666.94 711.447 1665.09 711.447 Q1660.94 711.447 1658.65 714.086 Q1656.36 716.701 1656.36 721.447 Q1656.36 726.192 1658.65 728.831 Q1660.94 731.447 1665.09 731.447 Q1666.94 731.447 1668.74 730.961 Q1670.57 730.451 1672.38 729.456 L1672.38 733.391 Q1670.6 734.224 1668.67 734.641 Q1666.78 735.058 1664.62 735.058 Q1658.77 735.058 1655.32 731.377 Q1651.87 727.697 1651.87 721.447 Q1651.87 715.104 1655.34 711.47 Q1658.84 707.836 1664.9 707.836 Q1666.87 707.836 1668.74 708.252 Q1670.62 708.646 1672.38 709.456 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip280)\" style=\"stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1361.56,768.946 1481.57,768.946 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip280)\" d=\"M1521.92 765.277 Q1523.51 762.407 1525.74 761.041 Q1527.96 759.676 1530.97 759.676 Q1535.02 759.676 1537.22 762.523 Q1539.42 765.347 1539.42 770.578 L1539.42 786.226 L1535.13 786.226 L1535.13 770.717 Q1535.13 766.99 1533.81 765.185 Q1532.49 763.379 1529.79 763.379 Q1526.48 763.379 1524.55 765.578 Q1522.63 767.777 1522.63 771.574 L1522.63 786.226 L1518.35 786.226 L1518.35 770.717 Q1518.35 766.967 1517.03 765.185 Q1515.71 763.379 1512.96 763.379 Q1509.69 763.379 1507.77 765.602 Q1505.85 767.801 1505.85 771.574 L1505.85 786.226 L1501.57 786.226 L1501.57 760.301 L1505.85 760.301 L1505.85 764.328 Q1507.31 761.944 1509.35 760.81 Q1511.38 759.676 1514.18 759.676 Q1517.01 759.676 1518.98 761.111 Q1520.97 762.546 1521.92 765.277 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M1570.09 772.199 L1570.09 774.282 L1550.5 774.282 Q1550.78 778.68 1553.14 780.995 Q1555.53 783.287 1559.76 783.287 Q1562.22 783.287 1564.51 782.685 Q1566.82 782.083 1569.09 780.879 L1569.09 784.907 Q1566.8 785.879 1564.39 786.388 Q1561.98 786.898 1559.51 786.898 Q1553.3 786.898 1549.67 783.287 Q1546.06 779.676 1546.06 773.518 Q1546.06 767.152 1549.48 763.426 Q1552.93 759.676 1558.77 759.676 Q1564 759.676 1567.03 763.055 Q1570.09 766.412 1570.09 772.199 M1565.83 770.949 Q1565.78 767.453 1563.86 765.37 Q1561.96 763.287 1558.81 763.287 Q1555.25 763.287 1553.1 765.301 Q1550.97 767.314 1550.64 770.972 L1565.83 770.949 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M1588.86 773.194 Q1583.7 773.194 1581.71 774.375 Q1579.72 775.555 1579.72 778.402 Q1579.72 780.671 1581.2 782.013 Q1582.7 783.333 1585.27 783.333 Q1588.81 783.333 1590.94 780.833 Q1593.1 778.31 1593.1 774.143 L1593.1 773.194 L1588.86 773.194 M1597.35 771.435 L1597.35 786.226 L1593.1 786.226 L1593.1 782.291 Q1591.64 784.652 1589.46 785.787 Q1587.29 786.898 1584.14 786.898 Q1580.16 786.898 1577.79 784.675 Q1575.46 782.43 1575.46 778.68 Q1575.46 774.305 1578.37 772.083 Q1581.31 769.861 1587.12 769.861 L1593.1 769.861 L1593.1 769.444 Q1593.1 766.504 1591.15 764.907 Q1589.23 763.287 1585.73 763.287 Q1583.51 763.287 1581.41 763.819 Q1579.3 764.352 1577.36 765.416 L1577.36 761.481 Q1579.69 760.578 1581.89 760.139 Q1584.09 759.676 1586.17 759.676 Q1591.8 759.676 1594.58 762.592 Q1597.35 765.509 1597.35 771.435 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M1627.68 770.578 L1627.68 786.226 L1623.42 786.226 L1623.42 770.717 Q1623.42 767.037 1621.98 765.208 Q1620.55 763.379 1617.68 763.379 Q1614.23 763.379 1612.24 765.578 Q1610.25 767.777 1610.25 771.574 L1610.25 786.226 L1605.97 786.226 L1605.97 760.301 L1610.25 760.301 L1610.25 764.328 Q1611.78 761.99 1613.84 760.833 Q1615.92 759.676 1618.63 759.676 Q1623.1 759.676 1625.39 762.453 Q1627.68 765.208 1627.68 770.578 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M1655.87 794.097 L1655.87 797.407 L1631.24 797.407 L1631.24 794.097 L1655.87 794.097 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M1682.05 772.199 L1682.05 774.282 L1662.47 774.282 Q1662.75 778.68 1665.11 780.995 Q1667.49 783.287 1671.73 783.287 Q1674.18 783.287 1676.47 782.685 Q1678.79 782.083 1681.06 780.879 L1681.06 784.907 Q1678.77 785.879 1676.36 786.388 Q1673.95 786.898 1671.47 786.898 Q1665.27 786.898 1661.64 783.287 Q1658.03 779.676 1658.03 773.518 Q1658.03 767.152 1661.45 763.426 Q1664.9 759.676 1670.73 759.676 Q1675.97 759.676 1679 763.055 Q1682.05 766.412 1682.05 772.199 M1677.79 770.949 Q1677.75 767.453 1675.83 765.37 Q1673.93 763.287 1670.78 763.287 Q1667.22 763.287 1665.06 765.301 Q1662.93 767.314 1662.61 770.972 L1677.79 770.949 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M1704.07 764.282 Q1703.35 763.865 1702.49 763.68 Q1701.66 763.472 1700.64 763.472 Q1697.03 763.472 1695.09 765.833 Q1693.16 768.171 1693.16 772.569 L1693.16 786.226 L1688.88 786.226 L1688.88 760.301 L1693.16 760.301 L1693.16 764.328 Q1694.51 761.967 1696.66 760.833 Q1698.81 759.676 1701.89 759.676 Q1702.33 759.676 1702.86 759.745 Q1703.4 759.791 1704.04 759.907 L1704.07 764.282 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M1722.72 764.282 Q1722.01 763.865 1721.15 763.68 Q1720.32 763.472 1719.3 763.472 Q1715.69 763.472 1713.74 765.833 Q1711.82 768.171 1711.82 772.569 L1711.82 786.226 L1707.54 786.226 L1707.54 760.301 L1711.82 760.301 L1711.82 764.328 Q1713.16 761.967 1715.32 760.833 Q1717.47 759.676 1720.55 759.676 Q1720.99 759.676 1721.52 759.745 Q1722.05 759.791 1722.7 759.907 L1722.72 764.282 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M1746.89 794.097 L1746.89 797.407 L1722.26 797.407 L1722.26 794.097 L1746.89 794.097 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M1755.11 752.94 L1755.11 760.301 L1763.88 760.301 L1763.88 763.611 L1755.11 763.611 L1755.11 777.685 Q1755.11 780.856 1755.97 781.759 Q1756.84 782.662 1759.51 782.662 L1763.88 782.662 L1763.88 786.226 L1759.51 786.226 Q1754.58 786.226 1752.7 784.398 Q1750.83 782.546 1750.83 777.685 L1750.83 763.611 L1747.7 763.611 L1747.7 760.301 L1750.83 760.301 L1750.83 752.94 L1755.11 752.94 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M1791.66 772.199 L1791.66 774.282 L1772.08 774.282 Q1772.35 778.68 1774.72 780.995 Q1777.1 783.287 1781.34 783.287 Q1783.79 783.287 1786.08 782.685 Q1788.4 782.083 1790.66 780.879 L1790.66 784.907 Q1788.37 785.879 1785.97 786.388 Q1783.56 786.898 1781.08 786.898 Q1774.88 786.898 1771.24 783.287 Q1767.63 779.676 1767.63 773.518 Q1767.63 767.152 1771.06 763.426 Q1774.51 759.676 1780.34 759.676 Q1785.57 759.676 1788.6 763.055 Q1791.66 766.412 1791.66 772.199 M1787.4 770.949 Q1787.35 767.453 1785.43 765.37 Q1783.53 763.287 1780.39 763.287 Q1776.82 763.287 1774.67 765.301 Q1772.54 767.314 1772.22 770.972 L1787.4 770.949 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M1815.18 761.065 L1815.18 765.092 Q1813.37 764.166 1811.43 763.703 Q1809.48 763.24 1807.4 763.24 Q1804.23 763.24 1802.63 764.213 Q1801.06 765.185 1801.06 767.129 Q1801.06 768.611 1802.19 769.467 Q1803.33 770.301 1806.75 771.064 L1808.21 771.389 Q1812.75 772.361 1814.65 774.143 Q1816.57 775.902 1816.57 779.074 Q1816.57 782.685 1813.7 784.791 Q1810.85 786.898 1805.85 786.898 Q1803.77 786.898 1801.5 786.481 Q1799.25 786.088 1796.75 785.277 L1796.75 780.879 Q1799.11 782.106 1801.4 782.731 Q1803.7 783.333 1805.94 783.333 Q1808.95 783.333 1810.57 782.314 Q1812.19 781.273 1812.19 779.398 Q1812.19 777.662 1811.01 776.736 Q1809.85 775.81 1805.9 774.953 L1804.41 774.606 Q1800.46 773.773 1798.7 772.06 Q1796.94 770.324 1796.94 767.314 Q1796.94 763.657 1799.53 761.666 Q1802.12 759.676 1806.89 759.676 Q1809.25 759.676 1811.34 760.023 Q1813.42 760.37 1815.18 761.065 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M1827.56 752.94 L1827.56 760.301 L1836.34 760.301 L1836.34 763.611 L1827.56 763.611 L1827.56 777.685 Q1827.56 780.856 1828.42 781.759 Q1829.3 782.662 1831.96 782.662 L1836.34 782.662 L1836.34 786.226 L1831.96 786.226 Q1827.03 786.226 1825.15 784.398 Q1823.28 782.546 1823.28 777.685 L1823.28 763.611 L1820.15 763.611 L1820.15 760.301 L1823.28 760.301 L1823.28 752.94 L1827.56 752.94 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip280)\" style=\"stroke:#3da44d; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1361.56,820.786 1481.57,820.786 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip280)\" d=\"M1521.92 817.117 Q1523.51 814.247 1525.74 812.881 Q1527.96 811.516 1530.97 811.516 Q1535.02 811.516 1537.22 814.363 Q1539.42 817.187 1539.42 822.418 L1539.42 838.066 L1535.13 838.066 L1535.13 822.557 Q1535.13 818.83 1533.81 817.025 Q1532.49 815.219 1529.79 815.219 Q1526.48 815.219 1524.55 817.418 Q1522.63 819.617 1522.63 823.414 L1522.63 838.066 L1518.35 838.066 L1518.35 822.557 Q1518.35 818.807 1517.03 817.025 Q1515.71 815.219 1512.96 815.219 Q1509.69 815.219 1507.77 817.442 Q1505.85 819.641 1505.85 823.414 L1505.85 838.066 L1501.57 838.066 L1501.57 812.141 L1505.85 812.141 L1505.85 816.168 Q1507.31 813.784 1509.35 812.65 Q1511.38 811.516 1514.18 811.516 Q1517.01 811.516 1518.98 812.951 Q1520.97 814.386 1521.92 817.117 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M1570.09 824.039 L1570.09 826.122 L1550.5 826.122 Q1550.78 830.52 1553.14 832.835 Q1555.53 835.127 1559.76 835.127 Q1562.22 835.127 1564.51 834.525 Q1566.82 833.923 1569.09 832.719 L1569.09 836.747 Q1566.8 837.719 1564.39 838.228 Q1561.98 838.738 1559.51 838.738 Q1553.3 838.738 1549.67 835.127 Q1546.06 831.516 1546.06 825.358 Q1546.06 818.992 1549.48 815.266 Q1552.93 811.516 1558.77 811.516 Q1564 811.516 1567.03 814.895 Q1570.09 818.252 1570.09 824.039 M1565.83 822.789 Q1565.78 819.293 1563.86 817.21 Q1561.96 815.127 1558.81 815.127 Q1555.25 815.127 1553.1 817.141 Q1550.97 819.154 1550.64 822.812 L1565.83 822.789 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M1588.86 825.034 Q1583.7 825.034 1581.71 826.215 Q1579.72 827.395 1579.72 830.242 Q1579.72 832.511 1581.2 833.853 Q1582.7 835.173 1585.27 835.173 Q1588.81 835.173 1590.94 832.673 Q1593.1 830.15 1593.1 825.983 L1593.1 825.034 L1588.86 825.034 M1597.35 823.275 L1597.35 838.066 L1593.1 838.066 L1593.1 834.131 Q1591.64 836.492 1589.46 837.627 Q1587.29 838.738 1584.14 838.738 Q1580.16 838.738 1577.79 836.515 Q1575.46 834.27 1575.46 830.52 Q1575.46 826.145 1578.37 823.923 Q1581.31 821.701 1587.12 821.701 L1593.1 821.701 L1593.1 821.284 Q1593.1 818.344 1591.15 816.747 Q1589.23 815.127 1585.73 815.127 Q1583.51 815.127 1581.41 815.659 Q1579.3 816.192 1577.36 817.256 L1577.36 813.321 Q1579.69 812.418 1581.89 811.979 Q1584.09 811.516 1586.17 811.516 Q1591.8 811.516 1594.58 814.432 Q1597.35 817.349 1597.35 823.275 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M1627.68 822.418 L1627.68 838.066 L1623.42 838.066 L1623.42 822.557 Q1623.42 818.877 1621.98 817.048 Q1620.55 815.219 1617.68 815.219 Q1614.23 815.219 1612.24 817.418 Q1610.25 819.617 1610.25 823.414 L1610.25 838.066 L1605.97 838.066 L1605.97 812.141 L1610.25 812.141 L1610.25 816.168 Q1611.78 813.83 1613.84 812.673 Q1615.92 811.516 1618.63 811.516 Q1623.1 811.516 1625.39 814.293 Q1627.68 817.048 1627.68 822.418 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M1655.87 845.937 L1655.87 849.247 L1631.24 849.247 L1631.24 845.937 L1655.87 845.937 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M1682.05 824.039 L1682.05 826.122 L1662.47 826.122 Q1662.75 830.52 1665.11 832.835 Q1667.49 835.127 1671.73 835.127 Q1674.18 835.127 1676.47 834.525 Q1678.79 833.923 1681.06 832.719 L1681.06 836.747 Q1678.77 837.719 1676.36 838.228 Q1673.95 838.738 1671.47 838.738 Q1665.27 838.738 1661.64 835.127 Q1658.03 831.516 1658.03 825.358 Q1658.03 818.992 1661.45 815.266 Q1664.9 811.516 1670.73 811.516 Q1675.97 811.516 1679 814.895 Q1682.05 818.252 1682.05 824.039 M1677.79 822.789 Q1677.75 819.293 1675.83 817.21 Q1673.93 815.127 1670.78 815.127 Q1667.22 815.127 1665.06 817.141 Q1662.93 819.154 1662.61 822.812 L1677.79 822.789 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M1704.07 816.122 Q1703.35 815.705 1702.49 815.52 Q1701.66 815.312 1700.64 815.312 Q1697.03 815.312 1695.09 817.673 Q1693.16 820.011 1693.16 824.409 L1693.16 838.066 L1688.88 838.066 L1688.88 812.141 L1693.16 812.141 L1693.16 816.168 Q1694.51 813.807 1696.66 812.673 Q1698.81 811.516 1701.89 811.516 Q1702.33 811.516 1702.86 811.585 Q1703.4 811.631 1704.04 811.747 L1704.07 816.122 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M1722.72 816.122 Q1722.01 815.705 1721.15 815.52 Q1720.32 815.312 1719.3 815.312 Q1715.69 815.312 1713.74 817.673 Q1711.82 820.011 1711.82 824.409 L1711.82 838.066 L1707.54 838.066 L1707.54 812.141 L1711.82 812.141 L1711.82 816.168 Q1713.16 813.807 1715.32 812.673 Q1717.47 811.516 1720.55 811.516 Q1720.99 811.516 1721.52 811.585 Q1722.05 811.631 1722.7 811.747 L1722.72 816.122 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M1746.89 845.937 L1746.89 849.247 L1722.26 849.247 L1722.26 845.937 L1746.89 845.937 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M1755.11 804.78 L1755.11 812.141 L1763.88 812.141 L1763.88 815.451 L1755.11 815.451 L1755.11 829.525 Q1755.11 832.696 1755.97 833.599 Q1756.84 834.502 1759.51 834.502 L1763.88 834.502 L1763.88 838.066 L1759.51 838.066 Q1754.58 838.066 1752.7 836.238 Q1750.83 834.386 1750.83 829.525 L1750.83 815.451 L1747.7 815.451 L1747.7 812.141 L1750.83 812.141 L1750.83 804.78 L1755.11 804.78 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M1784.51 816.122 Q1783.79 815.705 1782.93 815.52 Q1782.1 815.312 1781.08 815.312 Q1777.47 815.312 1775.53 817.673 Q1773.6 820.011 1773.6 824.409 L1773.6 838.066 L1769.32 838.066 L1769.32 812.141 L1773.6 812.141 L1773.6 816.168 Q1774.95 813.807 1777.1 812.673 Q1779.25 811.516 1782.33 811.516 Q1782.77 811.516 1783.3 811.585 Q1783.84 811.631 1784.48 811.747 L1784.51 816.122 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M1800.76 825.034 Q1795.59 825.034 1793.6 826.215 Q1791.61 827.395 1791.61 830.242 Q1791.61 832.511 1793.09 833.853 Q1794.6 835.173 1797.17 835.173 Q1800.71 835.173 1802.84 832.673 Q1804.99 830.15 1804.99 825.983 L1804.99 825.034 L1800.76 825.034 M1809.25 823.275 L1809.25 838.066 L1804.99 838.066 L1804.99 834.131 Q1803.53 836.492 1801.36 837.627 Q1799.18 838.738 1796.03 838.738 Q1792.05 838.738 1789.69 836.515 Q1787.35 834.27 1787.35 830.52 Q1787.35 826.145 1790.27 823.923 Q1793.21 821.701 1799.02 821.701 L1804.99 821.701 L1804.99 821.284 Q1804.99 818.344 1803.05 816.747 Q1801.13 815.127 1797.63 815.127 Q1795.41 815.127 1793.3 815.659 Q1791.2 816.192 1789.25 817.256 L1789.25 813.321 Q1791.59 812.418 1793.79 811.979 Q1795.99 811.516 1798.07 811.516 Q1803.7 811.516 1806.47 814.432 Q1809.25 817.349 1809.25 823.275 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M1818.03 812.141 L1822.28 812.141 L1822.28 838.066 L1818.03 838.066 L1818.03 812.141 M1818.03 802.048 L1822.28 802.048 L1822.28 807.442 L1818.03 807.442 L1818.03 802.048 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip280)\" d=\"M1852.75 822.418 L1852.75 838.066 L1848.49 838.066 L1848.49 822.557 Q1848.49 818.877 1847.05 817.048 Q1845.62 815.219 1842.75 815.219 Q1839.3 815.219 1837.31 817.418 Q1835.32 819.617 1835.32 823.414 L1835.32 838.066 L1831.03 838.066 L1831.03 812.141 L1835.32 812.141 L1835.32 816.168 Q1836.84 813.83 1838.9 812.673 Q1840.99 811.516 1843.7 811.516 Q1848.16 811.516 1850.46 814.293 Q1852.75 817.048 1852.75 822.418 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = range(1, nr_of_epoch, step=1)\n",
    "\n",
    "p= Plots.plot(x,val_acc,label=\"val_acc\",legend=:bottomright, size=(500, 250))\n",
    "p= Plots.plot!(p,x,mean_err_test,label=\"mean_err_test\")\n",
    "p= Plots.plot!(p,x,mean_err_train,label=\"mean_err_train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc8f8db",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Now that we have more practice in designing deep networks, the step from a single to multiple layers of deep networks does not pose such a significant challenge any longer. In particular, we can reuse the training algorithm and data loader. Note, though, that implementing MLPs from scratch is nonetheless messy: naming and keeping track of the model parameters makes it difficult to extend models. For instance, imagine wanting to insert another layer between layers 42 and 43. This might now be layer 42b, unless we are willing to perform sequential renaming. Moreover, if we implement the network from scratch, it is much more difficult for the framework to perform meaningful performance optimizations.\n",
    "\n",
    "Nonetheless, you have now reached the state of the art of the late 1980s when fully connected deep networks were the method of choice for neural network modeling. Our next conceptual step will be to consider images. Before we do so, we need to review a number of statistical basics and details on how to compute models efficiently.\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Change the number of hidden units `num_hiddens` and plot how its number affects the accuracy of the model. What is the best value of this hyperparameter?\n",
    "1. Try adding a hidden layer to see how it affects the results.\n",
    "1. Why is it a bad idea to insert a hidden layer with a single neuron? What could go wrong?\n",
    "1. How does changing the learning rate alter your results? With all other parameters fixed, which learning rate gives you the best results? How does this relate to the number of epochs?\n",
    "1. Let's optimize over all hyperparameters jointly, i.e., learning rate, number of epochs, number of hidden layers, and number of hidden units per layer.\n",
    "    1. What is the best result you can get by optimizing over all of them?\n",
    "    1. Why it is much more challenging to deal with multiple hyperparameters?\n",
    "    1. Describe an efficient strategy for optimizing over multiple parameters jointly.\n",
    "1. Compare the speed of the framework and the from-scratch implementation for a challenging problem. How does it change with the complexity of the network?\n",
    "1. Measure the speed of tensor-matrix multiplications for well-aligned and misaligned matrices. For instance, test for matrices with dimension 1024, 1025, 1026, 1028, and 1032.\n",
    "    1. How does this change between GPUs and CPUs?\n",
    "    1. Determine the memory bus width of your CPU and GPU.\n",
    "1. Try out different activation functions. Which one works best?\n",
    "1. Is there a difference between weight initializations of the network? Does it matter?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
