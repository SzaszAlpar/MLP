{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3007afe8",
   "metadata": {},
   "source": [
    "# Predicting House Prices on Kaggle\n",
    ":label:`sec_kaggle_house`\n",
    "\n",
    "Now that we have introduced some basic tools\n",
    "for building and training deep networks\n",
    "and regularizing them with techniques including\n",
    "weight decay and dropout,\n",
    "we are ready to put all this knowledge into practice\n",
    "by participating in a Kaggle competition.\n",
    "The house price prediction competition\n",
    "is a great place to start.\n",
    "The data is fairly generic and do not exhibit exotic structure\n",
    "that might require specialized models (as audio or video might).\n",
    "This dataset, collected by :citet:`De-Cock.2011`,\n",
    "covers house prices in Ames, IA from the period of 2006--2010.\n",
    "It is considerably larger than the famous [Boston housing dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names) of Harrison and Rubinfeld (1978),\n",
    "boasting both more examples and more features.\n",
    "\n",
    "\n",
    "In this section, we will walk you through details of\n",
    "data preprocessing, model design, and hyperparameter selection.\n",
    "We hope that through a hands-on approach,\n",
    "you will gain some intuitions that will guide you\n",
    "in your career as a data scientist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "eb3ece24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Flux\n",
    "using CSV, DataFrames, Statistics, CategoricalArrays\n",
    "# import torch\n",
    "# from torch import nn\n",
    "# from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb378f1",
   "metadata": {},
   "source": [
    "## Kaggle\n",
    "\n",
    "[Kaggle](https://www.kaggle.com) is a popular platform\n",
    "that hosts machine learning competitions.\n",
    "Each competition centers on a dataset and many\n",
    "are sponsored by stakeholders who offer prizes\n",
    "to the winning solutions.\n",
    "The platform helps users to interact\n",
    "via forums and shared code,\n",
    "fostering both collaboration and competition.\n",
    "While leaderboard chasing often spirals out of control,\n",
    "with researchers focusing myopically on preprocessing steps\n",
    "rather than asking fundamental questions,\n",
    "there is also tremendous value in the objectivity of a platform\n",
    "that facilitates direct quantitative comparisons\n",
    "among competing approaches as well as code sharing\n",
    "so that everyone can learn what did and did not work.\n",
    "If you want to participate in a Kaggle competition,\n",
    "you will first need to register for an account\n",
    "(see :numref:`fig_kaggle`).\n",
    "\n",
    "![The Kaggle website.](../img/kaggle.png)\n",
    ":width:`400px`\n",
    ":label:`fig_kaggle`\n",
    "\n",
    "On the house price prediction competition page, as illustrated\n",
    "in :numref:`fig_house_pricing`,\n",
    "you can find the dataset (under the \"Data\" tab),\n",
    "submit predictions, and see your ranking,\n",
    "The URL is right here:\n",
    "\n",
    "> https://www.kaggle.com/c/house-prices-advanced-regression-techniques\n",
    "\n",
    "![The house price prediction competition page.](../img/house-pricing.png)\n",
    ":width:`400px`\n",
    ":label:`fig_house_pricing`\n",
    "\n",
    "## Accessing and Reading the Dataset\n",
    "\n",
    "Note that the competition data is separated\n",
    "into training and test sets.\n",
    "Each record includes the property value of the house\n",
    "and attributes such as street type, year of construction,\n",
    "roof type, basement condition, etc.\n",
    "The features consist of various data types.\n",
    "For example, the year of construction\n",
    "is represented by an integer,\n",
    "the roof type by discrete categorical assignments,\n",
    "and other features by floating point numbers.\n",
    "And here is where reality complicates things:\n",
    "for some examples, some data is altogether missing\n",
    "with the missing value marked simply as \"na\".\n",
    "The price of each house is included\n",
    "for the training set only\n",
    "(it is a competition after all).\n",
    "We will want to partition the training set\n",
    "to create a validation set,\n",
    "but we only get to evaluate our models on the official test set\n",
    "after uploading predictions to Kaggle.\n",
    "The \"Data\" tab on the competition tab\n",
    "in :numref:`fig_house_pricing`\n",
    "has links to download the data.\n",
    "\n",
    "To get started, we will [**read in and process the data\n",
    "using `pandas`**], which we have introduced in :numref:`sec_pandas`.\n",
    "For convenience, we can download and cache\n",
    "the Kaggle housing dataset.\n",
    "If a file corresponding to this dataset already exists in the cache directory and its SHA-1 matches `sha1_hash`, our code will use the cached file to avoid clogging up your internet with redundant downloads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "id": "803dcbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "using HTTP\n",
    "\n",
    "DATA_URL = \"http://d2l-data.s3-accelerate.amazonaws.com/\"\n",
    "train_filename = \"kaggle_house_pred_train.csv\"\n",
    "test_filename = \"kaggle_house_pred_test.csv\"\n",
    "\n",
    "raw_train = CSV.read(HTTP.get(DATA_URL * train_filename).body, DataFrame)\n",
    "raw_val = CSV.read(HTTP.get(DATA_URL * test_filename).body, DataFrame)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd98d6bf",
   "metadata": {},
   "source": [
    "The training dataset includes 1460 examples,\n",
    "80 features, and 1 label, while the validation data\n",
    "contains 1459 examples and 80 features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 900,
   "id": "efd044a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 81)"
     ]
    }
   ],
   "source": [
    "print(size(raw_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 901,
   "id": "6e27eaf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1459, 80)"
     ]
    }
   ],
   "source": [
    "print(size(raw_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6920f71e",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Let's [**take a look at the first four and last two features\n",
    "as well as the label (SalePrice)**] from the first four examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "id": "f9018763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>4×6 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">Id</th><th style = \"text-align: left;\">MSSubClass</th><th style = \"text-align: left;\">MSZoning</th><th style = \"text-align: left;\">LotFrontage</th><th style = \"text-align: left;\">SaleCondition</th><th style = \"text-align: left;\">SalePrice</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"String7\" style = \"text-align: left;\">String7</th><th title = \"String3\" style = \"text-align: left;\">String3</th><th title = \"String7\" style = \"text-align: left;\">String7</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">60</td><td style = \"text-align: left;\">RL</td><td style = \"text-align: left;\">65</td><td style = \"text-align: left;\">Normal</td><td style = \"text-align: right;\">208500</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">20</td><td style = \"text-align: left;\">RL</td><td style = \"text-align: left;\">80</td><td style = \"text-align: left;\">Normal</td><td style = \"text-align: right;\">181500</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">60</td><td style = \"text-align: left;\">RL</td><td style = \"text-align: left;\">68</td><td style = \"text-align: left;\">Normal</td><td style = \"text-align: right;\">223500</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">70</td><td style = \"text-align: left;\">RL</td><td style = \"text-align: left;\">60</td><td style = \"text-align: left;\">Abnorml</td><td style = \"text-align: right;\">140000</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccc}\n",
       "\t& Id & MSSubClass & MSZoning & LotFrontage & SaleCondition & SalePrice\\\\\n",
       "\t\\hline\n",
       "\t& Int64 & Int64 & String7 & String3 & String7 & Int64\\\\\n",
       "\t\\hline\n",
       "\t1 & 1 & 60 & RL & 65 & Normal & 208500 \\\\\n",
       "\t2 & 2 & 20 & RL & 80 & Normal & 181500 \\\\\n",
       "\t3 & 3 & 60 & RL & 68 & Normal & 223500 \\\\\n",
       "\t4 & 4 & 70 & RL & 60 & Abnorml & 140000 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m4×6 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m Id    \u001b[0m\u001b[1m MSSubClass \u001b[0m\u001b[1m MSZoning \u001b[0m\u001b[1m LotFrontage \u001b[0m\u001b[1m SaleCondition \u001b[0m\u001b[1m SalePrice \u001b[0m\n",
       "     │\u001b[90m Int64 \u001b[0m\u001b[90m Int64      \u001b[0m\u001b[90m String7  \u001b[0m\u001b[90m String3     \u001b[0m\u001b[90m String7       \u001b[0m\u001b[90m Int64     \u001b[0m\n",
       "─────┼────────────────────────────────────────────────────────────────────\n",
       "   1 │     1          60  RL        65           Normal            208500\n",
       "   2 │     2          20  RL        80           Normal            181500\n",
       "   3 │     3          60  RL        68           Normal            223500\n",
       "   4 │     4          70  RL        60           Abnorml           140000"
      ]
     },
     "execution_count": 902,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train[[1, 2, 3, 4], [1, 2, 3, 4, end-1, end]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3360e95",
   "metadata": {},
   "source": [
    "We can see that in each example, the first feature is the ID.\n",
    "This helps the model identify each training example.\n",
    "While this is convenient, it does not carry\n",
    "any information for prediction purposes.\n",
    "Hence, we will remove it from the dataset\n",
    "before feeding the data into the model.\n",
    "Besides, given a wide variety of data types,\n",
    "we will need to preprocess the data before we can start modeling.\n",
    "\n",
    "\n",
    "Let's start with the numerical features.\n",
    "First, we apply a heuristic,\n",
    "[**replacing all missing values\n",
    "by the corresponding feature's mean.**]\n",
    "Then, to put all features on a common scale,\n",
    "we (***standardize* the data by\n",
    "rescaling features to zero mean and unit variance**):\n",
    "\n",
    "$$x \\leftarrow \\frac{x - \\mu}{\\sigma},$$\n",
    "\n",
    "where $\\mu$ and $\\sigma$ denote mean and standard deviation, respectively.\n",
    "To verify that this indeed transforms\n",
    "our feature (variable) such that it has zero mean and unit variance,\n",
    "note that $E[\\frac{x-\\mu}{\\sigma}] = \\frac{\\mu - \\mu}{\\sigma} = 0$\n",
    "and that $E[(x-\\mu)^2] = (\\sigma^2 + \\mu^2) - 2\\mu^2+\\mu^2 = \\sigma^2$.\n",
    "Intuitively, we standardize the data\n",
    "for two reasons.\n",
    "First, it proves convenient for optimization.\n",
    "Second, because we do not know *a priori*\n",
    "which features will be relevant,\n",
    "we do not want to penalize coefficients\n",
    "assigned to one feature more than on any other.\n",
    "\n",
    "[**Next we deal with discrete values.**]\n",
    "This includes features such as \"MSZoning\".\n",
    "(**We replace them by a one-hot encoding**)\n",
    "in the same way that we previously transformed\n",
    "multiclass labels into vectors (see :numref:`subsec_classification-problem`).\n",
    "For instance, \"MSZoning\" assumes the values \"RL\" and \"RM\".\n",
    "Dropping the \"MSZoning\" feature,\n",
    "two new indicator features\n",
    "\"MSZoning_RL\" and \"MSZoning_RM\" are created with values being either 0 or 1.\n",
    "According to one-hot encoding,\n",
    "if the original value of \"MSZoning\" is \"RL\",\n",
    "then \"MSZoning_RL\" is 1 and \"MSZoning_RM\" is 0.\n",
    "The `pandas` package does this automatically for us.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 903,
   "id": "66da71f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "find_categorical_columns (generic function with 1 method)"
      ]
     },
     "execution_count": 903,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function find_categorical_columns(df::DataFrame)\n",
    "    cat_cols = String[]\n",
    "    for col in names(df)\n",
    "        if !(eltype(features[!, col]) === Int64 || eltype(features[!, col]) === Float64)\n",
    "            push!(cat_cols, col)\n",
    "        end\n",
    "    end\n",
    "    return cat_cols\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "id": "11b41e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_onehot_encoding (generic function with 1 method)"
      ]
     },
     "execution_count": 904,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No onehot encoding when using Linear Regression!!!\n",
    "\n",
    "function get_onehot_encoding(df::DataFrame, col::String)\n",
    "    categories = unique(df[!, col])\n",
    "    df_new = df[:, []]\n",
    "    for cat in categories\n",
    "        col_name = col * \"_\" * string(cat)\n",
    "        df_new[!, col_name] = ifelse.(df[!, col] .== cat, 1, 0)\n",
    "    end\n",
    "    return df_new\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "id": "398755fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size(raw_train_preprocess) = (1460, 79)\n",
      "size(raw_test_preprocess) = (1459, 79)\n",
      "size(features) = (2919, 79)\n",
      "size(numeric_features) = (36,)\n",
      "size(features) = (2919, 311)\n"
     ]
    }
   ],
   "source": [
    "# Preprocess\n",
    "\n",
    "# using Flux: onehotbatch\n",
    "\n",
    "# Remove the ID and label columns\n",
    "raw_train_preprocess = select(raw_train, Not(:Id))\n",
    "raw_train_preprocess = select(raw_train_preprocess, Not(:SalePrice))\n",
    "@show size(raw_train_preprocess)\n",
    "    \n",
    "raw_test_preprocess = select(raw_val, Not(:Id))\n",
    "@show size(raw_test_preprocess)\n",
    "            \n",
    "features = vcat(raw_train_preprocess, raw_test_preprocess)\n",
    "@show size(features)\n",
    "    \n",
    "# Type conversion -- the dataframe is python and julia gets different column types\n",
    "\n",
    "for col in names(features)\n",
    "    replace!(features[!, col], \"NA\" => \"NaN\")\n",
    "end\n",
    "\n",
    "features[!,:LotFrontage] = parse.(Float64, features[!,:LotFrontage])\n",
    "features[!, :MasVnrArea] = parse.(Float64, features[!, :MasVnrArea])\n",
    "features[!, :GarageYrBlt] = parse.(Float64, features[!, :GarageYrBlt])\n",
    "features[!, :BsmtFinSF1] = parse.(Float64, string.(features[!, :BsmtFinSF1]))\n",
    "features[!, :BsmtFinSF2] = parse.(Float64, string.(features[!, :BsmtFinSF2]))\n",
    "features[!, :BsmtUnfSF] = parse.(Float64, string.(features[!, :BsmtUnfSF]))\n",
    "features[!, :TotalBsmtSF] = parse.(Float64, string.(features[!, :TotalBsmtSF]))\n",
    "features[!, :BsmtFullBath] = parse.(Float64, string.(features[!, :BsmtFullBath]))\n",
    "features[!, :BsmtHalfBath] = parse.(Float64, string.(features[!, :BsmtHalfBath]))\n",
    "features[!, :GarageCars] = parse.(Float64, string.(features[!, :GarageCars]))\n",
    "features[!, :GarageArea] = parse.(Float64, string.(features[!, :GarageArea]))\n",
    "\n",
    "# Standardize numerical columns\n",
    "numeric_features = names(features[!, [i for i in names(features) if eltype(features[!, i]) === Float64 || eltype(features[!, i]) === Int64]])\n",
    "@show size(numeric_features)\n",
    "\n",
    "features = transform!(features, numeric_features .=> (x -> (x .- mean(x)) ./ std(x)) .=> numeric_features)\n",
    "\n",
    "# Replace NAN numerical features by 0\n",
    "for feature in numeric_features\n",
    "    features[!, feature] = replace(features[!, feature], NaN => 0)\n",
    "end\n",
    "\n",
    "# Replace discrete features by one-hot encoding\n",
    "# get_dummies(features, dummy_na=True) => null values are also getting their columns\n",
    "cat_cols = find_categorical_columns(features)\n",
    "\n",
    "for col in cat_cols\n",
    "    features = hcat(features, get_onehot_encoding(features, col))\n",
    "    features = select!(features, Not(col))\n",
    "end\n",
    "\n",
    "@show size(features)\n",
    "\n",
    "# Save preprocessed features\n",
    "train = features[1:size(raw_train, 1), :]\n",
    "train[!, :SalePrice] = raw_train[!, :SalePrice]\n",
    "val = features[size(raw_train, 1)+1:end, :]\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 922,
   "id": "23225c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 312)"
      ]
     },
     "execution_count": 922,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(train) # should be 331 (some categorical columns are missing -> investigate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718b48bc",
   "metadata": {},
   "source": [
    "You can see that this conversion increases\n",
    "the number of features from 79 to 331 (excluding ID and label columns).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb06822",
   "metadata": {},
   "source": [
    "## Error Measure\n",
    "\n",
    "To get started we will train a linear model with squared loss. Not surprisingly, our linear model will not lead to a competition-winning submission but it provides a sanity check to see whether there is meaningful information in the data. If we cannot do better than random guessing here, then there might be a good chance that we have a data processing bug. And if things work, the linear model will serve as a baseline giving us some intuition about how close the simple model gets to the best reported models, giving us a sense of how much gain we should expect from fancier models.\n",
    "\n",
    "With house prices, as with stock prices,\n",
    "we care about relative quantities\n",
    "more than absolute quantities.\n",
    "Thus [**we tend to care more about\n",
    "the relative error $\\frac{y - \\hat{y}}{y}$**]\n",
    "than about the absolute error $y - \\hat{y}$.\n",
    "For instance, if our prediction is off by USD 100,000\n",
    "when estimating the price of a house in Rural Ohio,\n",
    "where the value of a typical house is 125,000 USD,\n",
    "then we are probably doing a horrible job.\n",
    "On the other hand, if we err by this amount\n",
    "in Los Altos Hills, California,\n",
    "this might represent a stunningly accurate prediction\n",
    "(there, the median house price exceeds 4 million USD).\n",
    "\n",
    "(**One way to address this problem is to\n",
    "measure the discrepancy in the logarithm of the price estimates.**)\n",
    "In fact, this is also the official error measure\n",
    "used by the competition to evaluate the quality of submissions.\n",
    "After all, a small value $\\delta$ for $|\\log y - \\log \\hat{y}| \\leq \\delta$\n",
    "translates into $e^{-\\delta} \\leq \\frac{\\hat{y}}{y} \\leq e^\\delta$.\n",
    "This leads to the following root-mean-squared-error between the logarithm of the predicted price and the logarithm of the label price:\n",
    "\n",
    "$$\\sqrt{\\frac{1}{n}\\sum_{i=1}^n\\left(\\log y_i -\\log \\hat{y}_i\\right)^2}.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834d3df1",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "\n",
    "- TensorLoader in d2l:\n",
    "```\n",
    "@d2l.add_to_class(d2l.DataModule)  #@save\n",
    "def get_tensorloader(self, tensors, train, indices=slice(0, None)):\n",
    "    tensors = tuple(a[indices] for a in tensors)\n",
    "    dataset = torch.utils.data.TensorDataset(*tensors)\n",
    "    return torch.utils.data.DataLoader(dataset, self.batch_size,\n",
    "                                       shuffle=train)\n",
    "```\n",
    "\n",
    "- DataLoader in d2l:\n",
    "```\n",
    "@d2l.add_to_class(SyntheticRegressionData)  #@save\n",
    "def get_dataloader(self, train):\n",
    "    i = slice(0, self.num_train) if train else slice(self.num_train, None)\n",
    "    return self.get_tensorloader((self.X, self.y), train, i)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 923,
   "id": "e0bb83a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_dataloader (generic function with 1 method)"
      ]
     },
     "execution_count": 923,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_dataloader(data, batch_size)\n",
    "    label = \"SalePrice\"\n",
    "\n",
    "    # Define a function to convert a DataFrame to a Float32 tensor\n",
    "    convert(x::DataFrame) = (Matrix(x))\n",
    "    \n",
    "    if !(label in names(data))\n",
    "#         @error \"Label not in dataframe!\"\n",
    "        return Flux.Data.DataLoader(convert(data), batchsize=batch_size, shuffle=true)\n",
    "    end\n",
    "\n",
    "    # Define the input and output tensors\n",
    "    X = convert(data[:, Not(label)])  # all columns except the label\n",
    "    Y = log.(data[:, label])    # logarithm of the label column\n",
    "    \n",
    "    # X has to be reshaped ib order to be able to use the dataloader\n",
    "    X = reshape(X, size(X)[2], size(X)[1])\n",
    "    Y = reshape(Y, 1, :)\n",
    "    \n",
    "    @show size(X)\n",
    "    @show size(Y)\n",
    "\n",
    "    # Create a DataLoader object\n",
    "    return Flux.Data.DataLoader((X, Y), batchsize=batch_size, shuffle=true)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0372927d",
   "metadata": {},
   "source": [
    "### Note: \n",
    "\n",
    "- `Question` about DataLoader: why doesn't it work with X & Y? [link](https://fluxml.ai/Flux.jl/dev/data/mlutils/)\n",
    "- `Answer`: X's dimensions were not right, a reshape was necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 924,
   "id": "55ef4fce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size(X) = (311, 1460)\n",
      "size(Y) = (1, 1460)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23-element DataLoader(::Tuple{Matrix{Float64}, Matrix{Float64}}, shuffle=true, batchsize=64)\n",
       "  with first element:\n",
       "  (311×64 Matrix{Float64}, 1×64 Matrix{Float64},)"
      ]
     },
     "execution_count": 924,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_dataloader(train, 64) # batch size is the size used in tensorflow example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcff303",
   "metadata": {},
   "source": [
    "## $K$-Fold Cross-Validation\n",
    "\n",
    "You might recall that we introduced [**cross-validation**]\n",
    "in :numref:`subsec_generalization-model-selection`, where we discussed how to deal\n",
    "with model selection.\n",
    "We will put this to good use to select the model design\n",
    "and to adjust the hyperparameters.\n",
    "We first need a function that returns\n",
    "the $i^\\mathrm{th}$ fold of the data\n",
    "in a $K$-fold cross-validation procedure.\n",
    "It proceeds by slicing out the $i^\\mathrm{th}$ segment\n",
    "as validation data and returning the rest as training data.\n",
    "Note that this is not the most efficient way of handling data\n",
    "and we would definitely do something much smarter\n",
    "if our dataset was considerably larger.\n",
    "But this added complexity might obfuscate our code unnecessarily\n",
    "so we can safely omit it here owing to the simplicity of our problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b265ede",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "\n",
    "- tensorflow -> contains manual implementation, Julia has built-in k-fold implementation\n",
    "\n",
    "- Options in Julia for K-Fold Cross-Validation:\n",
    "\n",
    "    - ScikitLearn.CrossValidation: cross_val_score: [see here](https://docs.juliahub.com/ScikitLearn/tbUuI/0.6.2/man/cross_validation/)\n",
    "    - MLDataUtils\n",
    "    - MLUtils\n",
    "    \n",
    "- using MLUtils.kfolds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 925,
   "id": "97b39d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size(X) = (311, 1460)\n",
      "size(Y) = (1, 1460)\n"
     ]
    }
   ],
   "source": [
    "using MLUtils\n",
    "\n",
    "# MLUtils.kfolds() usage:\n",
    "train_idx, val_idx = kfolds(get_dataloader(train, 6), 5)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f05b27",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "\n",
    "- LinearRegression in d2l:\n",
    "\n",
    "```\n",
    "class LinearRegression(d2l.Module):  #@save\n",
    "    \"\"\"The linear regression model implemented with high-level APIs.\"\"\"\n",
    "    def __init__(self, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        initializer = tf.initializers.RandomNormal(stddev=0.01)\n",
    "        self.net = tf.keras.layers.Dense(1, kernel_initializer=initializer)\n",
    "```\n",
    "\n",
    "- Loss:\n",
    "```\n",
    "@d2l.add_to_class(LinearRegression)  #@save\n",
    "def loss(self, y_hat, y):\n",
    "    fn = tf.keras.losses.MeanSquaredError()\n",
    "    return fn(y, y_hat)\n",
    "```\n",
    "\n",
    "- Optimizer:\n",
    "```\n",
    "@d2l.add_to_class(LinearRegression)  #@save\n",
    "def configure_optimizers(self):\n",
    "    return tf.keras.optimizers.SGD(self.lr)\n",
    "```\n",
    "\n",
    "- SGD in [documentation](https://fluxml.ai/Flux.jl/v0.4/training/optimisers.html#Optimiser-Reference-1) but it doesn't seem to exist anymore\n",
    "\n",
    "- Trainer:\n",
    "```\n",
    "class Trainer(d2l.HyperParameters):  #@save\n",
    "    \"\"\"The base class for training models with data.\"\"\"\n",
    "    def __init__(self, max_epochs, num_gpus=0, gradient_clip_val=0):\n",
    "        self.save_hyperparameters()\n",
    "        assert num_gpus == 0, 'No GPU support yet'\n",
    "\n",
    "    def prepare_data(self, data):\n",
    "        self.train_dataloader = data.train_dataloader()\n",
    "        self.val_dataloader = data.val_dataloader()\n",
    "        self.num_train_batches = len(self.train_dataloader)\n",
    "        self.num_val_batches = (len(self.val_dataloader)\n",
    "                                if self.val_dataloader is not None else 0)\n",
    "\n",
    "    def prepare_model(self, model):\n",
    "        model.trainer = self\n",
    "        model.board.xlim = [0, self.max_epochs]\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self, model, data):\n",
    "        self.prepare_data(data)\n",
    "        self.prepare_model(model)\n",
    "        self.optim = model.configure_optimizers()\n",
    "        self.epoch = 0\n",
    "        self.train_batch_idx = 0\n",
    "        self.val_batch_idx = 0\n",
    "        for self.epoch in range(self.max_epochs):\n",
    "            self.fit_epoch()\n",
    "\n",
    "    def fit_epoch(self):\n",
    "        self.model.training = True\n",
    "        for batch in self.train_dataloader:\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss = self.model.training_step(self.prepare_batch(batch))\n",
    "            grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "            if self.gradient_clip_val > 0:\n",
    "                grads = self.clip_gradients(self.gradient_clip_val, grads)\n",
    "            self.optim.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "            self.train_batch_idx += 1\n",
    "        if self.val_dataloader is None:\n",
    "            return\n",
    "        self.model.training = False\n",
    "        for batch in self.val_dataloader:\n",
    "            self.model.validation_step(self.prepare_batch(batch))\n",
    "            self.val_batch_idx += 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1b47e5",
   "metadata": {},
   "source": [
    "[**The average validation error is returned**]\n",
    "when we train $K$ times in the $K$-fold cross-validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 926,
   "id": "64868b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 312)"
      ]
     },
     "execution_count": 926,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "id": "260c41f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "k_fold (generic function with 3 methods)"
      ]
     },
     "execution_count": 929,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using MLUtils, Random, Distributions\n",
    "\n",
    "function k_fold(max_epochs, train, k)\n",
    "    val_loss, models = [], []\n",
    "    train_data = get_dataloader(train, 64)\n",
    "    kfolds_train = MLUtils.kfolds(train_data, k)\n",
    "    \n",
    "    model = Chain(\n",
    "        Flux.Dense(311, 64),\n",
    "        Flux.Dense(64, 1)\n",
    "        )\n",
    "    opt = Flux.ADAM() # Better than SGD\n",
    "    loss(X, y) = Flux.mse(model(X), y)\n",
    "    \n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    models = []\n",
    "    \n",
    "\n",
    "    for epoch in 1:max_epochs\n",
    "        i = 0\n",
    "        for (train, test) in kfolds_train\n",
    "            Flux.train!(loss, Flux.params(model), train.data, opt)\n",
    "            i += 1\n",
    "            if (i == k)\n",
    "                train_loss_fold = sum(loss(x, y) for (x, y) in train.data) / length(train.data)\n",
    "                push!(train_loss, train_loss_fold)\n",
    "        \n",
    "                val_loss_fold = sum(loss(x, y) for (x, y) in test.data) / length(test.data)\n",
    "                push!(val_loss, val_loss_fold)\n",
    "                \n",
    "                push!(models, model)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "#     # just for curiosity - trying out the other way around    \n",
    "#     for (train, test) in kfolds_train\n",
    "#         for epoch in 1:max_epochs\n",
    "#             Flux.train!(loss, Flux.params(model), train.data, opt)\n",
    "            \n",
    "#             if (epoch == max_epochs)\n",
    "#                 train_loss_fold = sum(loss(x, y) for (x, y) in train.data) / length(train.data)\n",
    "#                 push!(train_loss, train_loss_fold)\n",
    "        \n",
    "#                 val_loss_fold = sum(loss(x, y) for (x, y) in test.data) / length(test.data)\n",
    "#                 push!(val_loss, val_loss_fold)\n",
    "                \n",
    "#                 push!(models, model)\n",
    "#             end\n",
    "#         end\n",
    "#     end\n",
    "#    # got the same results\n",
    "    \n",
    "    average_val_log_mse = sum(val_loss)/length(val_loss) # average validation log mse\n",
    "    @show average_val_log_mse\n",
    "    \n",
    "    return train_loss, val_loss, models\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80c4bfa",
   "metadata": {},
   "source": [
    "## [**Model Selection**]\n",
    "\n",
    "In this example, we pick an untuned set of hyperparameters\n",
    "and leave it up to the reader to improve the model.\n",
    "Finding a good choice can take time,\n",
    "depending on how many variables one optimizes over.\n",
    "With a large enough dataset,\n",
    "and the normal sorts of hyperparameters,\n",
    "$K$-fold cross-validation tends to be\n",
    "reasonably resilient against multiple testing.\n",
    "However, if we try an unreasonably large number of options\n",
    "we might just get lucky and find that our validation\n",
    "performance is no longer representative of the true error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c7c8ed",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "\n",
    "- Training implementation in python tensorflow: [here](https://d2l.ai/chapter_linear-regression/linear-regression-scratch.html#training)\n",
    "\n",
    "- Implementation:\n",
    "```\n",
    "@d2l.add_to_class(d2l.Trainer)  #@save\n",
    "def fit_epoch(self):\n",
    "    self.model.training = True\n",
    "    for batch in self.train_dataloader:\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.model.training_step(self.prepare_batch(batch))\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        if self.gradient_clip_val > 0:\n",
    "            grads = self.clip_gradients(self.gradient_clip_val, grads)\n",
    "        self.optim.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        self.train_batch_idx += 1\n",
    "    if self.val_dataloader is None:\n",
    "        return\n",
    "    self.model.training = False\n",
    "    for batch in self.val_dataloader:\n",
    "        self.model.validation_step(self.prepare_batch(batch))\n",
    "        self.val_batch_idx += 1\n",
    "```\n",
    "- Usage:\n",
    "```\n",
    "trainer = d2l.Trainer(max_epochs=10)\n",
    "models = k_fold(trainer, train, val, k=5, lr=0.01)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 930,
   "id": "373d6d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size(X) = (311, 1460)\n",
      "size(Y) = (1, 1460)\n",
      "average_val_log_mse = 12.169549943577323\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip340\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip340)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip341\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip340)\" d=\"\n",
       "M158.369 1423.18 L2352.76 1423.18 L2352.76 47.2441 L158.369 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip342\">\n",
       "    <rect x=\"158\" y=\"47\" width=\"2195\" height=\"1377\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  450.494,1423.18 450.494,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  910.533,1423.18 910.533,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1370.57,1423.18 1370.57,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1830.61,1423.18 1830.61,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2290.65,1423.18 2290.65,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip340)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  158.369,1423.18 2352.76,1423.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip340)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  450.494,1423.18 450.494,1404.28 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip340)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  910.533,1423.18 910.533,1404.28 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip340)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1370.57,1423.18 1370.57,1404.28 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip340)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1830.61,1423.18 1830.61,1404.28 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip340)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2290.65,1423.18 2290.65,1404.28 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip340)\" d=\"M445.147 1481.64 L461.466 1481.64 L461.466 1485.58 L439.522 1485.58 L439.522 1481.64 Q442.184 1478.89 446.767 1474.26 Q451.374 1469.61 452.554 1468.27 Q454.799 1465.74 455.679 1464.01 Q456.582 1462.25 456.582 1460.56 Q456.582 1457.8 454.637 1456.07 Q452.716 1454.33 449.614 1454.33 Q447.415 1454.33 444.962 1455.09 Q442.531 1455.86 439.753 1457.41 L439.753 1452.69 Q442.577 1451.55 445.031 1450.97 Q447.485 1450.39 449.522 1450.39 Q454.892 1450.39 458.086 1453.08 Q461.281 1455.77 461.281 1460.26 Q461.281 1462.39 460.471 1464.31 Q459.684 1466.2 457.577 1468.8 Q456.999 1469.47 453.897 1472.69 Q450.795 1475.88 445.147 1481.64 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M913.542 1455.09 L901.737 1473.54 L913.542 1473.54 L913.542 1455.09 M912.316 1451.02 L918.195 1451.02 L918.195 1473.54 L923.126 1473.54 L923.126 1477.43 L918.195 1477.43 L918.195 1485.58 L913.542 1485.58 L913.542 1477.43 L897.941 1477.43 L897.941 1472.92 L912.316 1451.02 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1370.98 1466.44 Q1367.83 1466.44 1365.98 1468.59 Q1364.15 1470.74 1364.15 1474.49 Q1364.15 1478.22 1365.98 1480.39 Q1367.83 1482.55 1370.98 1482.55 Q1374.13 1482.55 1375.95 1480.39 Q1377.81 1478.22 1377.81 1474.49 Q1377.81 1470.74 1375.95 1468.59 Q1374.13 1466.44 1370.98 1466.44 M1380.26 1451.78 L1380.26 1456.04 Q1378.5 1455.21 1376.69 1454.77 Q1374.91 1454.33 1373.15 1454.33 Q1368.52 1454.33 1366.07 1457.45 Q1363.64 1460.58 1363.29 1466.9 Q1364.66 1464.89 1366.72 1463.82 Q1368.78 1462.73 1371.26 1462.73 Q1376.46 1462.73 1379.47 1465.9 Q1382.51 1469.05 1382.51 1474.49 Q1382.51 1479.82 1379.36 1483.03 Q1376.21 1486.25 1370.98 1486.25 Q1364.98 1486.25 1361.81 1481.67 Q1358.64 1477.06 1358.64 1468.33 Q1358.64 1460.14 1362.53 1455.28 Q1366.42 1450.39 1372.97 1450.39 Q1374.73 1450.39 1376.51 1450.74 Q1378.32 1451.09 1380.26 1451.78 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1830.61 1469.17 Q1827.28 1469.17 1825.36 1470.95 Q1823.46 1472.73 1823.46 1475.86 Q1823.46 1478.98 1825.36 1480.77 Q1827.28 1482.55 1830.61 1482.55 Q1833.94 1482.55 1835.87 1480.77 Q1837.79 1478.96 1837.79 1475.86 Q1837.79 1472.73 1835.87 1470.95 Q1833.97 1469.17 1830.61 1469.17 M1825.94 1467.18 Q1822.93 1466.44 1821.24 1464.38 Q1819.57 1462.32 1819.57 1459.35 Q1819.57 1455.21 1822.51 1452.8 Q1825.47 1450.39 1830.61 1450.39 Q1835.77 1450.39 1838.71 1452.8 Q1841.65 1455.21 1841.65 1459.35 Q1841.65 1462.32 1839.96 1464.38 Q1838.3 1466.44 1835.31 1467.18 Q1838.69 1467.96 1840.57 1470.26 Q1842.46 1472.55 1842.46 1475.86 Q1842.46 1480.88 1839.38 1483.57 Q1836.33 1486.25 1830.61 1486.25 Q1824.89 1486.25 1821.82 1483.57 Q1818.76 1480.88 1818.76 1475.86 Q1818.76 1472.55 1820.66 1470.26 Q1822.56 1467.96 1825.94 1467.18 M1824.22 1459.79 Q1824.22 1462.48 1825.89 1463.98 Q1827.58 1465.49 1830.61 1465.49 Q1833.62 1465.49 1835.31 1463.98 Q1837.02 1462.48 1837.02 1459.79 Q1837.02 1457.11 1835.31 1455.6 Q1833.62 1454.1 1830.61 1454.1 Q1827.58 1454.1 1825.89 1455.6 Q1824.22 1457.11 1824.22 1459.79 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M2265.34 1481.64 L2272.98 1481.64 L2272.98 1455.28 L2264.67 1456.95 L2264.67 1452.69 L2272.93 1451.02 L2277.61 1451.02 L2277.61 1481.64 L2285.25 1481.64 L2285.25 1485.58 L2265.34 1485.58 L2265.34 1481.64 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M2304.69 1454.1 Q2301.08 1454.1 2299.25 1457.66 Q2297.44 1461.2 2297.44 1468.33 Q2297.44 1475.44 2299.25 1479.01 Q2301.08 1482.55 2304.69 1482.55 Q2308.32 1482.55 2310.13 1479.01 Q2311.96 1475.44 2311.96 1468.33 Q2311.96 1461.2 2310.13 1457.66 Q2308.32 1454.1 2304.69 1454.1 M2304.69 1450.39 Q2310.5 1450.39 2313.56 1455 Q2316.63 1459.58 2316.63 1468.33 Q2316.63 1477.06 2313.56 1481.67 Q2310.5 1486.25 2304.69 1486.25 Q2298.88 1486.25 2295.8 1481.67 Q2292.75 1477.06 2292.75 1468.33 Q2292.75 1459.58 2295.8 1455 Q2298.88 1450.39 2304.69 1450.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1193.91 1548.76 L1193.91 1551.62 L1166.98 1551.62 Q1167.37 1557.67 1170.61 1560.85 Q1173.89 1564 1179.72 1564 Q1183.09 1564 1186.24 1563.17 Q1189.42 1562.35 1192.54 1560.69 L1192.54 1566.23 Q1189.39 1567.57 1186.08 1568.27 Q1182.77 1568.97 1179.36 1568.97 Q1170.83 1568.97 1165.84 1564 Q1160.87 1559.04 1160.87 1550.57 Q1160.87 1541.82 1165.58 1536.69 Q1170.33 1531.54 1178.35 1531.54 Q1185.54 1531.54 1189.71 1536.18 Q1193.91 1540.8 1193.91 1548.76 M1188.05 1547.04 Q1187.99 1542.23 1185.35 1539.37 Q1182.74 1536.5 1178.41 1536.5 Q1173.51 1536.5 1170.55 1539.27 Q1167.62 1542.04 1167.17 1547.07 L1188.05 1547.04 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1209.19 1562.7 L1209.19 1581.6 L1203.3 1581.6 L1203.3 1532.4 L1209.19 1532.4 L1209.19 1537.81 Q1211.03 1534.62 1213.84 1533.1 Q1216.67 1531.54 1220.58 1531.54 Q1227.08 1531.54 1231.12 1536.69 Q1235.19 1541.85 1235.19 1550.25 Q1235.19 1558.65 1231.12 1563.81 Q1227.08 1568.97 1220.58 1568.97 Q1216.67 1568.97 1213.84 1567.44 Q1211.03 1565.88 1209.19 1562.7 M1229.11 1550.25 Q1229.11 1543.79 1226.44 1540.13 Q1223.8 1536.44 1219.15 1536.44 Q1214.5 1536.44 1211.83 1540.13 Q1209.19 1543.79 1209.19 1550.25 Q1209.19 1556.71 1211.83 1560.4 Q1214.5 1564.07 1219.15 1564.07 Q1223.8 1564.07 1226.44 1560.4 Q1229.11 1556.71 1229.11 1550.25 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1258.71 1536.5 Q1254 1536.5 1251.27 1540.19 Q1248.53 1543.85 1248.53 1550.25 Q1248.53 1556.65 1251.23 1560.34 Q1253.97 1564 1258.71 1564 Q1263.39 1564 1266.13 1560.31 Q1268.87 1556.62 1268.87 1550.25 Q1268.87 1543.92 1266.13 1540.23 Q1263.39 1536.5 1258.71 1536.5 M1258.71 1531.54 Q1266.35 1531.54 1270.71 1536.5 Q1275.07 1541.47 1275.07 1550.25 Q1275.07 1559 1270.71 1564 Q1266.35 1568.97 1258.71 1568.97 Q1251.04 1568.97 1246.68 1564 Q1242.35 1559 1242.35 1550.25 Q1242.35 1541.47 1246.68 1536.5 Q1251.04 1531.54 1258.71 1531.54 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1310.43 1533.76 L1310.43 1539.24 Q1307.95 1537.87 1305.44 1537.2 Q1302.96 1536.5 1300.41 1536.5 Q1294.71 1536.5 1291.56 1540.13 Q1288.41 1543.73 1288.41 1550.25 Q1288.41 1556.78 1291.56 1560.4 Q1294.71 1564 1300.41 1564 Q1302.96 1564 1305.44 1563.33 Q1307.95 1562.63 1310.43 1561.26 L1310.43 1566.68 Q1307.98 1567.82 1305.34 1568.39 Q1302.73 1568.97 1299.77 1568.97 Q1291.72 1568.97 1286.98 1563.91 Q1282.23 1558.85 1282.23 1550.25 Q1282.23 1541.53 1287.01 1536.53 Q1291.82 1531.54 1300.15 1531.54 Q1302.86 1531.54 1305.44 1532.11 Q1308.02 1532.65 1310.43 1533.76 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1350.25 1546.53 L1350.25 1568.04 L1344.4 1568.04 L1344.4 1546.72 Q1344.4 1541.66 1342.42 1539.14 Q1340.45 1536.63 1336.5 1536.63 Q1331.76 1536.63 1329.02 1539.65 Q1326.29 1542.68 1326.29 1547.9 L1326.29 1568.04 L1320.4 1568.04 L1320.4 1518.52 L1326.29 1518.52 L1326.29 1537.93 Q1328.39 1534.72 1331.22 1533.13 Q1334.08 1531.54 1337.81 1531.54 Q1343.95 1531.54 1347.1 1535.36 Q1350.25 1539.14 1350.25 1546.53 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  158.369,1423.18 2352.76,1423.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  158.369,696.738 2352.76,696.738 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip340)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  158.369,1423.18 158.369,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip340)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  158.369,1423.18 177.267,1423.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip340)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  158.369,696.738 177.267,696.738 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip340)\" d=\"M51.6634 1442.97 L59.3023 1442.97 L59.3023 1416.61 L50.9921 1418.27 L50.9921 1414.01 L59.256 1412.35 L63.9319 1412.35 L63.9319 1442.97 L71.5707 1442.97 L71.5707 1446.91 L51.6634 1446.91 L51.6634 1442.97 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M91.0151 1415.43 Q87.404 1415.43 85.5753 1418.99 Q83.7697 1422.53 83.7697 1429.66 Q83.7697 1436.77 85.5753 1440.33 Q87.404 1443.88 91.0151 1443.88 Q94.6493 1443.88 96.4548 1440.33 Q98.2835 1436.77 98.2835 1429.66 Q98.2835 1422.53 96.4548 1418.99 Q94.6493 1415.43 91.0151 1415.43 M91.0151 1411.72 Q96.8252 1411.72 99.8808 1416.33 Q102.959 1420.91 102.959 1429.66 Q102.959 1438.39 99.8808 1443 Q96.8252 1447.58 91.0151 1447.58 Q85.2049 1447.58 82.1262 1443 Q79.0707 1438.39 79.0707 1429.66 Q79.0707 1420.91 82.1262 1416.33 Q85.2049 1411.72 91.0151 1411.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M112.664 1393.92 Q109.73 1393.92 108.244 1396.81 Q106.777 1399.69 106.777 1405.49 Q106.777 1411.26 108.244 1414.16 Q109.73 1417.03 112.664 1417.03 Q115.617 1417.03 117.084 1414.16 Q118.57 1411.26 118.57 1405.49 Q118.57 1399.69 117.084 1396.81 Q115.617 1393.92 112.664 1393.92 M112.664 1390.91 Q117.385 1390.91 119.868 1394.65 Q122.369 1398.38 122.369 1405.49 Q122.369 1412.58 119.868 1416.32 Q117.385 1420.04 112.664 1420.04 Q107.943 1420.04 105.442 1416.32 Q102.959 1412.58 102.959 1405.49 Q102.959 1398.38 105.442 1394.65 Q107.943 1390.91 112.664 1390.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M54.3529 716.53 L61.9918 716.53 L61.9918 690.165 L53.6816 691.831 L53.6816 687.572 L61.9455 685.905 L66.6214 685.905 L66.6214 716.53 L74.2602 716.53 L74.2602 720.465 L54.3529 720.465 L54.3529 716.53 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M93.7046 688.984 Q90.0935 688.984 88.2648 692.549 Q86.4592 696.09 86.4592 703.22 Q86.4592 710.326 88.2648 713.891 Q90.0935 717.433 93.7046 717.433 Q97.3388 717.433 99.1444 713.891 Q100.973 710.326 100.973 703.22 Q100.973 696.09 99.1444 692.549 Q97.3388 688.984 93.7046 688.984 M93.7046 685.28 Q99.5147 685.28 102.57 689.887 Q105.649 694.47 105.649 703.22 Q105.649 711.947 102.57 716.553 Q99.5147 721.137 93.7046 721.137 Q87.8944 721.137 84.8157 716.553 Q81.7602 711.947 81.7602 703.22 Q81.7602 694.47 84.8157 689.887 Q87.8944 685.28 93.7046 685.28 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M106.194 689.858 L112.401 689.858 L112.401 668.436 L105.649 669.79 L105.649 666.329 L112.363 664.975 L116.162 664.975 L116.162 689.858 L122.369 689.858 L122.369 693.055 L106.194 693.055 L106.194 689.858 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip342)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  220.474,47.3987 450.494,314.466 680.514,797.612 910.533,1367.58 1140.55,1716.18 1370.57,1911.09 1600.59,2016.73 1830.61,1949.32 2060.63,2011.52 2290.65,1935.27 \n",
       "  \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  220.474,47.2441 450.494,314.52 680.514,797.819 910.533,1367.55 1140.55,1716.27 1370.57,1910.98 1600.59,2016.38 1830.61,1949.01 2060.63,2009.35 2290.65,1935.97 \n",
       "  \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip340)\" d=\"\n",
       "M1813.75 248.629 L2279.61 248.629 L2279.61 93.1086 L1813.75 93.1086  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip340)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1813.75,248.629 2279.61,248.629 2279.61,93.1086 1813.75,93.1086 1813.75,248.629 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip340)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1838.14,144.949 1984.43,144.949 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip340)\" d=\"M2016.22 128.942 L2016.22 136.303 L2024.99 136.303 L2024.99 139.613 L2016.22 139.613 L2016.22 153.687 Q2016.22 156.858 2017.07 157.761 Q2017.95 158.664 2020.62 158.664 L2024.99 158.664 L2024.99 162.229 L2020.62 162.229 Q2015.68 162.229 2013.81 160.4 Q2011.93 158.548 2011.93 153.687 L2011.93 139.613 L2008.81 139.613 L2008.81 136.303 L2011.93 136.303 L2011.93 128.942 L2016.22 128.942 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M2045.62 140.284 Q2044.9 139.868 2044.04 139.682 Q2043.21 139.474 2042.19 139.474 Q2038.58 139.474 2036.63 141.835 Q2034.71 144.173 2034.71 148.571 L2034.71 162.229 L2030.43 162.229 L2030.43 136.303 L2034.71 136.303 L2034.71 140.331 Q2036.05 137.969 2038.21 136.835 Q2040.36 135.678 2043.44 135.678 Q2043.88 135.678 2044.41 135.747 Q2044.94 135.794 2045.59 135.909 L2045.62 140.284 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M2061.86 149.196 Q2056.7 149.196 2054.71 150.377 Q2052.72 151.557 2052.72 154.405 Q2052.72 156.673 2054.2 158.016 Q2055.71 159.335 2058.28 159.335 Q2061.82 159.335 2063.95 156.835 Q2066.1 154.312 2066.1 150.145 L2066.1 149.196 L2061.86 149.196 M2070.36 147.437 L2070.36 162.229 L2066.1 162.229 L2066.1 158.293 Q2064.64 160.655 2062.47 161.789 Q2060.29 162.9 2057.14 162.9 Q2053.16 162.9 2050.8 160.678 Q2048.46 158.432 2048.46 154.682 Q2048.46 150.307 2051.38 148.085 Q2054.32 145.863 2060.13 145.863 L2066.1 145.863 L2066.1 145.446 Q2066.1 142.507 2064.16 140.909 Q2062.24 139.289 2058.74 139.289 Q2056.52 139.289 2054.41 139.821 Q2052.3 140.354 2050.36 141.419 L2050.36 137.483 Q2052.7 136.581 2054.9 136.141 Q2057.1 135.678 2059.18 135.678 Q2064.8 135.678 2067.58 138.594 Q2070.36 141.511 2070.36 147.437 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M2079.13 136.303 L2083.39 136.303 L2083.39 162.229 L2079.13 162.229 L2079.13 136.303 M2079.13 126.21 L2083.39 126.21 L2083.39 131.604 L2079.13 131.604 L2079.13 126.21 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M2113.86 146.581 L2113.86 162.229 L2109.6 162.229 L2109.6 146.719 Q2109.6 143.039 2108.16 141.21 Q2106.73 139.382 2103.86 139.382 Q2100.41 139.382 2098.42 141.581 Q2096.42 143.78 2096.42 147.576 L2096.42 162.229 L2092.14 162.229 L2092.14 136.303 L2096.42 136.303 L2096.42 140.331 Q2097.95 137.993 2100.01 136.835 Q2102.1 135.678 2104.8 135.678 Q2109.27 135.678 2111.56 138.456 Q2113.86 141.21 2113.86 146.581 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M2142.05 170.099 L2142.05 173.409 L2117.42 173.409 L2117.42 170.099 L2142.05 170.099 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M2146.05 126.21 L2150.31 126.21 L2150.31 162.229 L2146.05 162.229 L2146.05 126.21 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M2169.27 139.289 Q2165.85 139.289 2163.86 141.974 Q2161.86 144.636 2161.86 149.289 Q2161.86 153.942 2163.83 156.627 Q2165.82 159.289 2169.27 159.289 Q2172.67 159.289 2174.67 156.604 Q2176.66 153.918 2176.66 149.289 Q2176.66 144.682 2174.67 141.997 Q2172.67 139.289 2169.27 139.289 M2169.27 135.678 Q2174.83 135.678 2178 139.289 Q2181.17 142.9 2181.17 149.289 Q2181.17 155.655 2178 159.289 Q2174.83 162.9 2169.27 162.9 Q2163.69 162.9 2160.52 159.289 Q2157.37 155.655 2157.37 149.289 Q2157.37 142.9 2160.52 139.289 Q2163.69 135.678 2169.27 135.678 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M2204.76 137.067 L2204.76 141.094 Q2202.95 140.169 2201.01 139.706 Q2199.06 139.243 2196.98 139.243 Q2193.81 139.243 2192.21 140.215 Q2190.64 141.187 2190.64 143.131 Q2190.64 144.613 2191.77 145.469 Q2192.91 146.303 2196.33 147.067 L2197.79 147.391 Q2202.33 148.363 2204.23 150.145 Q2206.15 151.905 2206.15 155.076 Q2206.15 158.687 2203.28 160.793 Q2200.43 162.9 2195.43 162.9 Q2193.35 162.9 2191.08 162.483 Q2188.83 162.09 2186.33 161.28 L2186.33 156.881 Q2188.69 158.108 2190.98 158.733 Q2193.28 159.335 2195.52 159.335 Q2198.53 159.335 2200.15 158.317 Q2201.77 157.275 2201.77 155.4 Q2201.77 153.664 2200.59 152.738 Q2199.43 151.812 2195.48 150.956 L2193.99 150.608 Q2190.04 149.775 2188.28 148.062 Q2186.52 146.326 2186.52 143.317 Q2186.52 139.659 2189.11 137.669 Q2191.7 135.678 2196.47 135.678 Q2198.83 135.678 2200.92 136.025 Q2203 136.372 2204.76 137.067 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M2229.46 137.067 L2229.46 141.094 Q2227.65 140.169 2225.71 139.706 Q2223.76 139.243 2221.68 139.243 Q2218.51 139.243 2216.91 140.215 Q2215.34 141.187 2215.34 143.131 Q2215.34 144.613 2216.47 145.469 Q2217.6 146.303 2221.03 147.067 L2222.49 147.391 Q2227.03 148.363 2228.92 150.145 Q2230.85 151.905 2230.85 155.076 Q2230.85 158.687 2227.98 160.793 Q2225.13 162.9 2220.13 162.9 Q2218.04 162.9 2215.78 162.483 Q2213.53 162.09 2211.03 161.28 L2211.03 156.881 Q2213.39 158.108 2215.68 158.733 Q2217.98 159.335 2220.22 159.335 Q2223.23 159.335 2224.85 158.317 Q2226.47 157.275 2226.47 155.4 Q2226.47 153.664 2225.29 152.738 Q2224.13 151.812 2220.17 150.956 L2218.69 150.608 Q2214.73 149.775 2212.98 148.062 Q2211.22 146.326 2211.22 143.317 Q2211.22 139.659 2213.81 137.669 Q2216.4 135.678 2221.17 135.678 Q2223.53 135.678 2225.61 136.025 Q2227.7 136.372 2229.46 137.067 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip340)\" style=\"stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1838.14,196.789 1984.43,196.789 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip340)\" d=\"M2008.81 188.143 L2013.32 188.143 L2021.43 209.902 L2029.53 188.143 L2034.04 188.143 L2024.32 214.069 L2018.53 214.069 L2008.81 188.143 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M2051.7 201.036 Q2046.54 201.036 2044.55 202.217 Q2042.56 203.397 2042.56 206.245 Q2042.56 208.513 2044.04 209.856 Q2045.55 211.175 2048.12 211.175 Q2051.66 211.175 2053.79 208.675 Q2055.94 206.152 2055.94 201.985 L2055.94 201.036 L2051.7 201.036 M2060.2 199.277 L2060.2 214.069 L2055.94 214.069 L2055.94 210.133 Q2054.48 212.495 2052.3 213.629 Q2050.13 214.74 2046.98 214.74 Q2043 214.74 2040.64 212.518 Q2038.3 210.272 2038.3 206.522 Q2038.3 202.147 2041.22 199.925 Q2044.16 197.703 2049.97 197.703 L2055.94 197.703 L2055.94 197.286 Q2055.94 194.347 2053.99 192.749 Q2052.07 191.129 2048.58 191.129 Q2046.36 191.129 2044.25 191.661 Q2042.14 192.194 2040.2 193.259 L2040.2 189.323 Q2042.54 188.421 2044.74 187.981 Q2046.93 187.518 2049.02 187.518 Q2054.64 187.518 2057.42 190.434 Q2060.2 193.351 2060.2 199.277 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M2068.97 178.05 L2073.23 178.05 L2073.23 214.069 L2068.97 214.069 L2068.97 178.05 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M2101.84 221.939 L2101.84 225.249 L2077.21 225.249 L2077.21 221.939 L2101.84 221.939 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M2105.85 178.05 L2110.11 178.05 L2110.11 214.069 L2105.85 214.069 L2105.85 178.05 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M2129.06 191.129 Q2125.64 191.129 2123.65 193.814 Q2121.66 196.476 2121.66 201.129 Q2121.66 205.782 2123.62 208.467 Q2125.61 211.129 2129.06 211.129 Q2132.47 211.129 2134.46 208.444 Q2136.45 205.758 2136.45 201.129 Q2136.45 196.522 2134.46 193.837 Q2132.47 191.129 2129.06 191.129 M2129.06 187.518 Q2134.62 187.518 2137.79 191.129 Q2140.96 194.74 2140.96 201.129 Q2140.96 207.495 2137.79 211.129 Q2134.62 214.74 2129.06 214.74 Q2123.48 214.74 2120.31 211.129 Q2117.17 207.495 2117.17 201.129 Q2117.17 194.74 2120.31 191.129 Q2123.48 187.518 2129.06 187.518 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M2164.55 188.907 L2164.55 192.934 Q2162.74 192.009 2160.8 191.546 Q2158.86 191.083 2156.77 191.083 Q2153.6 191.083 2152 192.055 Q2150.43 193.027 2150.43 194.971 Q2150.43 196.453 2151.56 197.309 Q2152.7 198.143 2156.12 198.907 L2157.58 199.231 Q2162.12 200.203 2164.02 201.985 Q2165.94 203.745 2165.94 206.916 Q2165.94 210.527 2163.07 212.633 Q2160.22 214.74 2155.22 214.74 Q2153.14 214.74 2150.87 214.323 Q2148.62 213.93 2146.12 213.12 L2146.12 208.721 Q2148.48 209.948 2150.78 210.573 Q2153.07 211.175 2155.31 211.175 Q2158.32 211.175 2159.94 210.157 Q2161.56 209.115 2161.56 207.24 Q2161.56 205.504 2160.38 204.578 Q2159.23 203.652 2155.27 202.796 L2153.79 202.448 Q2149.83 201.615 2148.07 199.902 Q2146.31 198.166 2146.31 195.157 Q2146.31 191.499 2148.9 189.509 Q2151.49 187.518 2156.26 187.518 Q2158.62 187.518 2160.71 187.865 Q2162.79 188.212 2164.55 188.907 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M2189.25 188.907 L2189.25 192.934 Q2187.44 192.009 2185.5 191.546 Q2183.55 191.083 2181.47 191.083 Q2178.3 191.083 2176.7 192.055 Q2175.13 193.027 2175.13 194.971 Q2175.13 196.453 2176.26 197.309 Q2177.4 198.143 2180.82 198.907 L2182.28 199.231 Q2186.82 200.203 2188.72 201.985 Q2190.64 203.745 2190.64 206.916 Q2190.64 210.527 2187.77 212.633 Q2184.92 214.74 2179.92 214.74 Q2177.84 214.74 2175.57 214.323 Q2173.32 213.93 2170.82 213.12 L2170.82 208.721 Q2173.18 209.948 2175.48 210.573 Q2177.77 211.175 2180.01 211.175 Q2183.02 211.175 2184.64 210.157 Q2186.26 209.115 2186.26 207.24 Q2186.26 205.504 2185.08 204.578 Q2183.92 203.652 2179.97 202.796 L2178.48 202.448 Q2174.53 201.615 2172.77 199.902 Q2171.01 198.166 2171.01 195.157 Q2171.01 191.499 2173.6 189.509 Q2176.19 187.518 2180.96 187.518 Q2183.32 187.518 2185.41 187.865 Q2187.49 188.212 2189.25 188.907 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 930,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Plots\n",
    "\n",
    "max_epochs = 10\n",
    "\n",
    "train_loss, val_loss, models = k_fold(max_epochs, train, 5)\n",
    "\n",
    "plot(train_loss, label=\"train_loss\", yaxis = (:log, (1,Inf)))\n",
    "plot!(val_loss, label=\"val_loss\")\n",
    "xlabel!(\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 931,
   "id": "21ca0927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 931,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a8bd9b",
   "metadata": {},
   "source": [
    "Notice that sometimes the number of training errors\n",
    "for a set of hyperparameters can be very low,\n",
    "even as the number of errors on $K$-fold cross-validation\n",
    "is considerably higher.\n",
    "This indicates that we are overfitting.\n",
    "Throughout training you will want to monitor both numbers.\n",
    "Less overfitting might indicate that our data can support a more powerful model.\n",
    "Massive overfitting might suggest that we can gain\n",
    "by incorporating regularization techniques.\n",
    "\n",
    "##  [**Submitting Predictions on Kaggle**]\n",
    "\n",
    "Now that we know what a good choice of hyperparameters should be,\n",
    "we might \n",
    "calculate the average predictions \n",
    "on the test set\n",
    "by all the $K$ models.\n",
    "Saving the predictions in a csv file\n",
    "will simplify uploading the results to Kaggle.\n",
    "The following code will generate a file called `submission.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 932,
   "id": "41359edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_matrix = Float64.(Matrix(val))\n",
    "reshape(val_matrix, 311, :)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 933,
   "id": "f91dcb75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.177955f0"
      ]
     },
     "execution_count": 933,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[1](val_matrix[5, :])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 934,
   "id": "fba66b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "using DelimitedFiles\n",
    "\n",
    "pred_values = [[model(val_matrix[1, :])[1] for model in models]]\n",
    "for i in 2:size(val)[1]\n",
    "    push!(pred_values, [model(val_matrix[i, :])[1] for model in models])\n",
    "end\n",
    "\n",
    "# Taking exponentiation of predictions in the logarithm scale\n",
    "ensemble_preds = []\n",
    "for i in 1:size(val)[1]\n",
    "    push!(ensemble_preds, Float64.(exp.(mean(pred_values[i]))))\n",
    "end\n",
    "\n",
    "submission = DataFrame(Id=raw_val.Id, \n",
    "    SalePrice=ensemble_preds)\n",
    "\n",
    "# CSV.write(\"submission.csv\", submission)\n",
    "i = 0\n",
    "open(\"submission.csv\", \"w\") do io\n",
    "    writedlm(io, [\"Id\" \"SalePrice\"])\n",
    "    writedlm(io, [raw_val.Id ensemble_preds])\n",
    "end\n",
    "# submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 935,
   "id": "1f75ebf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>1459×2 DataFrame</span></div><div style = \"float: right;\"><span style = \"font-style: italic;\">1434 rows omitted</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">Id</th><th style = \"text-align: left;\">SalePrice</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: right;\">1461</td><td style = \"text-align: right;\">2.2579e5</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: right;\">1462</td><td style = \"text-align: right;\">64724.6</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: right;\">1463</td><td style = \"text-align: right;\">1.24595e5</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: right;\">1464</td><td style = \"text-align: right;\">1.25416e5</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: right;\">1465</td><td style = \"text-align: right;\">1.94455e5</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: right;\">1466</td><td style = \"text-align: right;\">122937.0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">7</td><td style = \"text-align: right;\">1467</td><td style = \"text-align: right;\">1.14474e5</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">8</td><td style = \"text-align: right;\">1468</td><td style = \"text-align: right;\">1.3697e5</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">9</td><td style = \"text-align: right;\">1469</td><td style = \"text-align: right;\">1.20651e5</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">10</td><td style = \"text-align: right;\">1470</td><td style = \"text-align: right;\">1.13971e5</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">11</td><td style = \"text-align: right;\">1471</td><td style = \"text-align: right;\">2.2275e5</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">12</td><td style = \"text-align: right;\">1472</td><td style = \"text-align: right;\">2.03577e5</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">13</td><td style = \"text-align: right;\">1473</td><td style = \"text-align: right;\">1.66287e5</td></tr><tr><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1448</td><td style = \"text-align: right;\">2908</td><td style = \"text-align: right;\">1.32642e5</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1449</td><td style = \"text-align: right;\">2909</td><td style = \"text-align: right;\">63706.9</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1450</td><td style = \"text-align: right;\">2910</td><td style = \"text-align: right;\">2.7133e5</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1451</td><td style = \"text-align: right;\">2911</td><td style = \"text-align: right;\">1.91094e5</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1452</td><td style = \"text-align: right;\">2912</td><td style = \"text-align: right;\">87150.4</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1453</td><td style = \"text-align: right;\">2913</td><td style = \"text-align: right;\">2.8519e5</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1454</td><td style = \"text-align: right;\">2914</td><td style = \"text-align: right;\">2.46412e5</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1455</td><td style = \"text-align: right;\">2915</td><td style = \"text-align: right;\">2.62794e5</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1456</td><td style = \"text-align: right;\">2916</td><td style = \"text-align: right;\">245936.0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1457</td><td style = \"text-align: right;\">2917</td><td style = \"text-align: right;\">1.0372e5</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1458</td><td style = \"text-align: right;\">2918</td><td style = \"text-align: right;\">1.26155e5</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1459</td><td style = \"text-align: right;\">2919</td><td style = \"text-align: right;\">1.07896e5</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cc}\n",
       "\t& Id & SalePrice\\\\\n",
       "\t\\hline\n",
       "\t& Int64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & 1461 & 2.2579e5 \\\\\n",
       "\t2 & 1462 & 64724.6 \\\\\n",
       "\t3 & 1463 & 1.24595e5 \\\\\n",
       "\t4 & 1464 & 1.25416e5 \\\\\n",
       "\t5 & 1465 & 1.94455e5 \\\\\n",
       "\t6 & 1466 & 122937.0 \\\\\n",
       "\t7 & 1467 & 1.14474e5 \\\\\n",
       "\t8 & 1468 & 1.3697e5 \\\\\n",
       "\t9 & 1469 & 1.20651e5 \\\\\n",
       "\t10 & 1470 & 1.13971e5 \\\\\n",
       "\t11 & 1471 & 2.2275e5 \\\\\n",
       "\t12 & 1472 & 2.03577e5 \\\\\n",
       "\t13 & 1473 & 1.66287e5 \\\\\n",
       "\t14 & 1474 & 1.36029e5 \\\\\n",
       "\t15 & 1475 & 1.57685e5 \\\\\n",
       "\t16 & 1476 & 1.15289e5 \\\\\n",
       "\t17 & 1477 & 1.0152e5 \\\\\n",
       "\t18 & 1478 & 134754.0 \\\\\n",
       "\t19 & 1479 & 1.31503e5 \\\\\n",
       "\t20 & 1480 & 1.45337e5 \\\\\n",
       "\t21 & 1481 & 1.49956e5 \\\\\n",
       "\t22 & 1482 & 1.54536e5 \\\\\n",
       "\t23 & 1483 & 78482.9 \\\\\n",
       "\t24 & 1484 & 1.52856e5 \\\\\n",
       "\t25 & 1485 & 123439.0 \\\\\n",
       "\t26 & 1486 & 1.44713e5 \\\\\n",
       "\t27 & 1487 & 1.6871e5 \\\\\n",
       "\t28 & 1488 & 1.36903e5 \\\\\n",
       "\t29 & 1489 & 1.86959e5 \\\\\n",
       "\t30 & 1490 & 205371.0 \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m1459×2 DataFrame\u001b[0m\n",
       "\u001b[1m  Row \u001b[0m│\u001b[1m Id    \u001b[0m\u001b[1m SalePrice      \u001b[0m\n",
       "      │\u001b[90m Int64 \u001b[0m\u001b[90m Float64        \u001b[0m\n",
       "──────┼───────────────────────\n",
       "    1 │  1461       2.2579e5\n",
       "    2 │  1462   64724.6\n",
       "    3 │  1463       1.24595e5\n",
       "    4 │  1464       1.25416e5\n",
       "    5 │  1465       1.94455e5\n",
       "    6 │  1466  122937.0\n",
       "    7 │  1467       1.14474e5\n",
       "    8 │  1468       1.3697e5\n",
       "    9 │  1469       1.20651e5\n",
       "   10 │  1470       1.13971e5\n",
       "   11 │  1471       2.2275e5\n",
       "  ⋮   │   ⋮          ⋮\n",
       " 1450 │  2910       2.7133e5\n",
       " 1451 │  2911       1.91094e5\n",
       " 1452 │  2912   87150.4\n",
       " 1453 │  2913       2.8519e5\n",
       " 1454 │  2914       2.46412e5\n",
       " 1455 │  2915       2.62794e5\n",
       " 1456 │  2916  245936.0\n",
       " 1457 │  2917       1.0372e5\n",
       " 1458 │  2918       1.26155e5\n",
       " 1459 │  2919       1.07896e5\n",
       "\u001b[36m             1438 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 935,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = CSV.read(\"submission.csv\", DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88204aaf",
   "metadata": {},
   "source": [
    "Next, as demonstrated in :numref:`fig_kaggle_submit2`,\n",
    "we can submit our predictions on Kaggle\n",
    "and see how they compare with the actual house prices (labels)\n",
    "on the test set.\n",
    "The steps are quite simple:\n",
    "\n",
    "* Log in to the Kaggle website and visit the house price prediction competition page.\n",
    "* Click the “Submit Predictions” or “Late Submission” button (as of this writing, the button is located on the right).\n",
    "* Click the “Upload Submission File” button in the dashed box at the bottom of the page and select the prediction file you wish to upload.\n",
    "* Click the “Make Submission” button at the bottom of the page to view your results.\n",
    "\n",
    "![Submitting data to Kaggle](../img/kaggle-submit2.png)\n",
    ":width:`400px`\n",
    ":label:`fig_kaggle_submit2`\n",
    "\n",
    "## Summary\n",
    "\n",
    "Real data often contains a mix of different data types and needs to be preprocessed.\n",
    "Rescaling real-valued data to zero mean and unit variance is a good default. So is replacing missing values with their mean.\n",
    "Besides, transforming categorical features into indicator features allows us to treat them like one-hot vectors.\n",
    "When we tend to care more about\n",
    "the relative error than about the absolute error,\n",
    "we can \n",
    "measure the discrepancy in the logarithm of the prediction.\n",
    "To select the model and adjust the hyperparameters,\n",
    "we can use $K$-fold cross-validation .\n",
    "\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Submit your predictions for this section to Kaggle. How good are your predictions?\n",
    "1. Is it always a good idea to replace missing values by their mean? Hint: can you construct a situation where the values are not missing at random?\n",
    "1. Improve the score on Kaggle by tuning the hyperparameters through $K$-fold cross-validation.\n",
    "1. Improve the score by improving the model (e.g., layers, weight decay, and dropout).\n",
    "1. What happens if we do not standardize the continuous numerical features like what we have done in this section?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff231af8",
   "metadata": {},
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/107)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614e494f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba86f03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
