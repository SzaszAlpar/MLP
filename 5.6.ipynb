{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60615d67",
   "metadata": {},
   "source": [
    "# Dropout\n",
    "\n",
    "Az úgynevezett dropout , magában foglalja a zaj injektálását az egyes belső rétegek kiszámítása során az előrehaladás során, és ez a neurális hálózatok betanításának szabványos technikájává vált. <br>\n",
    "A módszert dropoutnak nevezik , mert egyes neuronokat szó szerint kiejtünk tanitas közben. A tanitass során, minden iterációnál a standard dropout abból áll, hogy minden rétegben nullázzuk a csomópontok egy részét a következő réteg kiszámítása előtt. A droptout felbontja a koadaptaciot es egy zaj bekuldesevel valosul meg."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266d8b81",
   "metadata": {},
   "source": [
    "### Simple example of dropout in Flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12b4c65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Flux,Plots,Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c680d69d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Dense(2 => 3),                        \u001b[90m# 9 parameters\u001b[39m\n",
       "  Dropout(0.4),\n",
       ") "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Flux,Statistics\n",
    "model = Chain(Dense(ones(3,2)), Dropout(0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da682e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×7 Matrix{Float64}:\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropout does not have effect in the test mode\n",
    "model(ones(2, 7))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b8d5d1",
   "metadata": {},
   "source": [
    "Dropout does not have effect in the test mode, but after training we can see that some nodes are 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bde16140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×7 Matrix{Float64}:\n",
       " 0.0      3.33333  3.33333  0.0      3.33333  3.33333  0.0\n",
       " 3.33333  3.33333  0.0      0.0      3.33333  0.0      3.33333\n",
       " 0.0      3.33333  3.33333  3.33333  0.0      3.33333  3.33333"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Flux.trainmode!(model);  \n",
    "model(ones(2, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de081356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mD\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mu\u001b[22m\u001b[0m\u001b[1mt\u001b[22m \u001b[0m\u001b[1md\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mu\u001b[22m\u001b[0m\u001b[1mt\u001b[22m \u001b[0m\u001b[1md\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mu\u001b[22m\u001b[0m\u001b[1mt\u001b[22m! Alpha\u001b[0m\u001b[1mD\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mu\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "Dropout(p; [dims, rng])\n",
       "\\end{verbatim}\n",
       "Layer implementing \\href{https://arxiv.org/abs/1207.0580}{dropout} with the given probability. This is used as a regularisation, i.e. to reduce overfitting.\n",
       "\n",
       "While training, it sets each input to \\texttt{0} (with probability \\texttt{p}) or else scales it by \\texttt{1 / (1 - p)}, using the \\href{@ref}{\\texttt{NNlib.dropout}} function. While testing, it has no effect.\n",
       "\n",
       "By default the mode will switch automatically, but it can also be controlled manually via \\href{@ref}{\\texttt{Flux.testmode!}}.\n",
       "\n",
       "By default every input is treated independently. With the \\texttt{dims} keyword, instead it takes a random choice only along that dimension. For example \\texttt{Dropout(p; dims = 3)} will randomly zero out entire channels on WHCN input (also called 2D dropout).\n",
       "\n",
       "Keyword \\texttt{rng} lets you specify a custom random number generator. (Only supported on the CPU.)\n",
       "\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "julia> m = Chain(Dense(ones(3,2)), Dropout(0.4))\n",
       "Chain(\n",
       "  Dense(2 => 3),                        # 9 parameters\n",
       "  Dropout(0.4),\n",
       ")\n",
       "\n",
       "julia> m(ones(2, 7))  # test mode, no effect\n",
       "3×7 Matrix{Float64}:\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0\n",
       "\n",
       "julia> Flux.trainmode!(m);  # equivalent to use within gradient\n",
       "\n",
       "julia> m(ones(2, 7))\n",
       "3×7 Matrix{Float64}:\n",
       " 0.0      0.0      3.33333  0.0      0.0      0.0  0.0\n",
       " 3.33333  0.0      3.33333  0.0      3.33333  0.0  3.33333\n",
       " 3.33333  3.33333  0.0      3.33333  0.0      0.0  3.33333\n",
       "\n",
       "julia> y = m(ones(2, 10_000));\n",
       "\n",
       "julia> using Statistics\n",
       "\n",
       "julia> mean(y)  # is about 2.0, same as in test mode\n",
       "1.9989999999999961\n",
       "\n",
       "julia> mean(iszero, y)  # is about 0.4\n",
       "0.4003\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "Dropout(p; [dims, rng])\n",
       "```\n",
       "\n",
       "Layer implementing [dropout](https://arxiv.org/abs/1207.0580) with the given probability. This is used as a regularisation, i.e. to reduce overfitting.\n",
       "\n",
       "While training, it sets each input to `0` (with probability `p`) or else scales it by `1 / (1 - p)`, using the [`NNlib.dropout`](@ref) function. While testing, it has no effect.\n",
       "\n",
       "By default the mode will switch automatically, but it can also be controlled manually via [`Flux.testmode!`](@ref).\n",
       "\n",
       "By default every input is treated independently. With the `dims` keyword, instead it takes a random choice only along that dimension. For example `Dropout(p; dims = 3)` will randomly zero out entire channels on WHCN input (also called 2D dropout).\n",
       "\n",
       "Keyword `rng` lets you specify a custom random number generator. (Only supported on the CPU.)\n",
       "\n",
       "# Examples\n",
       "\n",
       "```julia\n",
       "julia> m = Chain(Dense(ones(3,2)), Dropout(0.4))\n",
       "Chain(\n",
       "  Dense(2 => 3),                        # 9 parameters\n",
       "  Dropout(0.4),\n",
       ")\n",
       "\n",
       "julia> m(ones(2, 7))  # test mode, no effect\n",
       "3×7 Matrix{Float64}:\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0\n",
       "\n",
       "julia> Flux.trainmode!(m);  # equivalent to use within gradient\n",
       "\n",
       "julia> m(ones(2, 7))\n",
       "3×7 Matrix{Float64}:\n",
       " 0.0      0.0      3.33333  0.0      0.0      0.0  0.0\n",
       " 3.33333  0.0      3.33333  0.0      3.33333  0.0  3.33333\n",
       " 3.33333  3.33333  0.0      3.33333  0.0      0.0  3.33333\n",
       "\n",
       "julia> y = m(ones(2, 10_000));\n",
       "\n",
       "julia> using Statistics\n",
       "\n",
       "julia> mean(y)  # is about 2.0, same as in test mode\n",
       "1.9989999999999961\n",
       "\n",
       "julia> mean(iszero, y)  # is about 0.4\n",
       "0.4003\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  Dropout(p; [dims, rng])\u001b[39m\n",
       "\n",
       "  Layer implementing dropout (https://arxiv.org/abs/1207.0580) with the given\n",
       "  probability. This is used as a regularisation, i.e. to reduce overfitting.\n",
       "\n",
       "  While training, it sets each input to \u001b[36m0\u001b[39m (with probability \u001b[36mp\u001b[39m) or else scales\n",
       "  it by \u001b[36m1 / (1 - p)\u001b[39m, using the \u001b[36mNNlib.dropout\u001b[39m function. While testing, it has\n",
       "  no effect.\n",
       "\n",
       "  By default the mode will switch automatically, but it can also be controlled\n",
       "  manually via \u001b[36mFlux.testmode!\u001b[39m.\n",
       "\n",
       "  By default every input is treated independently. With the \u001b[36mdims\u001b[39m keyword,\n",
       "  instead it takes a random choice only along that dimension. For example\n",
       "  \u001b[36mDropout(p; dims = 3)\u001b[39m will randomly zero out entire channels on WHCN input\n",
       "  (also called 2D dropout).\n",
       "\n",
       "  Keyword \u001b[36mrng\u001b[39m lets you specify a custom random number generator. (Only\n",
       "  supported on the CPU.)\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> m = Chain(Dense(ones(3,2)), Dropout(0.4))\u001b[39m\n",
       "\u001b[36m  Chain(\u001b[39m\n",
       "\u001b[36m    Dense(2 => 3),                        # 9 parameters\u001b[39m\n",
       "\u001b[36m    Dropout(0.4),\u001b[39m\n",
       "\u001b[36m  )\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> m(ones(2, 7))  # test mode, no effect\u001b[39m\n",
       "\u001b[36m  3×7 Matrix{Float64}:\u001b[39m\n",
       "\u001b[36m   2.0  2.0  2.0  2.0  2.0  2.0  2.0\u001b[39m\n",
       "\u001b[36m   2.0  2.0  2.0  2.0  2.0  2.0  2.0\u001b[39m\n",
       "\u001b[36m   2.0  2.0  2.0  2.0  2.0  2.0  2.0\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> Flux.trainmode!(m);  # equivalent to use within gradient\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> m(ones(2, 7))\u001b[39m\n",
       "\u001b[36m  3×7 Matrix{Float64}:\u001b[39m\n",
       "\u001b[36m   0.0      0.0      3.33333  0.0      0.0      0.0  0.0\u001b[39m\n",
       "\u001b[36m   3.33333  0.0      3.33333  0.0      3.33333  0.0  3.33333\u001b[39m\n",
       "\u001b[36m   3.33333  3.33333  0.0      3.33333  0.0      0.0  3.33333\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> y = m(ones(2, 10_000));\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> using Statistics\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> mean(y)  # is about 2.0, same as in test mode\u001b[39m\n",
       "\u001b[36m  1.9989999999999961\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> mean(iszero, y)  # is about 0.4\u001b[39m\n",
       "\u001b[36m  0.4003\u001b[39m"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c685e69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model(ones(2, 10_000));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8942fd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9808888888888878"
     ]
    }
   ],
   "source": [
    "using Statistics\n",
    "# The dropout does not make a big change on the mean\n",
    "print(mean(y))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20dc0a6",
   "metadata": {},
   "source": [
    "### Using dropout in the previous MNIST problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b158f86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Dense(784 => 200, relu),              \u001b[90m# 157_000 parameters\u001b[39m\n",
       "  Dropout(0.5),\n",
       "  Dense(200 => 200, relu),              \u001b[90m# 40_200 parameters\u001b[39m\n",
       "  Dropout(0.5),\n",
       "  Dense(200 => 10),                     \u001b[90m# 2_010 parameters\u001b[39m\n",
       "  NNlib.softmax,\n",
       ") \u001b[90m                  # Total: 6 arrays, \u001b[39m199_210 parameters, 778.602 KiB."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Flux\n",
    "model = Chain(\n",
    "  Dense(28*28,200, relu),Dropout(0.5),\n",
    "  Dense(200,200,relu),Dropout(0.5),\n",
    "  Dense(200, 10),\n",
    "  softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a193bbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: MNIST.traindata() is deprecated, use `MNIST(split=:train)[:]` instead.\n",
      "└ @ MLDatasets C:\\Users\\Peter.Katalin\\.julia\\packages\\MLDatasets\\bg0uc\\src\\datasets\\vision\\mnist.jl:187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This program has requested access to the data dependency MNIST.\n",
      "which is not currently installed. It can be installed automatically, and you will not see this message again.\n",
      "\n",
      "Dataset: THE MNIST DATABASE of handwritten digits\n",
      "Authors: Yann LeCun, Corinna Cortes, Christopher J.C. Burges\n",
      "Website: http://yann.lecun.com/exdb/mnist/\n",
      "\n",
      "[LeCun et al., 1998a]\n",
      "    Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner.\n",
      "    \"Gradient-based learning applied to document recognition.\"\n",
      "    Proceedings of the IEEE, 86(11):2278-2324, November 1998\n",
      "\n",
      "The files are available for download at the offical\n",
      "website linked above. Note that using the data\n",
      "responsibly and respecting copyright remains your\n",
      "responsibility. The authors of MNIST aren't really\n",
      "explicit about any terms of use, so please read the\n",
      "website to make sure you want to download the\n",
      "dataset.\n",
      "\n",
      "\n",
      "\n",
      "Do you want to download the dataset from [\"https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\", \"https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\", \"https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\", \"https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\"] to \"C:\\Users\\Peter.Katalin\\.julia\\datadeps\\MNIST\"?\n",
      "[y/n]\n",
      "stdin> yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: MNIST.testdata() is deprecated, use `MNIST(split=:test)[:]` instead.\n",
      "└ @ MLDatasets C:\\Users\\Peter.Katalin\\.julia\\packages\\MLDatasets\\bg0uc\\src\\datasets\\vision\\mnist.jl:195\n"
     ]
    }
   ],
   "source": [
    "using MLDatasets\n",
    "\n",
    "# load training set\n",
    "train_x, train_y = MNIST.traindata();\n",
    "\n",
    "# load test set\n",
    "test_x,  test_y  = MNIST.testdata();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed01ba76",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = Float32.(train_x)\n",
    "y_train = Flux.onehotbatch(train_y, 0:9);\n",
    "\n",
    "x_test = Float32.(test_x)\n",
    "y_test = Flux.onehotbatch(test_y, 0:9);\n",
    "\n",
    "loss(X, y) = Flux.crossentropy(model(X), y)\n",
    "opt = Flux.ADAM();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82aa1bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Statistics\n",
    "\n",
    "function calculate_accuracy_test()\n",
    "    test_data = [(Flux.flatten(x_test), test_y)]\n",
    "    accuracy = 0\n",
    "    err=0;\n",
    "    nr_of_errors=0\n",
    "    for i in 1:length(test_y)\n",
    "        if findmax(model(test_data[1][1][:, i]))[2] - 1  == test_y[i]\n",
    "            accuracy = accuracy + 1\n",
    "        else\n",
    "            nr_of_errors=nr_of_errors+1\n",
    "            A=abs.(model(test_data[1][1][:, i]) .- y_test[:,i])\n",
    "            aux=0\n",
    "            for a in A\n",
    "                aux=aux+a\n",
    "            end\n",
    "            err=err+aux;\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    m = accuracy / length(test_y)\n",
    "    n = err/ length(test_y)\n",
    "    s = nr_of_errors \n",
    "    println(\"The val_accuracy is: \",m)\n",
    "    println(\"Mean absolute error (TEST): \",n)\n",
    "    println(\"Number of errors: \",s)\n",
    "    return m,n,s\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d9918e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with this function we'll calculate the accuracy in every epoch\n",
    "using Statistics\n",
    "\n",
    "function calculate_accuracy_train()\n",
    "    train_data = [(Flux.flatten(x_train), train_y)]\n",
    "    err=0;\n",
    "    for i in 1:length(train_y)\n",
    "        if findmax(model(train_data[1][1][:, i]))[2] - 1  != train_y[i]\n",
    "            A=abs.(model(train_data[1][1][:, i]) .- y_train[:,i])\n",
    "            aux=0\n",
    "            for a in A\n",
    "                aux=aux+a\n",
    "            end\n",
    "            err=err+aux;\n",
    "        end\n",
    "    end    \n",
    "    n = err/ length(train_y)\n",
    "    println(\"Mean absolute error (TRAIN): \",n)\n",
    "    return n\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2adb7c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "Mean absolute error (TRAIN): 1.1862954\n",
      "The val_accuracy is: 0.3458\n",
      "Mean absolute error (TEST): 1.1745566\n",
      "Number of errors: 6542\n",
      "epoch 2\n",
      "Mean absolute error (TRAIN): 0.93930775\n",
      "The val_accuracy is: 0.4763\n",
      "Mean absolute error (TEST): 0.9334824\n",
      "Number of errors: 5237\n",
      "epoch 3\n",
      "Mean absolute error (TRAIN): 0.779153\n",
      "The val_accuracy is: 0.564\n",
      "Mean absolute error (TEST): 0.77169704\n",
      "Number of errors: 4360\n",
      "epoch 4\n",
      "Mean absolute error (TRAIN): 0.66935587\n",
      "The val_accuracy is: 0.6245\n",
      "Mean absolute error (TEST): 0.66029495\n",
      "Number of errors: 3755\n",
      "epoch 5\n",
      "Mean absolute error (TRAIN): 0.59213674\n",
      "The val_accuracy is: 0.6661\n",
      "Mean absolute error (TEST): 0.5835435\n",
      "Number of errors: 3339\n",
      "epoch 6\n",
      "Mean absolute error (TRAIN): 0.5363364\n",
      "The val_accuracy is: 0.702\n",
      "Mean absolute error (TEST): 0.51738334\n",
      "Number of errors: 2980\n",
      "epoch 7\n",
      "Mean absolute error (TRAIN): 0.4877947\n",
      "The val_accuracy is: 0.7288\n",
      "Mean absolute error (TEST): 0.46759194\n",
      "Number of errors: 2712\n",
      "epoch 8\n",
      "Mean absolute error (TRAIN): 0.45235744\n",
      "The val_accuracy is: 0.7468\n",
      "Mean absolute error (TEST): 0.4329941\n",
      "Number of errors: 2532\n",
      "epoch 9\n",
      "Mean absolute error (TRAIN): 0.42427468\n",
      "The val_accuracy is: 0.7627\n",
      "Mean absolute error (TEST): 0.40224254\n",
      "Number of errors: 2373\n",
      "epoch 10\n",
      "Mean absolute error (TRAIN): 0.3990794\n",
      "The val_accuracy is: 0.7735\n",
      "Mean absolute error (TEST): 0.3795594\n",
      "Number of errors: 2265\n",
      "epoch 11\n",
      "Mean absolute error (TRAIN): 0.3726792\n",
      "The val_accuracy is: 0.7875\n",
      "Mean absolute error (TEST): 0.35267273\n",
      "Number of errors: 2125\n",
      "epoch 12\n",
      "Mean absolute error (TRAIN): 0.344568\n",
      "The val_accuracy is: 0.8036\n",
      "Mean absolute error (TEST): 0.3239328\n",
      "Number of errors: 1964\n",
      "epoch 13\n",
      "Mean absolute error (TRAIN): 0.32062426\n",
      "The val_accuracy is: 0.8162\n",
      "Mean absolute error (TEST): 0.30095553\n",
      "Number of errors: 1838\n",
      "epoch 14\n",
      "Mean absolute error (TRAIN): 0.30036753\n",
      "The val_accuracy is: 0.8266\n",
      "Mean absolute error (TEST): 0.28223607\n",
      "Number of errors: 1734\n",
      "epoch 15\n",
      "Mean absolute error (TRAIN): 0.28376755\n",
      "The val_accuracy is: 0.8349\n",
      "Mean absolute error (TEST): 0.2677101\n",
      "Number of errors: 1651\n",
      "epoch 16\n",
      "Mean absolute error (TRAIN): 0.2706765\n",
      "The val_accuracy is: 0.8429\n",
      "Mean absolute error (TEST): 0.2546395\n",
      "Number of errors: 1571\n",
      "epoch 17\n",
      "Mean absolute error (TRAIN): 0.25954273\n",
      "The val_accuracy is: 0.8473\n",
      "Mean absolute error (TEST): 0.24666792\n",
      "Number of errors: 1527\n",
      "epoch 18\n",
      "Mean absolute error (TRAIN): 0.25023773\n",
      "The val_accuracy is: 0.8517\n",
      "Mean absolute error (TEST): 0.23911053\n",
      "Number of errors: 1483\n",
      "epoch 19\n",
      "Mean absolute error (TRAIN): 0.24200627\n",
      "The val_accuracy is: 0.8567\n",
      "Mean absolute error (TEST): 0.23107493\n",
      "Number of errors: 1433\n",
      "epoch 20\n",
      "Mean absolute error (TRAIN): 0.23342487\n",
      "The val_accuracy is: 0.8633\n",
      "Mean absolute error (TEST): 0.22121577\n",
      "Number of errors: 1367\n",
      "epoch 21\n",
      "Mean absolute error (TRAIN): 0.22557414\n",
      "The val_accuracy is: 0.8687\n",
      "Mean absolute error (TEST): 0.21280149\n",
      "Number of errors: 1313\n",
      "epoch 22\n",
      "Mean absolute error (TRAIN): 0.21753496\n",
      "The val_accuracy is: 0.8724\n",
      "Mean absolute error (TEST): 0.20679323\n",
      "Number of errors: 1276\n",
      "epoch 23\n",
      "Mean absolute error (TRAIN): 0.21111865\n",
      "The val_accuracy is: 0.8771\n",
      "Mean absolute error (TEST): 0.19966973\n",
      "Number of errors: 1229\n",
      "epoch 24\n",
      "Mean absolute error (TRAIN): 0.20434317\n",
      "The val_accuracy is: 0.8804\n",
      "Mean absolute error (TEST): 0.19429979\n",
      "Number of errors: 1196\n",
      "epoch 25\n",
      "Mean absolute error (TRAIN): 0.19840463\n",
      "The val_accuracy is: 0.8836\n",
      "Mean absolute error (TEST): 0.18928427\n",
      "Number of errors: 1164\n",
      "epoch 26\n",
      "Mean absolute error (TRAIN): 0.19367206\n",
      "The val_accuracy is: 0.8864\n",
      "Mean absolute error (TEST): 0.1848051\n",
      "Number of errors: 1136\n",
      "epoch 27\n",
      "Mean absolute error (TRAIN): 0.18878965\n",
      "The val_accuracy is: 0.8888\n",
      "Mean absolute error (TEST): 0.18089487\n",
      "Number of errors: 1112\n",
      "epoch 28\n",
      "Mean absolute error (TRAIN): 0.18443277\n",
      "The val_accuracy is: 0.8917\n",
      "Mean absolute error (TEST): 0.17631501\n",
      "Number of errors: 1083\n",
      "epoch 29\n",
      "Mean absolute error (TRAIN): 0.17990302\n",
      "The val_accuracy is: 0.8944\n",
      "Mean absolute error (TEST): 0.17207536\n",
      "Number of errors: 1056\n",
      "epoch 30\n",
      "Mean absolute error (TRAIN): 0.17558523\n",
      "The val_accuracy is: 0.8974\n",
      "Mean absolute error (TEST): 0.16735256\n",
      "Number of errors: 1026\n",
      "epoch 31\n",
      "Mean absolute error (TRAIN): 0.1719778\n",
      "The val_accuracy is: 0.8994\n",
      "Mean absolute error (TEST): 0.16421343\n",
      "Number of errors: 1006\n",
      "epoch 32\n",
      "Mean absolute error (TRAIN): 0.16801089\n",
      "The val_accuracy is: 0.9017\n",
      "Mean absolute error (TEST): 0.16051771\n",
      "Number of errors: 983\n",
      "epoch 33\n",
      "Mean absolute error (TRAIN): 0.16478048\n",
      "The val_accuracy is: 0.9038\n",
      "Mean absolute error (TEST): 0.15722503\n",
      "Number of errors: 962\n",
      "epoch 34\n",
      "Mean absolute error (TRAIN): 0.1618506\n",
      "The val_accuracy is: 0.9067\n",
      "Mean absolute error (TEST): 0.15306605\n",
      "Number of errors: 933\n",
      "epoch 35\n",
      "Mean absolute error (TRAIN): 0.15946895\n",
      "The val_accuracy is: 0.9076\n",
      "Mean absolute error (TEST): 0.15157586\n",
      "Number of errors: 924\n",
      "epoch 36\n",
      "Mean absolute error (TRAIN): 0.15666185\n",
      "The val_accuracy is: 0.9087\n",
      "Mean absolute error (TEST): 0.14966942\n",
      "Number of errors: 913\n",
      "epoch 37\n",
      "Mean absolute error (TRAIN): 0.1541956\n",
      "The val_accuracy is: 0.9099\n",
      "Mean absolute error (TEST): 0.14773867\n",
      "Number of errors: 901\n",
      "epoch 38\n",
      "Mean absolute error (TRAIN): 0.15223436\n",
      "The val_accuracy is: 0.9117\n",
      "Mean absolute error (TEST): 0.14510532\n",
      "Number of errors: 883\n",
      "epoch 39\n",
      "Mean absolute error (TRAIN): 0.14974576\n",
      "The val_accuracy is: 0.9122\n",
      "Mean absolute error (TEST): 0.14407311\n",
      "Number of errors: 878\n",
      "epoch 40\n",
      "Mean absolute error (TRAIN): 0.14769769\n",
      "The val_accuracy is: 0.9137\n",
      "Mean absolute error (TEST): 0.14190996\n",
      "Number of errors: 863\n"
     ]
    }
   ],
   "source": [
    "using Flux\n",
    "\n",
    "parameters = Flux.params(model)\n",
    "# flatten() function converts array 28x28x60000 into 784x60000 \n",
    "train_data = [(Flux.flatten(x_train), Flux.flatten(y_train))]\n",
    "\n",
    "val_acc=zeros(0)\n",
    "mean_err_test=zeros(0)\n",
    "nr_err=zeros(0)\n",
    "mean_err_train=zeros(0)\n",
    "nr_of_epoch=8\n",
    "\n",
    "for i in 1:nr_of_epoch\n",
    "    println(\"epoch \",i)\n",
    "    Flux.train!(loss, parameters, train_data, opt)\n",
    "    \n",
    "    n = calculate_accuracy_train()\n",
    "    append!(mean_err_train,n)\n",
    "    m, n, s = calculate_accuracy_test()\n",
    "    append!( val_acc, m )\n",
    "    append!( mean_err_test, n )\n",
    "    append!( nr_err, s )\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1098cfff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"500\" height=\"250\" viewBox=\"0 0 2000 1000\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip530\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2000\" height=\"1000\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip530)\" d=\"\n",
       "M0 1000 L2000 1000 L2000 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip531\">\n",
       "    <rect x=\"400\" y=\"0\" width=\"1401\" height=\"1000\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip530)\" d=\"\n",
       "M182.274 901.088 L1952.76 901.088 L1952.76 47.2441 L182.274 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip532\">\n",
       "    <rect x=\"182\" y=\"47\" width=\"1771\" height=\"855\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  189.555,901.088 189.555,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  617.828,901.088 617.828,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1046.1,901.088 1046.1,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1474.37,901.088 1474.37,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1902.65,901.088 1902.65,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip530)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  182.274,901.088 1952.76,901.088 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip530)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  189.555,901.088 189.555,882.19 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip530)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  617.828,901.088 617.828,882.19 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip530)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1046.1,901.088 1046.1,882.19 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip530)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1474.37,901.088 1474.37,882.19 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip530)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1902.65,901.088 1902.65,882.19 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip530)\" d=\"M189.555 928.807 Q185.944 928.807 184.115 932.371 Q182.309 935.913 182.309 943.043 Q182.309 950.149 184.115 953.714 Q185.944 957.255 189.555 957.255 Q193.189 957.255 194.994 953.714 Q196.823 950.149 196.823 943.043 Q196.823 935.913 194.994 932.371 Q193.189 928.807 189.555 928.807 M189.555 925.103 Q195.365 925.103 198.42 929.709 Q201.499 934.293 201.499 943.043 Q201.499 951.769 198.42 956.376 Q195.365 960.959 189.555 960.959 Q183.744 960.959 180.666 956.376 Q177.61 951.769 177.61 943.043 Q177.61 934.293 180.666 929.709 Q183.744 925.103 189.555 925.103 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M592.516 956.353 L600.154 956.353 L600.154 929.987 L591.844 931.654 L591.844 927.395 L600.108 925.728 L604.784 925.728 L604.784 956.353 L612.423 956.353 L612.423 960.288 L592.516 960.288 L592.516 956.353 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M631.867 928.807 Q628.256 928.807 626.427 932.371 Q624.622 935.913 624.622 943.043 Q624.622 950.149 626.427 953.714 Q628.256 957.255 631.867 957.255 Q635.501 957.255 637.307 953.714 Q639.136 950.149 639.136 943.043 Q639.136 935.913 637.307 932.371 Q635.501 928.807 631.867 928.807 M631.867 925.103 Q637.677 925.103 640.733 929.709 Q643.812 934.293 643.812 943.043 Q643.812 951.769 640.733 956.376 Q637.677 960.959 631.867 960.959 Q626.057 960.959 622.978 956.376 Q619.923 951.769 619.923 943.043 Q619.923 934.293 622.978 929.709 Q626.057 925.103 631.867 925.103 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1024.87 956.353 L1041.19 956.353 L1041.19 960.288 L1019.25 960.288 L1019.25 956.353 Q1021.91 953.598 1026.49 948.968 Q1031.1 944.316 1032.28 942.973 Q1034.53 940.45 1035.41 938.714 Q1036.31 936.955 1036.31 935.265 Q1036.31 932.51 1034.37 930.774 Q1032.44 929.038 1029.34 929.038 Q1027.14 929.038 1024.69 929.802 Q1022.26 930.566 1019.48 932.117 L1019.48 927.395 Q1022.31 926.26 1024.76 925.682 Q1027.21 925.103 1029.25 925.103 Q1034.62 925.103 1037.81 927.788 Q1041.01 930.473 1041.01 934.964 Q1041.01 937.094 1040.2 939.015 Q1039.41 940.913 1037.31 943.506 Q1036.73 944.177 1033.62 947.394 Q1030.52 950.589 1024.87 956.353 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1061.01 928.807 Q1057.4 928.807 1055.57 932.371 Q1053.76 935.913 1053.76 943.043 Q1053.76 950.149 1055.57 953.714 Q1057.4 957.255 1061.01 957.255 Q1064.64 957.255 1066.45 953.714 Q1068.28 950.149 1068.28 943.043 Q1068.28 935.913 1066.45 932.371 Q1064.64 928.807 1061.01 928.807 M1061.01 925.103 Q1066.82 925.103 1069.87 929.709 Q1072.95 934.293 1072.95 943.043 Q1072.95 951.769 1069.87 956.376 Q1066.82 960.959 1061.01 960.959 Q1055.2 960.959 1052.12 956.376 Q1049.06 951.769 1049.06 943.043 Q1049.06 934.293 1052.12 929.709 Q1055.2 925.103 1061.01 925.103 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1463.22 941.654 Q1466.57 942.371 1468.45 944.64 Q1470.35 946.908 1470.35 950.242 Q1470.35 955.357 1466.83 958.158 Q1463.31 960.959 1456.83 960.959 Q1454.65 960.959 1452.34 960.519 Q1450.05 960.103 1447.59 959.246 L1447.59 954.732 Q1449.54 955.867 1451.85 956.445 Q1454.17 957.024 1456.69 957.024 Q1461.09 957.024 1463.38 955.288 Q1465.69 953.552 1465.69 950.242 Q1465.69 947.186 1463.54 945.473 Q1461.41 943.737 1457.59 943.737 L1453.56 943.737 L1453.56 939.894 L1457.78 939.894 Q1461.23 939.894 1463.06 938.529 Q1464.88 937.14 1464.88 934.547 Q1464.88 931.885 1462.99 930.473 Q1461.11 929.038 1457.59 929.038 Q1455.67 929.038 1453.47 929.455 Q1451.27 929.871 1448.63 930.751 L1448.63 926.584 Q1451.3 925.844 1453.61 925.473 Q1455.95 925.103 1458.01 925.103 Q1463.33 925.103 1466.43 927.533 Q1469.54 929.941 1469.54 934.061 Q1469.54 936.932 1467.89 938.922 Q1466.25 940.89 1463.22 941.654 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1489.21 928.807 Q1485.6 928.807 1483.77 932.371 Q1481.97 935.913 1481.97 943.043 Q1481.97 950.149 1483.77 953.714 Q1485.6 957.255 1489.21 957.255 Q1492.85 957.255 1494.65 953.714 Q1496.48 950.149 1496.48 943.043 Q1496.48 935.913 1494.65 932.371 Q1492.85 928.807 1489.21 928.807 M1489.21 925.103 Q1495.02 925.103 1498.08 929.709 Q1501.16 934.293 1501.16 943.043 Q1501.16 951.769 1498.08 956.376 Q1495.02 960.959 1489.21 960.959 Q1483.4 960.959 1480.32 956.376 Q1477.27 951.769 1477.27 943.043 Q1477.27 934.293 1480.32 929.709 Q1483.4 925.103 1489.21 925.103 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1890.82 929.802 L1879.01 948.251 L1890.82 948.251 L1890.82 929.802 M1889.59 925.728 L1895.47 925.728 L1895.47 948.251 L1900.4 948.251 L1900.4 952.14 L1895.47 952.14 L1895.47 960.288 L1890.82 960.288 L1890.82 952.14 L1875.22 952.14 L1875.22 947.626 L1889.59 925.728 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1918.13 928.807 Q1914.52 928.807 1912.69 932.371 Q1910.89 935.913 1910.89 943.043 Q1910.89 950.149 1912.69 953.714 Q1914.52 957.255 1918.13 957.255 Q1921.77 957.255 1923.57 953.714 Q1925.4 950.149 1925.4 943.043 Q1925.4 935.913 1923.57 932.371 Q1921.77 928.807 1918.13 928.807 M1918.13 925.103 Q1923.94 925.103 1927 929.709 Q1930.08 934.293 1930.08 943.043 Q1930.08 951.769 1927 956.376 Q1923.94 960.959 1918.13 960.959 Q1912.32 960.959 1909.25 956.376 Q1906.19 951.769 1906.19 943.043 Q1906.19 934.293 1909.25 929.709 Q1912.32 925.103 1918.13 925.103 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  182.274,793.555 1952.76,793.555 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  182.274,600.735 1952.76,600.735 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  182.274,407.915 1952.76,407.915 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  182.274,215.095 1952.76,215.095 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip530)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  182.274,901.088 182.274,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip530)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  182.274,793.555 201.172,793.555 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip530)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  182.274,600.735 201.172,600.735 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip530)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  182.274,407.915 201.172,407.915 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip530)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  182.274,215.095 201.172,215.095 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip530)\" d=\"M65.9319 779.354 Q62.3208 779.354 60.4921 782.918 Q58.6865 786.46 58.6865 793.59 Q58.6865 800.696 60.4921 804.261 Q62.3208 807.802 65.9319 807.802 Q69.5661 807.802 71.3717 804.261 Q73.2004 800.696 73.2004 793.59 Q73.2004 786.46 71.3717 782.918 Q69.5661 779.354 65.9319 779.354 M65.9319 775.65 Q71.742 775.65 74.7976 780.256 Q77.8763 784.84 77.8763 793.59 Q77.8763 802.316 74.7976 806.923 Q71.742 811.506 65.9319 811.506 Q60.1217 811.506 57.043 806.923 Q53.9875 802.316 53.9875 793.59 Q53.9875 784.84 57.043 780.256 Q60.1217 775.65 65.9319 775.65 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M86.0938 804.955 L90.978 804.955 L90.978 810.835 L86.0938 810.835 L86.0938 804.955 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M105.191 806.9 L121.51 806.9 L121.51 810.835 L99.566 810.835 L99.566 806.9 Q102.228 804.145 106.811 799.515 Q111.418 794.863 112.598 793.52 Q114.844 790.997 115.723 789.261 Q116.626 787.502 116.626 785.812 Q116.626 783.057 114.682 781.321 Q112.76 779.585 109.658 779.585 Q107.459 779.585 105.006 780.349 Q102.575 781.113 99.7974 782.664 L99.7974 777.942 Q102.621 776.807 105.075 776.229 Q107.529 775.65 109.566 775.65 Q114.936 775.65 118.131 778.335 Q121.325 781.02 121.325 785.511 Q121.325 787.641 120.515 789.562 Q119.728 791.46 117.621 794.053 Q117.043 794.724 113.941 797.941 Q110.839 801.136 105.191 806.9 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M131.371 776.275 L149.728 776.275 L149.728 780.21 L135.654 780.21 L135.654 788.682 Q136.672 788.335 137.691 788.173 Q138.709 787.988 139.728 787.988 Q145.515 787.988 148.894 791.159 Q152.274 794.33 152.274 799.747 Q152.274 805.326 148.802 808.427 Q145.33 811.506 139.01 811.506 Q136.834 811.506 134.566 811.136 Q132.32 810.765 129.913 810.025 L129.913 805.326 Q131.996 806.46 134.219 807.015 Q136.441 807.571 138.918 807.571 Q142.922 807.571 145.26 805.465 Q147.598 803.358 147.598 799.747 Q147.598 796.136 145.26 794.029 Q142.922 791.923 138.918 791.923 Q137.043 791.923 135.168 792.34 Q133.316 792.756 131.371 793.636 L131.371 776.275 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M64.9365 586.534 Q61.3254 586.534 59.4967 590.098 Q57.6912 593.64 57.6912 600.77 Q57.6912 607.876 59.4967 611.441 Q61.3254 614.983 64.9365 614.983 Q68.5707 614.983 70.3763 611.441 Q72.205 607.876 72.205 600.77 Q72.205 593.64 70.3763 590.098 Q68.5707 586.534 64.9365 586.534 M64.9365 582.83 Q70.7467 582.83 73.8022 587.436 Q76.8809 592.02 76.8809 600.77 Q76.8809 609.497 73.8022 614.103 Q70.7467 618.686 64.9365 618.686 Q59.1264 618.686 56.0477 614.103 Q52.9921 609.497 52.9921 600.77 Q52.9921 592.02 56.0477 587.436 Q59.1264 582.83 64.9365 582.83 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M85.0984 612.135 L89.9827 612.135 L89.9827 618.015 L85.0984 618.015 L85.0984 612.135 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M100.214 583.455 L118.57 583.455 L118.57 587.39 L104.496 587.39 L104.496 595.862 Q105.515 595.515 106.534 595.353 Q107.552 595.168 108.571 595.168 Q114.358 595.168 117.737 598.339 Q121.117 601.51 121.117 606.927 Q121.117 612.506 117.645 615.608 Q114.172 618.686 107.853 618.686 Q105.677 618.686 103.409 618.316 Q101.163 617.946 98.7558 617.205 L98.7558 612.506 Q100.839 613.64 103.061 614.196 Q105.284 614.751 107.76 614.751 Q111.765 614.751 114.103 612.645 Q116.441 610.538 116.441 606.927 Q116.441 603.316 114.103 601.21 Q111.765 599.103 107.76 599.103 Q105.885 599.103 104.01 599.52 Q102.159 599.936 100.214 600.816 L100.214 583.455 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M140.33 586.534 Q136.719 586.534 134.89 590.098 Q133.084 593.64 133.084 600.77 Q133.084 607.876 134.89 611.441 Q136.719 614.983 140.33 614.983 Q143.964 614.983 145.769 611.441 Q147.598 607.876 147.598 600.77 Q147.598 593.64 145.769 590.098 Q143.964 586.534 140.33 586.534 M140.33 582.83 Q146.14 582.83 149.195 587.436 Q152.274 592.02 152.274 600.77 Q152.274 609.497 149.195 614.103 Q146.14 618.686 140.33 618.686 Q134.519 618.686 131.441 614.103 Q128.385 609.497 128.385 600.77 Q128.385 592.02 131.441 587.436 Q134.519 582.83 140.33 582.83 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M65.9319 393.714 Q62.3208 393.714 60.4921 397.279 Q58.6865 400.82 58.6865 407.95 Q58.6865 415.056 60.4921 418.621 Q62.3208 422.163 65.9319 422.163 Q69.5661 422.163 71.3717 418.621 Q73.2004 415.056 73.2004 407.95 Q73.2004 400.82 71.3717 397.279 Q69.5661 393.714 65.9319 393.714 M65.9319 390.01 Q71.742 390.01 74.7976 394.617 Q77.8763 399.2 77.8763 407.95 Q77.8763 416.677 74.7976 421.283 Q71.742 425.866 65.9319 425.866 Q60.1217 425.866 57.043 421.283 Q53.9875 416.677 53.9875 407.95 Q53.9875 399.2 57.043 394.617 Q60.1217 390.01 65.9319 390.01 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M86.0938 419.316 L90.978 419.316 L90.978 425.195 L86.0938 425.195 L86.0938 419.316 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M99.9826 390.635 L122.205 390.635 L122.205 392.626 L109.658 425.195 L104.774 425.195 L116.58 394.57 L99.9826 394.57 L99.9826 390.635 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M131.371 390.635 L149.728 390.635 L149.728 394.57 L135.654 394.57 L135.654 403.042 Q136.672 402.695 137.691 402.533 Q138.709 402.348 139.728 402.348 Q145.515 402.348 148.894 405.519 Q152.274 408.691 152.274 414.107 Q152.274 419.686 148.802 422.788 Q145.33 425.866 139.01 425.866 Q136.834 425.866 134.566 425.496 Q132.32 425.126 129.913 424.385 L129.913 419.686 Q131.996 420.82 134.219 421.376 Q136.441 421.931 138.918 421.931 Q142.922 421.931 145.26 419.825 Q147.598 417.718 147.598 414.107 Q147.598 410.496 145.26 408.39 Q142.922 406.283 138.918 406.283 Q137.043 406.283 135.168 406.7 Q133.316 407.117 131.371 407.996 L131.371 390.635 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M55.7467 228.44 L63.3856 228.44 L63.3856 202.075 L55.0754 203.741 L55.0754 199.482 L63.3393 197.815 L68.0152 197.815 L68.0152 228.44 L75.654 228.44 L75.654 232.375 L55.7467 232.375 L55.7467 228.44 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M85.0984 226.496 L89.9827 226.496 L89.9827 232.375 L85.0984 232.375 L85.0984 226.496 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M110.168 200.894 Q106.557 200.894 104.728 204.459 Q102.922 208 102.922 215.13 Q102.922 222.236 104.728 225.801 Q106.557 229.343 110.168 229.343 Q113.802 229.343 115.608 225.801 Q117.436 222.236 117.436 215.13 Q117.436 208 115.608 204.459 Q113.802 200.894 110.168 200.894 M110.168 197.19 Q115.978 197.19 119.033 201.797 Q122.112 206.38 122.112 215.13 Q122.112 223.857 119.033 228.463 Q115.978 233.047 110.168 233.047 Q104.358 233.047 101.279 228.463 Q98.2234 223.857 98.2234 215.13 Q98.2234 206.38 101.279 201.797 Q104.358 197.19 110.168 197.19 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M140.33 200.894 Q136.719 200.894 134.89 204.459 Q133.084 208 133.084 215.13 Q133.084 222.236 134.89 225.801 Q136.719 229.343 140.33 229.343 Q143.964 229.343 145.769 225.801 Q147.598 222.236 147.598 215.13 Q147.598 208 145.769 204.459 Q143.964 200.894 140.33 200.894 M140.33 197.19 Q146.14 197.19 149.195 201.797 Q152.274 206.38 152.274 215.13 Q152.274 223.857 149.195 228.463 Q146.14 233.047 140.33 233.047 Q134.519 233.047 131.441 228.463 Q128.385 223.857 128.385 215.13 Q128.385 206.38 131.441 201.797 Q134.519 197.19 140.33 197.19 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip532)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  232.382,719.666 275.209,619.014 318.037,551.373 360.864,504.711 403.691,472.625 446.519,444.937 489.346,424.266 532.173,410.383 575.001,398.12 617.828,389.79 \n",
       "  660.655,378.992 703.483,366.575 746.31,356.856 789.137,348.835 831.965,342.434 874.792,336.263 917.619,332.87 960.447,329.476 1003.27,325.62 1046.1,320.529 \n",
       "  1088.93,316.364 1131.76,313.511 1174.58,309.886 1217.41,307.34 1260.24,304.872 1303.07,302.713 1345.89,300.862 1388.72,298.625 1431.55,296.542 1474.37,294.229 \n",
       "  1517.2,292.686 1560.03,290.912 1602.86,289.292 1645.68,287.056 1688.51,286.362 1731.34,285.513 1774.17,284.588 1816.99,283.199 1859.82,282.814 1902.65,281.657 \n",
       "  \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  232.382,80.4634 275.209,266.399 318.037,391.181 360.864,477.103 403.691,536.3 446.519,587.328 489.346,625.731 532.173,652.415 575.001,676.133 617.828,693.628 \n",
       "  660.655,714.365 703.483,736.532 746.31,754.254 789.137,768.692 831.965,779.895 874.792,789.977 917.619,796.125 960.447,801.954 1003.27,808.151 1046.1,815.756 \n",
       "  1088.93,822.245 1131.76,826.879 1174.58,832.374 1217.41,836.515 1260.24,840.384 1303.07,843.838 1345.89,846.854 1388.72,850.387 1431.55,853.657 1474.37,857.299 \n",
       "  1517.2,859.72 1560.03,862.571 1602.86,865.11 1645.68,868.318 1688.51,869.467 1731.34,870.938 1774.17,872.427 1816.99,874.458 1859.82,875.254 1902.65,876.922 \n",
       "  \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#3da44d; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  232.382,71.4095 275.209,261.906 318.037,385.43 360.864,470.114 403.691,529.672 446.519,572.709 489.346,610.149 532.173,637.481 575.001,659.14 617.828,678.573 \n",
       "  660.655,698.935 703.483,720.616 746.31,739.084 789.137,754.707 831.965,767.511 874.792,777.608 917.619,786.195 960.447,793.371 1003.27,799.72 1046.1,806.339 \n",
       "  1088.93,812.394 1131.76,818.594 1174.58,823.543 1217.41,828.769 1260.24,833.349 1303.07,836.999 1345.89,840.765 1388.72,844.126 1431.55,847.619 1474.37,850.949 \n",
       "  1517.2,853.732 1560.03,856.791 1602.86,859.283 1645.68,861.543 1688.51,863.38 1731.34,865.545 1774.17,867.447 1816.99,868.959 1859.82,870.879 1902.65,872.459 \n",
       "  \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip530)\" d=\"\n",
       "M1345.84 872.626 L1893.74 872.626 L1893.74 665.266 L1345.84 665.266  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip530)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1345.84,872.626 1893.74,872.626 1893.74,665.266 1345.84,665.266 1345.84,872.626 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip530)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1365.51,717.106 1483.55,717.106 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip530)\" d=\"M1503.22 708.461 L1507.73 708.461 L1515.83 730.22 L1523.93 708.461 L1528.45 708.461 L1518.73 734.386 L1512.94 734.386 L1503.22 708.461 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1546.11 721.354 Q1540.95 721.354 1538.96 722.535 Q1536.97 723.715 1536.97 726.562 Q1536.97 728.831 1538.45 730.173 Q1539.95 731.493 1542.52 731.493 Q1546.06 731.493 1548.19 728.993 Q1550.35 726.47 1550.35 722.303 L1550.35 721.354 L1546.11 721.354 M1554.61 719.595 L1554.61 734.386 L1550.35 734.386 L1550.35 730.451 Q1548.89 732.812 1546.71 733.947 Q1544.54 735.058 1541.39 735.058 Q1537.41 735.058 1535.05 732.835 Q1532.71 730.59 1532.71 726.84 Q1532.71 722.465 1535.62 720.243 Q1538.56 718.021 1544.37 718.021 L1550.35 718.021 L1550.35 717.604 Q1550.35 714.664 1548.4 713.067 Q1546.48 711.447 1542.99 711.447 Q1540.76 711.447 1538.66 711.979 Q1536.55 712.512 1534.61 713.576 L1534.61 709.641 Q1536.94 708.738 1539.14 708.299 Q1541.34 707.836 1543.43 707.836 Q1549.05 707.836 1551.83 710.752 Q1554.61 713.669 1554.61 719.595 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1563.38 698.368 L1567.64 698.368 L1567.64 734.386 L1563.38 734.386 L1563.38 698.368 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1596.25 742.257 L1596.25 745.567 L1571.62 745.567 L1571.62 742.257 L1596.25 742.257 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1612.04 721.354 Q1606.87 721.354 1604.88 722.535 Q1602.89 723.715 1602.89 726.562 Q1602.89 728.831 1604.37 730.173 Q1605.88 731.493 1608.45 731.493 Q1611.99 731.493 1614.12 728.993 Q1616.27 726.47 1616.27 722.303 L1616.27 721.354 L1612.04 721.354 M1620.53 719.595 L1620.53 734.386 L1616.27 734.386 L1616.27 730.451 Q1614.81 732.812 1612.64 733.947 Q1610.46 735.058 1607.31 735.058 Q1603.33 735.058 1600.97 732.835 Q1598.63 730.59 1598.63 726.84 Q1598.63 722.465 1601.55 720.243 Q1604.49 718.021 1610.3 718.021 L1616.27 718.021 L1616.27 717.604 Q1616.27 714.664 1614.33 713.067 Q1612.41 711.447 1608.91 711.447 Q1606.69 711.447 1604.58 711.979 Q1602.48 712.512 1600.53 713.576 L1600.53 709.641 Q1602.87 708.738 1605.07 708.299 Q1607.27 707.836 1609.35 707.836 Q1614.98 707.836 1617.75 710.752 Q1620.53 713.669 1620.53 719.595 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1647.96 709.456 L1647.96 713.437 Q1646.16 712.442 1644.33 711.956 Q1642.52 711.447 1640.67 711.447 Q1636.53 711.447 1634.23 714.086 Q1631.94 716.701 1631.94 721.447 Q1631.94 726.192 1634.23 728.831 Q1636.53 731.447 1640.67 731.447 Q1642.52 731.447 1644.33 730.961 Q1646.16 730.451 1647.96 729.456 L1647.96 733.391 Q1646.18 734.224 1644.26 734.641 Q1642.36 735.058 1640.21 735.058 Q1634.35 735.058 1630.9 731.377 Q1627.45 727.697 1627.45 721.447 Q1627.45 715.104 1630.92 711.47 Q1634.42 707.836 1640.48 707.836 Q1642.45 707.836 1644.33 708.252 Q1646.2 708.646 1647.96 709.456 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1674.03 709.456 L1674.03 713.437 Q1672.22 712.442 1670.39 711.956 Q1668.59 711.447 1666.73 711.447 Q1662.59 711.447 1660.3 714.086 Q1658.01 716.701 1658.01 721.447 Q1658.01 726.192 1660.3 728.831 Q1662.59 731.447 1666.73 731.447 Q1668.59 731.447 1670.39 730.961 Q1672.22 730.451 1674.03 729.456 L1674.03 733.391 Q1672.24 734.224 1670.32 734.641 Q1668.42 735.058 1666.27 735.058 Q1660.42 735.058 1656.97 731.377 Q1653.52 727.697 1653.52 721.447 Q1653.52 715.104 1656.99 711.47 Q1660.48 707.836 1666.55 707.836 Q1668.52 707.836 1670.39 708.252 Q1672.27 708.646 1674.03 709.456 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip530)\" style=\"stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1365.51,768.946 1483.55,768.946 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip530)\" d=\"M1523.56 765.277 Q1525.16 762.407 1527.38 761.041 Q1529.61 759.676 1532.62 759.676 Q1536.67 759.676 1538.87 762.523 Q1541.06 765.347 1541.06 770.578 L1541.06 786.226 L1536.78 786.226 L1536.78 770.717 Q1536.78 766.99 1535.46 765.185 Q1534.14 763.379 1531.43 763.379 Q1528.12 763.379 1526.2 765.578 Q1524.28 767.777 1524.28 771.574 L1524.28 786.226 L1520 786.226 L1520 770.717 Q1520 766.967 1518.68 765.185 Q1517.36 763.379 1514.61 763.379 Q1511.34 763.379 1509.42 765.602 Q1507.5 767.801 1507.5 771.574 L1507.5 786.226 L1503.22 786.226 L1503.22 760.301 L1507.5 760.301 L1507.5 764.328 Q1508.96 761.944 1510.99 760.81 Q1513.03 759.676 1515.83 759.676 Q1518.66 759.676 1520.62 761.111 Q1522.62 762.546 1523.56 765.277 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1571.74 772.199 L1571.74 774.282 L1552.15 774.282 Q1552.43 778.68 1554.79 780.995 Q1557.18 783.287 1561.41 783.287 Q1563.86 783.287 1566.16 782.685 Q1568.47 782.083 1570.74 780.879 L1570.74 784.907 Q1568.45 785.879 1566.04 786.388 Q1563.63 786.898 1561.16 786.898 Q1554.95 786.898 1551.32 783.287 Q1547.71 779.676 1547.71 773.518 Q1547.71 767.152 1551.13 763.426 Q1554.58 759.676 1560.42 759.676 Q1565.65 759.676 1568.68 763.055 Q1571.74 766.412 1571.74 772.199 M1567.48 770.949 Q1567.43 767.453 1565.51 765.37 Q1563.61 763.287 1560.46 763.287 Q1556.9 763.287 1554.74 765.301 Q1552.62 767.314 1552.29 770.972 L1567.48 770.949 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1590.51 773.194 Q1585.35 773.194 1583.36 774.375 Q1581.36 775.555 1581.36 778.402 Q1581.36 780.671 1582.85 782.013 Q1584.35 783.333 1586.92 783.333 Q1590.46 783.333 1592.59 780.833 Q1594.74 778.31 1594.74 774.143 L1594.74 773.194 L1590.51 773.194 M1599 771.435 L1599 786.226 L1594.74 786.226 L1594.74 782.291 Q1593.29 784.652 1591.11 785.787 Q1588.93 786.898 1585.79 786.898 Q1581.8 786.898 1579.44 784.675 Q1577.11 782.43 1577.11 778.68 Q1577.11 774.305 1580.02 772.083 Q1582.96 769.861 1588.77 769.861 L1594.74 769.861 L1594.74 769.444 Q1594.74 766.504 1592.8 764.907 Q1590.88 763.287 1587.38 763.287 Q1585.16 763.287 1583.05 763.819 Q1580.95 764.352 1579 765.416 L1579 761.481 Q1581.34 760.578 1583.54 760.139 Q1585.74 759.676 1587.82 759.676 Q1593.45 759.676 1596.23 762.592 Q1599 765.509 1599 771.435 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1629.33 770.578 L1629.33 786.226 L1625.07 786.226 L1625.07 770.717 Q1625.07 767.037 1623.63 765.208 Q1622.2 763.379 1619.33 763.379 Q1615.88 763.379 1613.89 765.578 Q1611.9 767.777 1611.9 771.574 L1611.9 786.226 L1607.61 786.226 L1607.61 760.301 L1611.9 760.301 L1611.9 764.328 Q1613.42 761.99 1615.49 760.833 Q1617.57 759.676 1620.28 759.676 Q1624.74 759.676 1627.04 762.453 Q1629.33 765.208 1629.33 770.578 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1657.52 794.097 L1657.52 797.407 L1632.89 797.407 L1632.89 794.097 L1657.52 794.097 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1683.7 772.199 L1683.7 774.282 L1664.12 774.282 Q1664.4 778.68 1666.76 780.995 Q1669.14 783.287 1673.38 783.287 Q1675.83 783.287 1678.12 782.685 Q1680.44 782.083 1682.71 780.879 L1682.71 784.907 Q1680.42 785.879 1678.01 786.388 Q1675.6 786.898 1673.12 786.898 Q1666.92 786.898 1663.29 783.287 Q1659.67 779.676 1659.67 773.518 Q1659.67 767.152 1663.1 763.426 Q1666.55 759.676 1672.38 759.676 Q1677.61 759.676 1680.65 763.055 Q1683.7 766.412 1683.7 772.199 M1679.44 770.949 Q1679.4 767.453 1677.48 765.37 Q1675.58 763.287 1672.43 763.287 Q1668.86 763.287 1666.71 765.301 Q1664.58 767.314 1664.26 770.972 L1679.44 770.949 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1705.72 764.282 Q1705 763.865 1704.14 763.68 Q1703.31 763.472 1702.29 763.472 Q1698.68 763.472 1696.73 765.833 Q1694.81 768.171 1694.81 772.569 L1694.81 786.226 L1690.53 786.226 L1690.53 760.301 L1694.81 760.301 L1694.81 764.328 Q1696.16 761.967 1698.31 760.833 Q1700.46 759.676 1703.54 759.676 Q1703.98 759.676 1704.51 759.745 Q1705.04 759.791 1705.69 759.907 L1705.72 764.282 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1724.37 764.282 Q1723.66 763.865 1722.8 763.68 Q1721.97 763.472 1720.95 763.472 Q1717.34 763.472 1715.39 765.833 Q1713.47 768.171 1713.47 772.569 L1713.47 786.226 L1709.19 786.226 L1709.19 760.301 L1713.47 760.301 L1713.47 764.328 Q1714.81 761.967 1716.97 760.833 Q1719.12 759.676 1722.2 759.676 Q1722.64 759.676 1723.17 759.745 Q1723.7 759.791 1724.35 759.907 L1724.37 764.282 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1748.54 794.097 L1748.54 797.407 L1723.91 797.407 L1723.91 794.097 L1748.54 794.097 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1756.76 752.94 L1756.76 760.301 L1765.53 760.301 L1765.53 763.611 L1756.76 763.611 L1756.76 777.685 Q1756.76 780.856 1757.61 781.759 Q1758.49 782.662 1761.16 782.662 L1765.53 782.662 L1765.53 786.226 L1761.16 786.226 Q1756.23 786.226 1754.35 784.398 Q1752.48 782.546 1752.48 777.685 L1752.48 763.611 L1749.35 763.611 L1749.35 760.301 L1752.48 760.301 L1752.48 752.94 L1756.76 752.94 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1793.31 772.199 L1793.31 774.282 L1773.72 774.282 Q1774 778.68 1776.36 780.995 Q1778.75 783.287 1782.98 783.287 Q1785.44 783.287 1787.73 782.685 Q1790.04 782.083 1792.31 780.879 L1792.31 784.907 Q1790.02 785.879 1787.61 786.388 Q1785.21 786.898 1782.73 786.898 Q1776.53 786.898 1772.89 783.287 Q1769.28 779.676 1769.28 773.518 Q1769.28 767.152 1772.71 763.426 Q1776.16 759.676 1781.99 759.676 Q1787.22 759.676 1790.25 763.055 Q1793.31 766.412 1793.31 772.199 M1789.05 770.949 Q1789 767.453 1787.08 765.37 Q1785.18 763.287 1782.04 763.287 Q1778.47 763.287 1776.32 765.301 Q1774.19 767.314 1773.86 770.972 L1789.05 770.949 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1816.83 761.065 L1816.83 765.092 Q1815.02 764.166 1813.08 763.703 Q1811.13 763.24 1809.05 763.24 Q1805.88 763.24 1804.28 764.213 Q1802.71 765.185 1802.71 767.129 Q1802.71 768.611 1803.84 769.467 Q1804.97 770.301 1808.4 771.064 L1809.86 771.389 Q1814.4 772.361 1816.29 774.143 Q1818.22 775.902 1818.22 779.074 Q1818.22 782.685 1815.35 784.791 Q1812.5 786.898 1807.5 786.898 Q1805.41 786.898 1803.15 786.481 Q1800.9 786.088 1798.4 785.277 L1798.4 780.879 Q1800.76 782.106 1803.05 782.731 Q1805.35 783.333 1807.59 783.333 Q1810.6 783.333 1812.22 782.314 Q1813.84 781.273 1813.84 779.398 Q1813.84 777.662 1812.66 776.736 Q1811.5 775.81 1807.54 774.953 L1806.06 774.606 Q1802.1 773.773 1800.35 772.06 Q1798.59 770.324 1798.59 767.314 Q1798.59 763.657 1801.18 761.666 Q1803.77 759.676 1808.54 759.676 Q1810.9 759.676 1812.98 760.023 Q1815.07 760.37 1816.83 761.065 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1829.21 752.94 L1829.21 760.301 L1837.98 760.301 L1837.98 763.611 L1829.21 763.611 L1829.21 777.685 Q1829.21 780.856 1830.07 781.759 Q1830.95 782.662 1833.61 782.662 L1837.98 782.662 L1837.98 786.226 L1833.61 786.226 Q1828.68 786.226 1826.8 784.398 Q1824.93 782.546 1824.93 777.685 L1824.93 763.611 L1821.8 763.611 L1821.8 760.301 L1824.93 760.301 L1824.93 752.94 L1829.21 752.94 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip530)\" style=\"stroke:#3da44d; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1365.51,820.786 1483.55,820.786 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip530)\" d=\"M1523.56 817.117 Q1525.16 814.247 1527.38 812.881 Q1529.61 811.516 1532.62 811.516 Q1536.67 811.516 1538.87 814.363 Q1541.06 817.187 1541.06 822.418 L1541.06 838.066 L1536.78 838.066 L1536.78 822.557 Q1536.78 818.83 1535.46 817.025 Q1534.14 815.219 1531.43 815.219 Q1528.12 815.219 1526.2 817.418 Q1524.28 819.617 1524.28 823.414 L1524.28 838.066 L1520 838.066 L1520 822.557 Q1520 818.807 1518.68 817.025 Q1517.36 815.219 1514.61 815.219 Q1511.34 815.219 1509.42 817.442 Q1507.5 819.641 1507.5 823.414 L1507.5 838.066 L1503.22 838.066 L1503.22 812.141 L1507.5 812.141 L1507.5 816.168 Q1508.96 813.784 1510.99 812.65 Q1513.03 811.516 1515.83 811.516 Q1518.66 811.516 1520.62 812.951 Q1522.62 814.386 1523.56 817.117 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1571.74 824.039 L1571.74 826.122 L1552.15 826.122 Q1552.43 830.52 1554.79 832.835 Q1557.18 835.127 1561.41 835.127 Q1563.86 835.127 1566.16 834.525 Q1568.47 833.923 1570.74 832.719 L1570.74 836.747 Q1568.45 837.719 1566.04 838.228 Q1563.63 838.738 1561.16 838.738 Q1554.95 838.738 1551.32 835.127 Q1547.71 831.516 1547.71 825.358 Q1547.71 818.992 1551.13 815.266 Q1554.58 811.516 1560.42 811.516 Q1565.65 811.516 1568.68 814.895 Q1571.74 818.252 1571.74 824.039 M1567.48 822.789 Q1567.43 819.293 1565.51 817.21 Q1563.61 815.127 1560.46 815.127 Q1556.9 815.127 1554.74 817.141 Q1552.62 819.154 1552.29 822.812 L1567.48 822.789 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1590.51 825.034 Q1585.35 825.034 1583.36 826.215 Q1581.36 827.395 1581.36 830.242 Q1581.36 832.511 1582.85 833.853 Q1584.35 835.173 1586.92 835.173 Q1590.46 835.173 1592.59 832.673 Q1594.74 830.15 1594.74 825.983 L1594.74 825.034 L1590.51 825.034 M1599 823.275 L1599 838.066 L1594.74 838.066 L1594.74 834.131 Q1593.29 836.492 1591.11 837.627 Q1588.93 838.738 1585.79 838.738 Q1581.8 838.738 1579.44 836.515 Q1577.11 834.27 1577.11 830.52 Q1577.11 826.145 1580.02 823.923 Q1582.96 821.701 1588.77 821.701 L1594.74 821.701 L1594.74 821.284 Q1594.74 818.344 1592.8 816.747 Q1590.88 815.127 1587.38 815.127 Q1585.16 815.127 1583.05 815.659 Q1580.95 816.192 1579 817.256 L1579 813.321 Q1581.34 812.418 1583.54 811.979 Q1585.74 811.516 1587.82 811.516 Q1593.45 811.516 1596.23 814.432 Q1599 817.349 1599 823.275 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1629.33 822.418 L1629.33 838.066 L1625.07 838.066 L1625.07 822.557 Q1625.07 818.877 1623.63 817.048 Q1622.2 815.219 1619.33 815.219 Q1615.88 815.219 1613.89 817.418 Q1611.9 819.617 1611.9 823.414 L1611.9 838.066 L1607.61 838.066 L1607.61 812.141 L1611.9 812.141 L1611.9 816.168 Q1613.42 813.83 1615.49 812.673 Q1617.57 811.516 1620.28 811.516 Q1624.74 811.516 1627.04 814.293 Q1629.33 817.048 1629.33 822.418 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1657.52 845.937 L1657.52 849.247 L1632.89 849.247 L1632.89 845.937 L1657.52 845.937 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1683.7 824.039 L1683.7 826.122 L1664.12 826.122 Q1664.4 830.52 1666.76 832.835 Q1669.14 835.127 1673.38 835.127 Q1675.83 835.127 1678.12 834.525 Q1680.44 833.923 1682.71 832.719 L1682.71 836.747 Q1680.42 837.719 1678.01 838.228 Q1675.6 838.738 1673.12 838.738 Q1666.92 838.738 1663.29 835.127 Q1659.67 831.516 1659.67 825.358 Q1659.67 818.992 1663.1 815.266 Q1666.55 811.516 1672.38 811.516 Q1677.61 811.516 1680.65 814.895 Q1683.7 818.252 1683.7 824.039 M1679.44 822.789 Q1679.4 819.293 1677.48 817.21 Q1675.58 815.127 1672.43 815.127 Q1668.86 815.127 1666.71 817.141 Q1664.58 819.154 1664.26 822.812 L1679.44 822.789 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1705.72 816.122 Q1705 815.705 1704.14 815.52 Q1703.31 815.312 1702.29 815.312 Q1698.68 815.312 1696.73 817.673 Q1694.81 820.011 1694.81 824.409 L1694.81 838.066 L1690.53 838.066 L1690.53 812.141 L1694.81 812.141 L1694.81 816.168 Q1696.16 813.807 1698.31 812.673 Q1700.46 811.516 1703.54 811.516 Q1703.98 811.516 1704.51 811.585 Q1705.04 811.631 1705.69 811.747 L1705.72 816.122 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1724.37 816.122 Q1723.66 815.705 1722.8 815.52 Q1721.97 815.312 1720.95 815.312 Q1717.34 815.312 1715.39 817.673 Q1713.47 820.011 1713.47 824.409 L1713.47 838.066 L1709.19 838.066 L1709.19 812.141 L1713.47 812.141 L1713.47 816.168 Q1714.81 813.807 1716.97 812.673 Q1719.12 811.516 1722.2 811.516 Q1722.64 811.516 1723.17 811.585 Q1723.7 811.631 1724.35 811.747 L1724.37 816.122 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1748.54 845.937 L1748.54 849.247 L1723.91 849.247 L1723.91 845.937 L1748.54 845.937 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1756.76 804.78 L1756.76 812.141 L1765.53 812.141 L1765.53 815.451 L1756.76 815.451 L1756.76 829.525 Q1756.76 832.696 1757.61 833.599 Q1758.49 834.502 1761.16 834.502 L1765.53 834.502 L1765.53 838.066 L1761.16 838.066 Q1756.23 838.066 1754.35 836.238 Q1752.48 834.386 1752.48 829.525 L1752.48 815.451 L1749.35 815.451 L1749.35 812.141 L1752.48 812.141 L1752.48 804.78 L1756.76 804.78 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1786.16 816.122 Q1785.44 815.705 1784.58 815.52 Q1783.75 815.312 1782.73 815.312 Q1779.12 815.312 1777.17 817.673 Q1775.25 820.011 1775.25 824.409 L1775.25 838.066 L1770.97 838.066 L1770.97 812.141 L1775.25 812.141 L1775.25 816.168 Q1776.6 813.807 1778.75 812.673 Q1780.9 811.516 1783.98 811.516 Q1784.42 811.516 1784.95 811.585 Q1785.48 811.631 1786.13 811.747 L1786.16 816.122 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1802.41 825.034 Q1797.24 825.034 1795.25 826.215 Q1793.26 827.395 1793.26 830.242 Q1793.26 832.511 1794.74 833.853 Q1796.25 835.173 1798.82 835.173 Q1802.36 835.173 1804.49 832.673 Q1806.64 830.15 1806.64 825.983 L1806.64 825.034 L1802.41 825.034 M1810.9 823.275 L1810.9 838.066 L1806.64 838.066 L1806.64 834.131 Q1805.18 836.492 1803.01 837.627 Q1800.83 838.738 1797.68 838.738 Q1793.7 838.738 1791.34 836.515 Q1789 834.27 1789 830.52 Q1789 826.145 1791.92 823.923 Q1794.86 821.701 1800.67 821.701 L1806.64 821.701 L1806.64 821.284 Q1806.64 818.344 1804.7 816.747 Q1802.78 815.127 1799.28 815.127 Q1797.06 815.127 1794.95 815.659 Q1792.85 816.192 1790.9 817.256 L1790.9 813.321 Q1793.24 812.418 1795.44 811.979 Q1797.64 811.516 1799.72 811.516 Q1805.35 811.516 1808.12 814.432 Q1810.9 817.349 1810.9 823.275 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1819.67 812.141 L1823.93 812.141 L1823.93 838.066 L1819.67 838.066 L1819.67 812.141 M1819.67 802.048 L1823.93 802.048 L1823.93 807.442 L1819.67 807.442 L1819.67 802.048 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1854.4 822.418 L1854.4 838.066 L1850.14 838.066 L1850.14 822.557 Q1850.14 818.877 1848.7 817.048 Q1847.27 815.219 1844.4 815.219 Q1840.95 815.219 1838.96 817.418 Q1836.97 819.617 1836.97 823.414 L1836.97 838.066 L1832.68 838.066 L1832.68 812.141 L1836.97 812.141 L1836.97 816.168 Q1838.49 813.83 1840.55 812.673 Q1842.64 811.516 1845.34 811.516 Q1849.81 811.516 1852.1 814.293 Q1854.4 817.048 1854.4 822.418 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = range(1, nr_of_epoch, step=1)\n",
    "\n",
    "p= Plots.plot(x,val_acc,label=\"val_acc\",legend=:bottomright, size=(500, 250))\n",
    "p= Plots.plot!(p,x,mean_err_test,label=\"mean_err_test\")\n",
    "p= Plots.plot!(p,x,mean_err_train,label=\"mean_err_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed97919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
