{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60615d67",
   "metadata": {},
   "source": [
    "# Dropout\n",
    ":label:`sec_dropout`\n",
    "\n",
    "\n",
    "Let's think briefly about what we\n",
    "expect from a good predictive model.\n",
    "We want it to peform well on unseen data.\n",
    "Classical generalization theory\n",
    "suggests that to close the gap between\n",
    "train and test performance,\n",
    "we should aim for a simple model.\n",
    "Simplicity can come in the form\n",
    "of a small number of dimensions.\n",
    "We explored this when discussing the\n",
    "monomial basis functions of linear models\n",
    "in :numref:`sec_generalization_basics`.\n",
    "Additionally, as we saw when discussing weight decay\n",
    "($\\ell_2$ regularization) in :numref:`sec_weight_decay`,\n",
    "the (inverse) norm of the parameters also\n",
    "represents a useful measure of simplicity.\n",
    "Another useful notion of simplicity is smoothness,\n",
    "i.e., that the function should not be sensitive\n",
    "to small changes to its inputs.\n",
    "For instance, when we classify images,\n",
    "we would expect that adding some random noise\n",
    "to the pixels should be mostly harmless.\n",
    "\n",
    "In 1995, Christopher Bishop formalized\n",
    "this idea when he proved that training with input noise\n",
    "is equivalent to Tikhonov regularization :cite:`Bishop.1995`.\n",
    "This work drew a clear mathematical connection\n",
    "between the requirement that a function be smooth (and thus simple),\n",
    "and the requirement that it be resilient\n",
    "to perturbations in the input.\n",
    "\n",
    "Then, in 2014, :citet:`Srivastava.Hinton.Krizhevsky.ea.2014`\n",
    "developed a clever idea for how to apply Bishop's idea\n",
    "to the internal layers of a network, too.\n",
    "Their idea, called *dropout*, involves\n",
    "injecting noise while computing\n",
    "each internal layer during forward propagation,\n",
    "and it has become a standard technique\n",
    "for training neural networks.\n",
    "The method is called *dropout* because we literally\n",
    "*drop out* some neurons during training.\n",
    "Throughout training, on each iteration,\n",
    "standard dropout consists of zeroing out\n",
    "some fraction of the nodes in each layer\n",
    "before calculating the subsequent layer.\n",
    "\n",
    "To be clear, we are imposing\n",
    "our own narrative with the link to Bishop.\n",
    "The original paper on dropout\n",
    "offers intuition through a surprising\n",
    "analogy to sexual reproduction.\n",
    "The authors argue that neural network overfitting\n",
    "is characterized by a state in which\n",
    "each layer relies on a specific\n",
    "pattern of activations in the previous layer,\n",
    "calling this condition *co-adaptation*.\n",
    "dropout, they claim, breaks up co-adaptation\n",
    "just as sexual reproduction is argued to\n",
    "break up co-adapted genes.\n",
    "While the explanatory of this theory is certainly up for debate,\n",
    "the dropout technique itself has proved enduring,\n",
    "and various forms of dropout are implemented\n",
    "in most deep learning libraries. \n",
    "\n",
    "\n",
    "The key challenge is how to inject this noise.\n",
    "One idea is to inject the noise in an *unbiased* manner\n",
    "so that the expected value of each layer---while fixing\n",
    "the others---equals to the value it would have taken absent noise.\n",
    "In Bishop's work, he added Gaussian noise\n",
    "to the inputs to a linear model.\n",
    "At each training iteration, he added noise\n",
    "sampled from a distribution with mean zero\n",
    "$\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$ to the input $\\mathbf{x}$,\n",
    "yielding a perturbed point $\\mathbf{x}' = \\mathbf{x} + \\epsilon$.\n",
    "In expectation, $E[\\mathbf{x}'] = \\mathbf{x}$.\n",
    "\n",
    "In standard dropout regularization,\n",
    "one zeros out some fraction of the nodes in each layer\n",
    "and then *debiases* each layer by normalizing\n",
    "by the fraction of nodes that were retained (not dropped out).\n",
    "In other words,\n",
    "with *dropout probability* $p$,\n",
    "each intermediate activation $h$ is replaced by\n",
    "a random variable $h'$ as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h' =\n",
    "\\begin{cases}\n",
    "    0 & \\text{ with probability } p \\\\\n",
    "    \\frac{h}{1-p} & \\text{ otherwise}\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "By design, the expectation remains unchanged, i.e., $E[h'] = h$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12b4c65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Flux,Plots,Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c680d69d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Dense(2 => 3),                        \u001b[90m# 9 parameters\u001b[39m\n",
       "  Dropout(0.4),\n",
       ") "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Flux,Statistics\n",
    "model = Chain(Dense(ones(3,2)), Dropout(0.4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe552fa3",
   "metadata": {},
   "source": [
    "## Dropout in Practice\n",
    "\n",
    "Recall the MLP with a hidden layer and 5 hidden units\n",
    "in :numref:`fig_mlp`.\n",
    "When we apply dropout to a hidden layer,\n",
    "zeroing out each hidden unit with probability $p$,\n",
    "the result can be viewed as a network\n",
    "containing only a subset of the original neurons.\n",
    "In :numref:`fig_dropout2`, $h_2$ and $h_5$ are removed.\n",
    "Consequently, the calculation of the outputs\n",
    "no longer depends on $h_2$ or $h_5$\n",
    "and their respective gradient also vanishes\n",
    "when performing backpropagation.\n",
    "In this way, the calculation of the output layer\n",
    "cannot be overly dependent on any\n",
    "one element of $h_1, \\ldots, h_5$.\n",
    "\n",
    "![MLP before and after dropout.](../img/dropout2.svg)\n",
    ":label:`fig_dropout2`\n",
    "\n",
    "Typically, we disable dropout at test time.\n",
    "Given a trained model and a new example,\n",
    "we do not drop out any nodes\n",
    "and thus do not need to normalize.\n",
    "However, there are some exceptions:\n",
    "some researchers use dropout at test time as a heuristic\n",
    "for estimating the *uncertainty* of neural network predictions:\n",
    "if the predictions agree across many different dropout masks,\n",
    "then we might say that the network is more confident.\n",
    "\n",
    "## Implementation from Scratch\n",
    "\n",
    "To implement the dropout function for a single layer,\n",
    "we must draw as many samples\n",
    "from a Bernoulli (binary) random variable\n",
    "as our layer has dimensions,\n",
    "where the random variable takes value $1$ (keep)\n",
    "with probability $1-p$ and $0$ (drop) with probability $p$.\n",
    "One easy way to implement this is to first draw samples\n",
    "from the uniform distribution $U[0, 1]$.\n",
    "Then we can keep those nodes for which the corresponding\n",
    "sample is greater than $p$, dropping the rest.\n",
    "\n",
    "In the following code, we (**implement a `dropout_layer` function\n",
    "that drops out the elements in the tensor input `X`\n",
    "with probability `dropout`**),\n",
    "rescaling the remainder as described above:\n",
    "dividing the survivors by `1.0-dropout`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da682e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×7 Matrix{Float64}:\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropout does not have effect in the test mode\n",
    "model(ones(2, 7))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b8d5d1",
   "metadata": {},
   "source": [
    "Dropout does not have effect in the test mode, but after training we can see that some nodes are 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bde16140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×7 Matrix{Float64}:\n",
       " 0.0      0.0      0.0      3.33333  3.33333  3.33333  0.0\n",
       " 0.0      0.0      3.33333  3.33333  3.33333  0.0      3.33333\n",
       " 3.33333  3.33333  3.33333  3.33333  0.0      3.33333  0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Flux.trainmode!(model);  \n",
    "model(ones(2, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c685e69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model(ones(2, 10_000));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8942fd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.99811111111111"
     ]
    }
   ],
   "source": [
    "using Statistics\n",
    "# The dropout does not make a big change on the mean\n",
    "print(mean(y))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20dc0a6",
   "metadata": {},
   "source": [
    "### Using dropout in the previous MNIST problem \n",
    "\n",
    "### Defining the Model\n",
    "\n",
    "The model below applies dropout to the output\n",
    "of each hidden layer (following the activation function).\n",
    "We can set dropout probabilities for each layer separately.\n",
    "A common trend is to set\n",
    "a lower dropout probability closer to the input layer.\n",
    "We ensure that dropout is only active during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b158f86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Dense(784 => 200, relu),              \u001b[90m# 157_000 parameters\u001b[39m\n",
       "  Dropout(0.5),\n",
       "  Dense(200 => 200, relu),              \u001b[90m# 40_200 parameters\u001b[39m\n",
       "  Dropout(0.5),\n",
       "  Dense(200 => 10),                     \u001b[90m# 2_010 parameters\u001b[39m\n",
       "  NNlib.softmax,\n",
       ") \u001b[90m                  # Total: 6 arrays, \u001b[39m199_210 parameters, 778.602 KiB."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Flux\n",
    "model = Chain(\n",
    "  Dense(28*28,200, relu),Dropout(0.5),\n",
    "  Dense(200,200,relu),Dropout(0.5),\n",
    "  Dense(200, 10),\n",
    "  softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a193bbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: MNIST.traindata() is deprecated, use `MNIST(split=:train)[:]` instead.\n",
      "└ @ MLDatasets C:\\Users\\Szasz\\.julia\\packages\\MLDatasets\\bg0uc\\src\\datasets\\vision\\mnist.jl:187\n",
      "┌ Warning: MNIST.testdata() is deprecated, use `MNIST(split=:test)[:]` instead.\n",
      "└ @ MLDatasets C:\\Users\\Szasz\\.julia\\packages\\MLDatasets\\bg0uc\\src\\datasets\\vision\\mnist.jl:195\n"
     ]
    }
   ],
   "source": [
    "using MLDatasets\n",
    "\n",
    "# load training set\n",
    "train_x, train_y = MNIST.traindata();\n",
    "\n",
    "# load test set\n",
    "test_x,  test_y  = MNIST.testdata();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed01ba76",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = Float32.(train_x)\n",
    "y_train = Flux.onehotbatch(train_y, 0:9);\n",
    "\n",
    "x_test = Float32.(test_x)\n",
    "y_test = Flux.onehotbatch(test_y, 0:9);\n",
    "\n",
    "loss(X, y) = Flux.crossentropy(model(X), y)\n",
    "opt = Flux.ADAM();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f424b9",
   "metadata": {},
   "source": [
    "### [**Training**]\n",
    "\n",
    "The following is similar to the training of MLPs described previously.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82aa1bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Statistics\n",
    "\n",
    "function calculate_accuracy_test()\n",
    "    test_data = [(Flux.flatten(x_test), test_y)]\n",
    "    accuracy = 0\n",
    "    err=0;\n",
    "    nr_of_errors=0\n",
    "    for i in 1:length(test_y)\n",
    "        if findmax(model(test_data[1][1][:, i]))[2] - 1  == test_y[i]\n",
    "            accuracy = accuracy + 1\n",
    "        else\n",
    "            nr_of_errors=nr_of_errors+1\n",
    "            A=abs.(model(test_data[1][1][:, i]) .- y_test[:,i])\n",
    "            aux=0\n",
    "            for a in A\n",
    "                aux=aux+a\n",
    "            end\n",
    "            err=err+aux;\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    m = accuracy / length(test_y)\n",
    "    n = err/ length(test_y)\n",
    "    s = nr_of_errors \n",
    "    println(\"The val_accuracy is: \",m)\n",
    "    println(\"Mean absolute error (TEST): \",n)\n",
    "    println(\"Number of errors: \",s)\n",
    "    return m,n,s\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d9918e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with this function we'll calculate the accuracy in every epoch\n",
    "using Statistics\n",
    "\n",
    "function calculate_accuracy_train()\n",
    "    train_data = [(Flux.flatten(x_train), train_y)]\n",
    "    err=0;\n",
    "    for i in 1:length(train_y)\n",
    "        if findmax(model(train_data[1][1][:, i]))[2] - 1  != train_y[i]\n",
    "            A=abs.(model(train_data[1][1][:, i]) .- y_train[:,i])\n",
    "            aux=0\n",
    "            for a in A\n",
    "                aux=aux+a\n",
    "            end\n",
    "            err=err+aux;\n",
    "        end\n",
    "    end    \n",
    "    n = err/ length(train_y)\n",
    "    println(\"Mean absolute error (TRAIN): \",n)\n",
    "    return n\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2adb7c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "Mean absolute error (TRAIN): 1.4040046\n",
      "The val_accuracy is: 0.2259\n",
      "Mean absolute error (TEST): 1.3896588\n",
      "Number of errors: 7741\n",
      "epoch 2\n",
      "Mean absolute error (TRAIN): 1.0660497\n",
      "The val_accuracy is: 0.413\n",
      "Mean absolute error (TEST): 1.0462993\n",
      "Number of errors: 5870\n",
      "epoch 3\n",
      "Mean absolute error (TRAIN): 0.7766581\n",
      "The val_accuracy is: 0.5655\n",
      "Mean absolute error (TEST): 0.7701285\n",
      "Number of errors: 4345\n",
      "epoch 4\n",
      "Mean absolute error (TRAIN): 0.6594505\n",
      "The val_accuracy is: 0.6283\n",
      "Mean absolute error (TEST): 0.65415174\n",
      "Number of errors: 3717\n",
      "epoch 5\n",
      "Mean absolute error (TRAIN): 0.59138894\n",
      "The val_accuracy is: 0.6658\n",
      "Mean absolute error (TEST): 0.58365583\n",
      "Number of errors: 3342\n",
      "epoch 6\n",
      "Mean absolute error (TRAIN): 0.5424026\n",
      "The val_accuracy is: 0.6955\n",
      "Mean absolute error (TEST): 0.52774143\n",
      "Number of errors: 3045\n",
      "epoch 7\n",
      "Mean absolute error (TRAIN): 0.49865982\n",
      "The val_accuracy is: 0.7178\n",
      "Mean absolute error (TEST): 0.4850984\n",
      "Number of errors: 2822\n",
      "epoch 8\n",
      "Mean absolute error (TRAIN): 0.458038\n",
      "The val_accuracy is: 0.7411\n",
      "Mean absolute error (TEST): 0.44153398\n",
      "Number of errors: 2589\n"
     ]
    }
   ],
   "source": [
    "using Flux\n",
    "\n",
    "parameters = Flux.params(model)\n",
    "# flatten() function converts array 28x28x60000 into 784x60000 \n",
    "train_data = [(Flux.flatten(x_train), Flux.flatten(y_train))]\n",
    "\n",
    "val_acc=zeros(0)\n",
    "mean_err_test=zeros(0)\n",
    "nr_err=zeros(0)\n",
    "mean_err_train=zeros(0)\n",
    "nr_of_epoch=8\n",
    "\n",
    "for i in 1:nr_of_epoch\n",
    "    println(\"epoch \",i)\n",
    "    Flux.train!(loss, parameters, train_data, opt)\n",
    "    \n",
    "    n = calculate_accuracy_train()\n",
    "    append!(mean_err_train,n)\n",
    "    m, n, s = calculate_accuracy_test()\n",
    "    append!( val_acc, m )\n",
    "    append!( mean_err_test, n )\n",
    "    append!( nr_err, s )\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1098cfff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"500\" height=\"250\" viewBox=\"0 0 2000 1000\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip980\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2000\" height=\"1000\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip980)\" d=\"\n",
       "M0 1000 L2000 1000 L2000 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip981\">\n",
       "    <rect x=\"400\" y=\"0\" width=\"1401\" height=\"1000\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip980)\" d=\"\n",
       "M182.274 901.088 L1952.76 901.088 L1952.76 47.2441 L182.274 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip982\">\n",
       "    <rect x=\"182\" y=\"47\" width=\"1771\" height=\"855\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip982)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  470.991,901.088 470.991,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip982)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  948.21,901.088 948.21,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip982)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1425.43,901.088 1425.43,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip982)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1902.65,901.088 1902.65,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip980)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  182.274,901.088 1952.76,901.088 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip980)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  470.991,901.088 470.991,882.19 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip980)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  948.21,901.088 948.21,882.19 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip980)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1425.43,901.088 1425.43,882.19 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip980)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1902.65,901.088 1902.65,882.19 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip980)\" d=\"M465.644 956.353 L481.964 956.353 L481.964 960.288 L460.019 960.288 L460.019 956.353 Q462.681 953.598 467.265 948.968 Q471.871 944.316 473.052 942.973 Q475.297 940.45 476.177 938.714 Q477.079 936.955 477.079 935.265 Q477.079 932.51 475.135 930.774 Q473.214 929.038 470.112 929.038 Q467.913 929.038 465.459 929.802 Q463.028 930.566 460.251 932.117 L460.251 927.395 Q463.075 926.26 465.528 925.682 Q467.982 925.103 470.019 925.103 Q475.39 925.103 478.584 927.788 Q481.778 930.473 481.778 934.964 Q481.778 937.094 480.968 939.015 Q480.181 940.913 478.075 943.506 Q477.496 944.177 474.394 947.394 Q471.292 950.589 465.644 956.353 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M951.219 929.802 L939.414 948.251 L951.219 948.251 L951.219 929.802 M949.993 925.728 L955.872 925.728 L955.872 948.251 L960.803 948.251 L960.803 952.14 L955.872 952.14 L955.872 960.288 L951.219 960.288 L951.219 952.14 L935.618 952.14 L935.618 947.626 L949.993 925.728 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M1425.83 941.144 Q1422.69 941.144 1420.83 943.297 Q1419.01 945.45 1419.01 949.2 Q1419.01 952.927 1420.83 955.103 Q1422.69 957.255 1425.83 957.255 Q1428.98 957.255 1430.81 955.103 Q1432.66 952.927 1432.66 949.2 Q1432.66 945.45 1430.81 943.297 Q1428.98 941.144 1425.83 941.144 M1435.12 926.492 L1435.12 930.751 Q1433.36 929.918 1431.55 929.478 Q1429.77 929.038 1428.01 929.038 Q1423.38 929.038 1420.93 932.163 Q1418.5 935.288 1418.15 941.607 Q1419.51 939.594 1421.57 938.529 Q1423.64 937.441 1426.11 937.441 Q1431.32 937.441 1434.33 940.612 Q1437.36 943.76 1437.36 949.2 Q1437.36 954.524 1434.21 957.742 Q1431.07 960.959 1425.83 960.959 Q1419.84 960.959 1416.67 956.376 Q1413.5 951.769 1413.5 943.043 Q1413.5 934.848 1417.39 929.987 Q1421.27 925.103 1427.82 925.103 Q1429.58 925.103 1431.37 925.45 Q1433.17 925.797 1435.12 926.492 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M1902.65 943.876 Q1899.31 943.876 1897.39 945.658 Q1895.5 947.441 1895.5 950.566 Q1895.5 953.691 1897.39 955.473 Q1899.31 957.255 1902.65 957.255 Q1905.98 957.255 1907.9 955.473 Q1909.82 953.668 1909.82 950.566 Q1909.82 947.441 1907.9 945.658 Q1906 943.876 1902.65 943.876 M1897.97 941.885 Q1894.96 941.144 1893.27 939.084 Q1891.61 937.024 1891.61 934.061 Q1891.61 929.918 1894.55 927.51 Q1897.51 925.103 1902.65 925.103 Q1907.81 925.103 1910.75 927.51 Q1913.69 929.918 1913.69 934.061 Q1913.69 937.024 1912 939.084 Q1910.33 941.144 1907.35 941.885 Q1910.73 942.672 1912.6 944.964 Q1914.5 947.256 1914.5 950.566 Q1914.5 955.589 1911.42 958.274 Q1908.37 960.959 1902.65 960.959 Q1896.93 960.959 1893.85 958.274 Q1890.8 955.589 1890.8 950.566 Q1890.8 947.256 1892.69 944.964 Q1894.59 942.672 1897.97 941.885 M1896.26 934.501 Q1896.26 937.186 1897.93 938.691 Q1899.62 940.195 1902.65 940.195 Q1905.66 940.195 1907.35 938.691 Q1909.06 937.186 1909.06 934.501 Q1909.06 931.816 1907.35 930.311 Q1905.66 928.807 1902.65 928.807 Q1899.62 928.807 1897.93 930.311 Q1896.26 931.816 1896.26 934.501 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip982)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  182.274,860.444 1952.76,860.444 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip982)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  182.274,689.51 1952.76,689.51 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip982)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  182.274,518.576 1952.76,518.576 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip982)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  182.274,347.642 1952.76,347.642 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip982)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  182.274,176.708 1952.76,176.708 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip980)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  182.274,901.088 182.274,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip980)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  182.274,860.444 201.172,860.444 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip980)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  182.274,689.51 201.172,689.51 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip980)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  182.274,518.576 201.172,518.576 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip980)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  182.274,347.642 201.172,347.642 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip980)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  182.274,176.708 201.172,176.708 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip980)\" d=\"M65.9319 846.243 Q62.3208 846.243 60.4921 849.808 Q58.6865 853.35 58.6865 860.479 Q58.6865 867.586 60.4921 871.15 Q62.3208 874.692 65.9319 874.692 Q69.5661 874.692 71.3717 871.15 Q73.2004 867.586 73.2004 860.479 Q73.2004 853.35 71.3717 849.808 Q69.5661 846.243 65.9319 846.243 M65.9319 842.539 Q71.742 842.539 74.7976 847.146 Q77.8763 851.729 77.8763 860.479 Q77.8763 869.206 74.7976 873.812 Q71.742 878.396 65.9319 878.396 Q60.1217 878.396 57.043 873.812 Q53.9875 869.206 53.9875 860.479 Q53.9875 851.729 57.043 847.146 Q60.1217 842.539 65.9319 842.539 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M86.0938 871.845 L90.978 871.845 L90.978 877.724 L86.0938 877.724 L86.0938 871.845 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M105.191 873.789 L121.51 873.789 L121.51 877.724 L99.566 877.724 L99.566 873.789 Q102.228 871.035 106.811 866.405 Q111.418 861.752 112.598 860.41 Q114.844 857.887 115.723 856.15 Q116.626 854.391 116.626 852.701 Q116.626 849.947 114.682 848.211 Q112.76 846.475 109.658 846.475 Q107.459 846.475 105.006 847.238 Q102.575 848.002 99.7974 849.553 L99.7974 844.831 Q102.621 843.697 105.075 843.118 Q107.529 842.539 109.566 842.539 Q114.936 842.539 118.131 845.225 Q121.325 847.91 121.325 852.4 Q121.325 854.53 120.515 856.451 Q119.728 858.35 117.621 860.942 Q117.043 861.613 113.941 864.831 Q110.839 868.025 105.191 873.789 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M131.371 843.164 L149.728 843.164 L149.728 847.1 L135.654 847.1 L135.654 855.572 Q136.672 855.225 137.691 855.063 Q138.709 854.877 139.728 854.877 Q145.515 854.877 148.894 858.049 Q152.274 861.22 152.274 866.637 Q152.274 872.215 148.802 875.317 Q145.33 878.396 139.01 878.396 Q136.834 878.396 134.566 878.025 Q132.32 877.655 129.913 876.914 L129.913 872.215 Q131.996 873.349 134.219 873.905 Q136.441 874.461 138.918 874.461 Q142.922 874.461 145.26 872.354 Q147.598 870.248 147.598 866.637 Q147.598 863.025 145.26 860.919 Q142.922 858.813 138.918 858.813 Q137.043 858.813 135.168 859.229 Q133.316 859.646 131.371 860.525 L131.371 843.164 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M64.9365 675.309 Q61.3254 675.309 59.4967 678.874 Q57.6912 682.415 57.6912 689.545 Q57.6912 696.651 59.4967 700.216 Q61.3254 703.758 64.9365 703.758 Q68.5707 703.758 70.3763 700.216 Q72.205 696.651 72.205 689.545 Q72.205 682.415 70.3763 678.874 Q68.5707 675.309 64.9365 675.309 M64.9365 671.605 Q70.7467 671.605 73.8022 676.212 Q76.8809 680.795 76.8809 689.545 Q76.8809 698.272 73.8022 702.878 Q70.7467 707.462 64.9365 707.462 Q59.1264 707.462 56.0477 702.878 Q52.9921 698.272 52.9921 689.545 Q52.9921 680.795 56.0477 676.212 Q59.1264 671.605 64.9365 671.605 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M85.0984 700.911 L89.9827 700.911 L89.9827 706.79 L85.0984 706.79 L85.0984 700.911 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M100.214 672.23 L118.57 672.23 L118.57 676.165 L104.496 676.165 L104.496 684.638 Q105.515 684.29 106.534 684.128 Q107.552 683.943 108.571 683.943 Q114.358 683.943 117.737 687.115 Q121.117 690.286 121.117 695.702 Q121.117 701.281 117.645 704.383 Q114.172 707.462 107.853 707.462 Q105.677 707.462 103.409 707.091 Q101.163 706.721 98.7558 705.98 L98.7558 701.281 Q100.839 702.415 103.061 702.971 Q105.284 703.526 107.76 703.526 Q111.765 703.526 114.103 701.42 Q116.441 699.314 116.441 695.702 Q116.441 692.091 114.103 689.985 Q111.765 687.878 107.76 687.878 Q105.885 687.878 104.01 688.295 Q102.159 688.712 100.214 689.591 L100.214 672.23 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M140.33 675.309 Q136.719 675.309 134.89 678.874 Q133.084 682.415 133.084 689.545 Q133.084 696.651 134.89 700.216 Q136.719 703.758 140.33 703.758 Q143.964 703.758 145.769 700.216 Q147.598 696.651 147.598 689.545 Q147.598 682.415 145.769 678.874 Q143.964 675.309 140.33 675.309 M140.33 671.605 Q146.14 671.605 149.195 676.212 Q152.274 680.795 152.274 689.545 Q152.274 698.272 149.195 702.878 Q146.14 707.462 140.33 707.462 Q134.519 707.462 131.441 702.878 Q128.385 698.272 128.385 689.545 Q128.385 680.795 131.441 676.212 Q134.519 671.605 140.33 671.605 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M65.9319 504.375 Q62.3208 504.375 60.4921 507.94 Q58.6865 511.481 58.6865 518.611 Q58.6865 525.717 60.4921 529.282 Q62.3208 532.824 65.9319 532.824 Q69.5661 532.824 71.3717 529.282 Q73.2004 525.717 73.2004 518.611 Q73.2004 511.481 71.3717 507.94 Q69.5661 504.375 65.9319 504.375 M65.9319 500.671 Q71.742 500.671 74.7976 505.278 Q77.8763 509.861 77.8763 518.611 Q77.8763 527.338 74.7976 531.944 Q71.742 536.528 65.9319 536.528 Q60.1217 536.528 57.043 531.944 Q53.9875 527.338 53.9875 518.611 Q53.9875 509.861 57.043 505.278 Q60.1217 500.671 65.9319 500.671 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M86.0938 529.977 L90.978 529.977 L90.978 535.856 L86.0938 535.856 L86.0938 529.977 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M99.9826 501.296 L122.205 501.296 L122.205 503.287 L109.658 535.856 L104.774 535.856 L116.58 505.231 L99.9826 505.231 L99.9826 501.296 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M131.371 501.296 L149.728 501.296 L149.728 505.231 L135.654 505.231 L135.654 513.704 Q136.672 513.356 137.691 513.194 Q138.709 513.009 139.728 513.009 Q145.515 513.009 148.894 516.18 Q152.274 519.352 152.274 524.768 Q152.274 530.347 148.802 533.449 Q145.33 536.528 139.01 536.528 Q136.834 536.528 134.566 536.157 Q132.32 535.787 129.913 535.046 L129.913 530.347 Q131.996 531.481 134.219 532.037 Q136.441 532.592 138.918 532.592 Q142.922 532.592 145.26 530.486 Q147.598 528.379 147.598 524.768 Q147.598 521.157 145.26 519.051 Q142.922 516.944 138.918 516.944 Q137.043 516.944 135.168 517.361 Q133.316 517.778 131.371 518.657 L131.371 501.296 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M55.7467 360.987 L63.3856 360.987 L63.3856 334.621 L55.0754 336.288 L55.0754 332.029 L63.3393 330.362 L68.0152 330.362 L68.0152 360.987 L75.654 360.987 L75.654 364.922 L55.7467 364.922 L55.7467 360.987 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M85.0984 359.043 L89.9827 359.043 L89.9827 364.922 L85.0984 364.922 L85.0984 359.043 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M110.168 333.441 Q106.557 333.441 104.728 337.006 Q102.922 340.547 102.922 347.677 Q102.922 354.783 104.728 358.348 Q106.557 361.89 110.168 361.89 Q113.802 361.89 115.608 358.348 Q117.436 354.783 117.436 347.677 Q117.436 340.547 115.608 337.006 Q113.802 333.441 110.168 333.441 M110.168 329.737 Q115.978 329.737 119.033 334.344 Q122.112 338.927 122.112 347.677 Q122.112 356.404 119.033 361.01 Q115.978 365.593 110.168 365.593 Q104.358 365.593 101.279 361.01 Q98.2234 356.404 98.2234 347.677 Q98.2234 338.927 101.279 334.344 Q104.358 329.737 110.168 329.737 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M140.33 333.441 Q136.719 333.441 134.89 337.006 Q133.084 340.547 133.084 347.677 Q133.084 354.783 134.89 358.348 Q136.719 361.89 140.33 361.89 Q143.964 361.89 145.769 358.348 Q147.598 354.783 147.598 347.677 Q147.598 340.547 145.769 337.006 Q143.964 333.441 140.33 333.441 M140.33 329.737 Q146.14 329.737 149.195 334.344 Q152.274 338.927 152.274 347.677 Q152.274 356.404 149.195 361.01 Q146.14 365.593 140.33 365.593 Q134.519 365.593 131.441 361.01 Q128.385 356.404 128.385 347.677 Q128.385 338.927 131.441 334.344 Q134.519 329.737 140.33 329.737 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M56.7421 190.053 L64.381 190.053 L64.381 163.687 L56.0708 165.354 L56.0708 161.095 L64.3347 159.428 L69.0106 159.428 L69.0106 190.053 L76.6494 190.053 L76.6494 193.988 L56.7421 193.988 L56.7421 190.053 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M86.0938 188.108 L90.978 188.108 L90.978 193.988 L86.0938 193.988 L86.0938 188.108 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M105.191 190.053 L121.51 190.053 L121.51 193.988 L99.566 193.988 L99.566 190.053 Q102.228 187.298 106.811 182.669 Q111.418 178.016 112.598 176.673 Q114.844 174.15 115.723 172.414 Q116.626 170.655 116.626 168.965 Q116.626 166.21 114.682 164.474 Q112.76 162.738 109.658 162.738 Q107.459 162.738 105.006 163.502 Q102.575 164.266 99.7974 165.817 L99.7974 161.095 Q102.621 159.96 105.075 159.382 Q107.529 158.803 109.566 158.803 Q114.936 158.803 118.131 161.488 Q121.325 164.173 121.325 168.664 Q121.325 170.794 120.515 172.715 Q119.728 174.613 117.621 177.206 Q117.043 177.877 113.941 181.095 Q110.839 184.289 105.191 190.053 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M131.371 159.428 L149.728 159.428 L149.728 163.363 L135.654 163.363 L135.654 171.835 Q136.672 171.488 137.691 171.326 Q138.709 171.141 139.728 171.141 Q145.515 171.141 148.894 174.312 Q152.274 177.483 152.274 182.9 Q152.274 188.479 148.802 191.581 Q145.33 194.659 139.01 194.659 Q136.834 194.659 134.566 194.289 Q132.32 193.919 129.913 193.178 L129.913 188.479 Q131.996 189.613 134.219 190.169 Q136.441 190.724 138.918 190.724 Q142.922 190.724 145.26 188.618 Q147.598 186.511 147.598 182.9 Q147.598 179.289 145.26 177.183 Q142.922 175.076 138.918 175.076 Q137.043 175.076 135.168 175.493 Q133.316 175.909 131.371 176.789 L131.371 159.428 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip982)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  232.382,876.922 470.991,748.995 709.601,644.726 948.21,601.787 1186.82,576.147 1425.43,555.84 1664.04,540.593 1902.65,524.661 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip982)\" style=\"stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  232.382,81.2182 470.991,315.986 709.601,504.814 948.21,584.111 1186.82,632.312 1425.43,670.543 1664.04,699.699 1902.65,729.486 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip982)\" style=\"stroke:#3da44d; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  232.382,71.4095 470.991,302.482 709.601,500.349 948.21,580.488 1186.82,627.024 1425.43,660.518 1664.04,690.427 1902.65,718.201 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip980)\" d=\"\n",
       "M1345.84 872.626 L1893.74 872.626 L1893.74 665.266 L1345.84 665.266  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip980)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1345.84,872.626 1893.74,872.626 1893.74,665.266 1345.84,665.266 1345.84,872.626 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip980)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1365.51,717.106 1483.55,717.106 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip980)\" d=\"M1503.22 708.461 L1507.73 708.461 L1515.83 730.22 L1523.93 708.461 L1528.45 708.461 L1518.73 734.386 L1512.94 734.386 L1503.22 708.461 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M1546.11 721.354 Q1540.95 721.354 1538.96 722.535 Q1536.97 723.715 1536.97 726.562 Q1536.97 728.831 1538.45 730.173 Q1539.95 731.493 1542.52 731.493 Q1546.06 731.493 1548.19 728.993 Q1550.35 726.47 1550.35 722.303 L1550.35 721.354 L1546.11 721.354 M1554.61 719.595 L1554.61 734.386 L1550.35 734.386 L1550.35 730.451 Q1548.89 732.812 1546.71 733.947 Q1544.54 735.058 1541.39 735.058 Q1537.41 735.058 1535.05 732.835 Q1532.71 730.59 1532.71 726.84 Q1532.71 722.465 1535.62 720.243 Q1538.56 718.021 1544.37 718.021 L1550.35 718.021 L1550.35 717.604 Q1550.35 714.664 1548.4 713.067 Q1546.48 711.447 1542.99 711.447 Q1540.76 711.447 1538.66 711.979 Q1536.55 712.512 1534.61 713.576 L1534.61 709.641 Q1536.94 708.738 1539.14 708.299 Q1541.34 707.836 1543.43 707.836 Q1549.05 707.836 1551.83 710.752 Q1554.61 713.669 1554.61 719.595 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M1563.38 698.368 L1567.64 698.368 L1567.64 734.386 L1563.38 734.386 L1563.38 698.368 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M1596.25 742.257 L1596.25 745.567 L1571.62 745.567 L1571.62 742.257 L1596.25 742.257 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M1612.04 721.354 Q1606.87 721.354 1604.88 722.535 Q1602.89 723.715 1602.89 726.562 Q1602.89 728.831 1604.37 730.173 Q1605.88 731.493 1608.45 731.493 Q1611.99 731.493 1614.12 728.993 Q1616.27 726.47 1616.27 722.303 L1616.27 721.354 L1612.04 721.354 M1620.53 719.595 L1620.53 734.386 L1616.27 734.386 L1616.27 730.451 Q1614.81 732.812 1612.64 733.947 Q1610.46 735.058 1607.31 735.058 Q1603.33 735.058 1600.97 732.835 Q1598.63 730.59 1598.63 726.84 Q1598.63 722.465 1601.55 720.243 Q1604.49 718.021 1610.3 718.021 L1616.27 718.021 L1616.27 717.604 Q1616.27 714.664 1614.33 713.067 Q1612.41 711.447 1608.91 711.447 Q1606.69 711.447 1604.58 711.979 Q1602.48 712.512 1600.53 713.576 L1600.53 709.641 Q1602.87 708.738 1605.07 708.299 Q1607.27 707.836 1609.35 707.836 Q1614.98 707.836 1617.75 710.752 Q1620.53 713.669 1620.53 719.595 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M1647.96 709.456 L1647.96 713.437 Q1646.16 712.442 1644.33 711.956 Q1642.52 711.447 1640.67 711.447 Q1636.53 711.447 1634.23 714.086 Q1631.94 716.701 1631.94 721.447 Q1631.94 726.192 1634.23 728.831 Q1636.53 731.447 1640.67 731.447 Q1642.52 731.447 1644.33 730.961 Q1646.16 730.451 1647.96 729.456 L1647.96 733.391 Q1646.18 734.224 1644.26 734.641 Q1642.36 735.058 1640.21 735.058 Q1634.35 735.058 1630.9 731.377 Q1627.45 727.697 1627.45 721.447 Q1627.45 715.104 1630.92 711.47 Q1634.42 707.836 1640.48 707.836 Q1642.45 707.836 1644.33 708.252 Q1646.2 708.646 1647.96 709.456 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M1674.03 709.456 L1674.03 713.437 Q1672.22 712.442 1670.39 711.956 Q1668.59 711.447 1666.73 711.447 Q1662.59 711.447 1660.3 714.086 Q1658.01 716.701 1658.01 721.447 Q1658.01 726.192 1660.3 728.831 Q1662.59 731.447 1666.73 731.447 Q1668.59 731.447 1670.39 730.961 Q1672.22 730.451 1674.03 729.456 L1674.03 733.391 Q1672.24 734.224 1670.32 734.641 Q1668.42 735.058 1666.27 735.058 Q1660.42 735.058 1656.97 731.377 Q1653.52 727.697 1653.52 721.447 Q1653.52 715.104 1656.99 711.47 Q1660.48 707.836 1666.55 707.836 Q1668.52 707.836 1670.39 708.252 Q1672.27 708.646 1674.03 709.456 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip980)\" style=\"stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1365.51,768.946 1483.55,768.946 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip980)\" d=\"M1523.56 765.277 Q1525.16 762.407 1527.38 761.041 Q1529.61 759.676 1532.62 759.676 Q1536.67 759.676 1538.87 762.523 Q1541.06 765.347 1541.06 770.578 L1541.06 786.226 L1536.78 786.226 L1536.78 770.717 Q1536.78 766.99 1535.46 765.185 Q1534.14 763.379 1531.43 763.379 Q1528.12 763.379 1526.2 765.578 Q1524.28 767.777 1524.28 771.574 L1524.28 786.226 L1520 786.226 L1520 770.717 Q1520 766.967 1518.68 765.185 Q1517.36 763.379 1514.61 763.379 Q1511.34 763.379 1509.42 765.602 Q1507.5 767.801 1507.5 771.574 L1507.5 786.226 L1503.22 786.226 L1503.22 760.301 L1507.5 760.301 L1507.5 764.328 Q1508.96 761.944 1510.99 760.81 Q1513.03 759.676 1515.83 759.676 Q1518.66 759.676 1520.62 761.111 Q1522.62 762.546 1523.56 765.277 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M1571.74 772.199 L1571.74 774.282 L1552.15 774.282 Q1552.43 778.68 1554.79 780.995 Q1557.18 783.287 1561.41 783.287 Q1563.86 783.287 1566.16 782.685 Q1568.47 782.083 1570.74 780.879 L1570.74 784.907 Q1568.45 785.879 1566.04 786.388 Q1563.63 786.898 1561.16 786.898 Q1554.95 786.898 1551.32 783.287 Q1547.71 779.676 1547.71 773.518 Q1547.71 767.152 1551.13 763.426 Q1554.58 759.676 1560.42 759.676 Q1565.65 759.676 1568.68 763.055 Q1571.74 766.412 1571.74 772.199 M1567.48 770.949 Q1567.43 767.453 1565.51 765.37 Q1563.61 763.287 1560.46 763.287 Q1556.9 763.287 1554.74 765.301 Q1552.62 767.314 1552.29 770.972 L1567.48 770.949 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M1590.51 773.194 Q1585.35 773.194 1583.36 774.375 Q1581.36 775.555 1581.36 778.402 Q1581.36 780.671 1582.85 782.013 Q1584.35 783.333 1586.92 783.333 Q1590.46 783.333 1592.59 780.833 Q1594.74 778.31 1594.74 774.143 L1594.74 773.194 L1590.51 773.194 M1599 771.435 L1599 786.226 L1594.74 786.226 L1594.74 782.291 Q1593.29 784.652 1591.11 785.787 Q1588.93 786.898 1585.79 786.898 Q1581.8 786.898 1579.44 784.675 Q1577.11 782.43 1577.11 778.68 Q1577.11 774.305 1580.02 772.083 Q1582.96 769.861 1588.77 769.861 L1594.74 769.861 L1594.74 769.444 Q1594.74 766.504 1592.8 764.907 Q1590.88 763.287 1587.38 763.287 Q1585.16 763.287 1583.05 763.819 Q1580.95 764.352 1579 765.416 L1579 761.481 Q1581.34 760.578 1583.54 760.139 Q1585.74 759.676 1587.82 759.676 Q1593.45 759.676 1596.23 762.592 Q1599 765.509 1599 771.435 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M1629.33 770.578 L1629.33 786.226 L1625.07 786.226 L1625.07 770.717 Q1625.07 767.037 1623.63 765.208 Q1622.2 763.379 1619.33 763.379 Q1615.88 763.379 1613.89 765.578 Q1611.9 767.777 1611.9 771.574 L1611.9 786.226 L1607.61 786.226 L1607.61 760.301 L1611.9 760.301 L1611.9 764.328 Q1613.42 761.99 1615.49 760.833 Q1617.57 759.676 1620.28 759.676 Q1624.74 759.676 1627.04 762.453 Q1629.33 765.208 1629.33 770.578 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M1657.52 794.097 L1657.52 797.407 L1632.89 797.407 L1632.89 794.097 L1657.52 794.097 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M1683.7 772.199 L1683.7 774.282 L1664.12 774.282 Q1664.4 778.68 1666.76 780.995 Q1669.14 783.287 1673.38 783.287 Q1675.83 783.287 1678.12 782.685 Q1680.44 782.083 1682.71 780.879 L1682.71 784.907 Q1680.42 785.879 1678.01 786.388 Q1675.6 786.898 1673.12 786.898 Q1666.92 786.898 1663.29 783.287 Q1659.67 779.676 1659.67 773.518 Q1659.67 767.152 1663.1 763.426 Q1666.55 759.676 1672.38 759.676 Q1677.61 759.676 1680.65 763.055 Q1683.7 766.412 1683.7 772.199 M1679.44 770.949 Q1679.4 767.453 1677.48 765.37 Q1675.58 763.287 1672.43 763.287 Q1668.86 763.287 1666.71 765.301 Q1664.58 767.314 1664.26 770.972 L1679.44 770.949 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M1705.72 764.282 Q1705 763.865 1704.14 763.68 Q1703.31 763.472 1702.29 763.472 Q1698.68 763.472 1696.73 765.833 Q1694.81 768.171 1694.81 772.569 L1694.81 786.226 L1690.53 786.226 L1690.53 760.301 L1694.81 760.301 L1694.81 764.328 Q1696.16 761.967 1698.31 760.833 Q1700.46 759.676 1703.54 759.676 Q1703.98 759.676 1704.51 759.745 Q1705.04 759.791 1705.69 759.907 L1705.72 764.282 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M1724.37 764.282 Q1723.66 763.865 1722.8 763.68 Q1721.97 763.472 1720.95 763.472 Q1717.34 763.472 1715.39 765.833 Q1713.47 768.171 1713.47 772.569 L1713.47 786.226 L1709.19 786.226 L1709.19 760.301 L1713.47 760.301 L1713.47 764.328 Q1714.81 761.967 1716.97 760.833 Q1719.12 759.676 1722.2 759.676 Q1722.64 759.676 1723.17 759.745 Q1723.7 759.791 1724.35 759.907 L1724.37 764.282 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M1748.54 794.097 L1748.54 797.407 L1723.91 797.407 L1723.91 794.097 L1748.54 794.097 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M1756.76 752.94 L1756.76 760.301 L1765.53 760.301 L1765.53 763.611 L1756.76 763.611 L1756.76 777.685 Q1756.76 780.856 1757.61 781.759 Q1758.49 782.662 1761.16 782.662 L1765.53 782.662 L1765.53 786.226 L1761.16 786.226 Q1756.23 786.226 1754.35 784.398 Q1752.48 782.546 1752.48 777.685 L1752.48 763.611 L1749.35 763.611 L1749.35 760.301 L1752.48 760.301 L1752.48 752.94 L1756.76 752.94 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M1793.31 772.199 L1793.31 774.282 L1773.72 774.282 Q1774 778.68 1776.36 780.995 Q1778.75 783.287 1782.98 783.287 Q1785.44 783.287 1787.73 782.685 Q1790.04 782.083 1792.31 780.879 L1792.31 784.907 Q1790.02 785.879 1787.61 786.388 Q1785.21 786.898 1782.73 786.898 Q1776.53 786.898 1772.89 783.287 Q1769.28 779.676 1769.28 773.518 Q1769.28 767.152 1772.71 763.426 Q1776.16 759.676 1781.99 759.676 Q1787.22 759.676 1790.25 763.055 Q1793.31 766.412 1793.31 772.199 M1789.05 770.949 Q1789 767.453 1787.08 765.37 Q1785.18 763.287 1782.04 763.287 Q1778.47 763.287 1776.32 765.301 Q1774.19 767.314 1773.86 770.972 L1789.05 770.949 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M1816.83 761.065 L1816.83 765.092 Q1815.02 764.166 1813.08 763.703 Q1811.13 763.24 1809.05 763.24 Q1805.88 763.24 1804.28 764.213 Q1802.71 765.185 1802.71 767.129 Q1802.71 768.611 1803.84 769.467 Q1804.97 770.301 1808.4 771.064 L1809.86 771.389 Q1814.4 772.361 1816.29 774.143 Q1818.22 775.902 1818.22 779.074 Q1818.22 782.685 1815.35 784.791 Q1812.5 786.898 1807.5 786.898 Q1805.41 786.898 1803.15 786.481 Q1800.9 786.088 1798.4 785.277 L1798.4 780.879 Q1800.76 782.106 1803.05 782.731 Q1805.35 783.333 1807.59 783.333 Q1810.6 783.333 1812.22 782.314 Q1813.84 781.273 1813.84 779.398 Q1813.84 777.662 1812.66 776.736 Q1811.5 775.81 1807.54 774.953 L1806.06 774.606 Q1802.1 773.773 1800.35 772.06 Q1798.59 770.324 1798.59 767.314 Q1798.59 763.657 1801.18 761.666 Q1803.77 759.676 1808.54 759.676 Q1810.9 759.676 1812.98 760.023 Q1815.07 760.37 1816.83 761.065 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M1829.21 752.94 L1829.21 760.301 L1837.98 760.301 L1837.98 763.611 L1829.21 763.611 L1829.21 777.685 Q1829.21 780.856 1830.07 781.759 Q1830.95 782.662 1833.61 782.662 L1837.98 782.662 L1837.98 786.226 L1833.61 786.226 Q1828.68 786.226 1826.8 784.398 Q1824.93 782.546 1824.93 777.685 L1824.93 763.611 L1821.8 763.611 L1821.8 760.301 L1824.93 760.301 L1824.93 752.94 L1829.21 752.94 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip980)\" style=\"stroke:#3da44d; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1365.51,820.786 1483.55,820.786 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip980)\" d=\"M1523.56 817.117 Q1525.16 814.247 1527.38 812.881 Q1529.61 811.516 1532.62 811.516 Q1536.67 811.516 1538.87 814.363 Q1541.06 817.187 1541.06 822.418 L1541.06 838.066 L1536.78 838.066 L1536.78 822.557 Q1536.78 818.83 1535.46 817.025 Q1534.14 815.219 1531.43 815.219 Q1528.12 815.219 1526.2 817.418 Q1524.28 819.617 1524.28 823.414 L1524.28 838.066 L1520 838.066 L1520 822.557 Q1520 818.807 1518.68 817.025 Q1517.36 815.219 1514.61 815.219 Q1511.34 815.219 1509.42 817.442 Q1507.5 819.641 1507.5 823.414 L1507.5 838.066 L1503.22 838.066 L1503.22 812.141 L1507.5 812.141 L1507.5 816.168 Q1508.96 813.784 1510.99 812.65 Q1513.03 811.516 1515.83 811.516 Q1518.66 811.516 1520.62 812.951 Q1522.62 814.386 1523.56 817.117 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M1571.74 824.039 L1571.74 826.122 L1552.15 826.122 Q1552.43 830.52 1554.79 832.835 Q1557.18 835.127 1561.41 835.127 Q1563.86 835.127 1566.16 834.525 Q1568.47 833.923 1570.74 832.719 L1570.74 836.747 Q1568.45 837.719 1566.04 838.228 Q1563.63 838.738 1561.16 838.738 Q1554.95 838.738 1551.32 835.127 Q1547.71 831.516 1547.71 825.358 Q1547.71 818.992 1551.13 815.266 Q1554.58 811.516 1560.42 811.516 Q1565.65 811.516 1568.68 814.895 Q1571.74 818.252 1571.74 824.039 M1567.48 822.789 Q1567.43 819.293 1565.51 817.21 Q1563.61 815.127 1560.46 815.127 Q1556.9 815.127 1554.74 817.141 Q1552.62 819.154 1552.29 822.812 L1567.48 822.789 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M1590.51 825.034 Q1585.35 825.034 1583.36 826.215 Q1581.36 827.395 1581.36 830.242 Q1581.36 832.511 1582.85 833.853 Q1584.35 835.173 1586.92 835.173 Q1590.46 835.173 1592.59 832.673 Q1594.74 830.15 1594.74 825.983 L1594.74 825.034 L1590.51 825.034 M1599 823.275 L1599 838.066 L1594.74 838.066 L1594.74 834.131 Q1593.29 836.492 1591.11 837.627 Q1588.93 838.738 1585.79 838.738 Q1581.8 838.738 1579.44 836.515 Q1577.11 834.27 1577.11 830.52 Q1577.11 826.145 1580.02 823.923 Q1582.96 821.701 1588.77 821.701 L1594.74 821.701 L1594.74 821.284 Q1594.74 818.344 1592.8 816.747 Q1590.88 815.127 1587.38 815.127 Q1585.16 815.127 1583.05 815.659 Q1580.95 816.192 1579 817.256 L1579 813.321 Q1581.34 812.418 1583.54 811.979 Q1585.74 811.516 1587.82 811.516 Q1593.45 811.516 1596.23 814.432 Q1599 817.349 1599 823.275 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M1629.33 822.418 L1629.33 838.066 L1625.07 838.066 L1625.07 822.557 Q1625.07 818.877 1623.63 817.048 Q1622.2 815.219 1619.33 815.219 Q1615.88 815.219 1613.89 817.418 Q1611.9 819.617 1611.9 823.414 L1611.9 838.066 L1607.61 838.066 L1607.61 812.141 L1611.9 812.141 L1611.9 816.168 Q1613.42 813.83 1615.49 812.673 Q1617.57 811.516 1620.28 811.516 Q1624.74 811.516 1627.04 814.293 Q1629.33 817.048 1629.33 822.418 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M1657.52 845.937 L1657.52 849.247 L1632.89 849.247 L1632.89 845.937 L1657.52 845.937 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M1683.7 824.039 L1683.7 826.122 L1664.12 826.122 Q1664.4 830.52 1666.76 832.835 Q1669.14 835.127 1673.38 835.127 Q1675.83 835.127 1678.12 834.525 Q1680.44 833.923 1682.71 832.719 L1682.71 836.747 Q1680.42 837.719 1678.01 838.228 Q1675.6 838.738 1673.12 838.738 Q1666.92 838.738 1663.29 835.127 Q1659.67 831.516 1659.67 825.358 Q1659.67 818.992 1663.1 815.266 Q1666.55 811.516 1672.38 811.516 Q1677.61 811.516 1680.65 814.895 Q1683.7 818.252 1683.7 824.039 M1679.44 822.789 Q1679.4 819.293 1677.48 817.21 Q1675.58 815.127 1672.43 815.127 Q1668.86 815.127 1666.71 817.141 Q1664.58 819.154 1664.26 822.812 L1679.44 822.789 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M1705.72 816.122 Q1705 815.705 1704.14 815.52 Q1703.31 815.312 1702.29 815.312 Q1698.68 815.312 1696.73 817.673 Q1694.81 820.011 1694.81 824.409 L1694.81 838.066 L1690.53 838.066 L1690.53 812.141 L1694.81 812.141 L1694.81 816.168 Q1696.16 813.807 1698.31 812.673 Q1700.46 811.516 1703.54 811.516 Q1703.98 811.516 1704.51 811.585 Q1705.04 811.631 1705.69 811.747 L1705.72 816.122 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M1724.37 816.122 Q1723.66 815.705 1722.8 815.52 Q1721.97 815.312 1720.95 815.312 Q1717.34 815.312 1715.39 817.673 Q1713.47 820.011 1713.47 824.409 L1713.47 838.066 L1709.19 838.066 L1709.19 812.141 L1713.47 812.141 L1713.47 816.168 Q1714.81 813.807 1716.97 812.673 Q1719.12 811.516 1722.2 811.516 Q1722.64 811.516 1723.17 811.585 Q1723.7 811.631 1724.35 811.747 L1724.37 816.122 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M1748.54 845.937 L1748.54 849.247 L1723.91 849.247 L1723.91 845.937 L1748.54 845.937 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M1756.76 804.78 L1756.76 812.141 L1765.53 812.141 L1765.53 815.451 L1756.76 815.451 L1756.76 829.525 Q1756.76 832.696 1757.61 833.599 Q1758.49 834.502 1761.16 834.502 L1765.53 834.502 L1765.53 838.066 L1761.16 838.066 Q1756.23 838.066 1754.35 836.238 Q1752.48 834.386 1752.48 829.525 L1752.48 815.451 L1749.35 815.451 L1749.35 812.141 L1752.48 812.141 L1752.48 804.78 L1756.76 804.78 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M1786.16 816.122 Q1785.44 815.705 1784.58 815.52 Q1783.75 815.312 1782.73 815.312 Q1779.12 815.312 1777.17 817.673 Q1775.25 820.011 1775.25 824.409 L1775.25 838.066 L1770.97 838.066 L1770.97 812.141 L1775.25 812.141 L1775.25 816.168 Q1776.6 813.807 1778.75 812.673 Q1780.9 811.516 1783.98 811.516 Q1784.42 811.516 1784.95 811.585 Q1785.48 811.631 1786.13 811.747 L1786.16 816.122 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M1802.41 825.034 Q1797.24 825.034 1795.25 826.215 Q1793.26 827.395 1793.26 830.242 Q1793.26 832.511 1794.74 833.853 Q1796.25 835.173 1798.82 835.173 Q1802.36 835.173 1804.49 832.673 Q1806.64 830.15 1806.64 825.983 L1806.64 825.034 L1802.41 825.034 M1810.9 823.275 L1810.9 838.066 L1806.64 838.066 L1806.64 834.131 Q1805.18 836.492 1803.01 837.627 Q1800.83 838.738 1797.68 838.738 Q1793.7 838.738 1791.34 836.515 Q1789 834.27 1789 830.52 Q1789 826.145 1791.92 823.923 Q1794.86 821.701 1800.67 821.701 L1806.64 821.701 L1806.64 821.284 Q1806.64 818.344 1804.7 816.747 Q1802.78 815.127 1799.28 815.127 Q1797.06 815.127 1794.95 815.659 Q1792.85 816.192 1790.9 817.256 L1790.9 813.321 Q1793.24 812.418 1795.44 811.979 Q1797.64 811.516 1799.72 811.516 Q1805.35 811.516 1808.12 814.432 Q1810.9 817.349 1810.9 823.275 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M1819.67 812.141 L1823.93 812.141 L1823.93 838.066 L1819.67 838.066 L1819.67 812.141 M1819.67 802.048 L1823.93 802.048 L1823.93 807.442 L1819.67 807.442 L1819.67 802.048 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip980)\" d=\"M1854.4 822.418 L1854.4 838.066 L1850.14 838.066 L1850.14 822.557 Q1850.14 818.877 1848.7 817.048 Q1847.27 815.219 1844.4 815.219 Q1840.95 815.219 1838.96 817.418 Q1836.97 819.617 1836.97 823.414 L1836.97 838.066 L1832.68 838.066 L1832.68 812.141 L1836.97 812.141 L1836.97 816.168 Q1838.49 813.83 1840.55 812.673 Q1842.64 811.516 1845.34 811.516 Q1849.81 811.516 1852.1 814.293 Q1854.4 817.048 1854.4 822.418 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = range(1, nr_of_epoch, step=1)\n",
    "\n",
    "p= Plots.plot(x,val_acc,label=\"val_acc\",legend=:bottomright, size=(500, 250))\n",
    "p= Plots.plot!(p,x,mean_err_test,label=\"mean_err_test\")\n",
    "p= Plots.plot!(p,x,mean_err_train,label=\"mean_err_train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be34ad80",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Beyond controlling the number of dimensions and the size of the weight vector, dropout is yet another tool to avoid overfitting. Often they are used jointly.\n",
    "Note that dropout is\n",
    "used only during training:\n",
    "it replaces an activation $h$ with a random variable with expected value $h$.\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. What happens if you change the dropout probabilities for the first and second layers? In particular, what happens if you switch the ones for both layers? Design an experiment to answer these questions, describe your results quantitatively, and summarize the qualitative takeaways.\n",
    "1. Increase the number of epochs and compare the results obtained when using dropout with those when not using it.\n",
    "1. What is the variance of the activations in each hidden layer when dropout is and is not applied? Draw a plot to show how this quantity evolves over time for both models.\n",
    "1. Why is dropout not typically used at test time?\n",
    "1. Using the model in this section as an example, compare the effects of using dropout and weight decay. What happens when dropout and weight decay are used at the same time? Are the results additive? Are there diminished returns (or worse)? Do they cancel each other out?\n",
    "1. What happens if we apply dropout to the individual weights of the weight matrix rather than the activations?\n",
    "1. Invent another technique for injecting random noise at each layer that is different from the standard dropout technique. Can you develop a method that outperforms dropout on the Fashion-MNIST dataset (for a fixed architecture)?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
