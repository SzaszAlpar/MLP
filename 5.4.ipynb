{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12df275e",
   "metadata": {},
   "source": [
    "# Numerical Stability and Initialization\n",
    ":label:`sec_numerical_stability`\n",
    "\n",
    "\n",
    "Thus far, every model that we have implemented\n",
    "required that we initialize its parameters\n",
    "according to some pre-specified distribution.\n",
    "Until now, we took the initialization scheme for granted,\n",
    "glossing over the details of how these choices are made.\n",
    "You might have even gotten the impression that these choices\n",
    "are not especially important.\n",
    "To the contrary, the choice of initialization scheme\n",
    "plays a significant role in neural network learning,\n",
    "and it can be crucial for maintaining numerical stability.\n",
    "Moreover, these choices can be tied up in interesting ways\n",
    "with the choice of the nonlinear activation function.\n",
    "Which function we choose and how we initialize parameters\n",
    "can determine how quickly our optimization algorithm converges.\n",
    "Poor choices here can cause us to encounter\n",
    "exploding or vanishing gradients while training.\n",
    "In this section, we delve into these topics with greater detail\n",
    "and discuss some useful heuristics\n",
    "that you will find useful\n",
    "throughout your career in deep learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e376be97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Flux\n",
    "import Zygote\n",
    "import Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317a43c6",
   "metadata": {},
   "source": [
    "## Vanishing and Exploding Gradients\n",
    "\n",
    "Consider a deep network with $L$ layers,\n",
    "input $\\mathbf{x}$ and output $\\mathbf{o}$.\n",
    "With each layer $l$ defined by a transformation $f_l$\n",
    "parameterized by weights $\\mathbf{W}^{(l)}$,\n",
    "whose hidden layer output is $\\mathbf{h}^{(l)}$ (let $\\mathbf{h}^{(0)} = \\mathbf{x}$),\n",
    "our network can be expressed as:\n",
    "\n",
    "$$\\mathbf{h}^{(l)} = f_l (\\mathbf{h}^{(l-1)}) \\text{ and thus } \\mathbf{o} = f_L \\circ \\ldots \\circ f_1(\\mathbf{x}).$$\n",
    "\n",
    "If all the hidden layer output and the input are vectors,\n",
    "we can write the gradient of $\\mathbf{o}$ with respect to\n",
    "any set of parameters $\\mathbf{W}^{(l)}$ as follows:\n",
    "\n",
    "$$\\partial_{\\mathbf{W}^{(l)}} \\mathbf{o} = \\underbrace{\\partial_{\\mathbf{h}^{(L-1)}} \\mathbf{h}^{(L)}}_{ \\mathbf{M}^{(L)} \\stackrel{\\mathrm{def}}{=}} \\cdot \\ldots \\cdot \\underbrace{\\partial_{\\mathbf{h}^{(l)}} \\mathbf{h}^{(l+1)}}_{ \\mathbf{M}^{(l+1)} \\stackrel{\\mathrm{def}}{=}} \\underbrace{\\partial_{\\mathbf{W}^{(l)}} \\mathbf{h}^{(l)}}_{ \\mathbf{v}^{(l)} \\stackrel{\\mathrm{def}}{=}}.$$\n",
    "\n",
    "In other words, this gradient is\n",
    "the product of $L-l$ matrices\n",
    "$\\mathbf{M}^{(L)} \\cdot \\ldots \\cdot \\mathbf{M}^{(l+1)}$\n",
    "and the gradient vector $\\mathbf{v}^{(l)}$.\n",
    "Thus we are susceptible to the same\n",
    "problems of numerical underflow that often crop up\n",
    "when multiplying together too many probabilities.\n",
    "When dealing with probabilities, a common trick is to\n",
    "switch into log-space, i.e., shifting\n",
    "pressure from the mantissa to the exponent\n",
    "of the numerical representation.\n",
    "Unfortunately, our problem above is more serious:\n",
    "initially the matrices $\\mathbf{M}^{(l)}$ may have a wide variety of eigenvalues.\n",
    "They might be small or large, and\n",
    "their product might be *very large* or *very small*.\n",
    "\n",
    "The risks posed by unstable gradients\n",
    "go beyond numerical representation.\n",
    "Gradients of unpredictable magnitude\n",
    "also threaten the stability of our optimization algorithms.\n",
    "We may be facing parameter updates that are either\n",
    "(i) excessively large, destroying our model\n",
    "(the *exploding gradient* problem);\n",
    "or (ii) excessively small\n",
    "(the *vanishing gradient* problem),\n",
    "rendering learning impossible as parameters\n",
    "hardly move on each update.\n",
    "\n",
    "\n",
    "### (**Vanishing Gradients**)\n",
    "\n",
    "One frequent culprit causing the vanishing gradient problem\n",
    "is the choice of the activation function $\\sigma$\n",
    "that is appended following each layer's linear operations.\n",
    "Historically, the sigmoid function\n",
    "$1/(1 + \\exp(-x))$ (introduced in :numref:`sec_mlp`)\n",
    "was popular because it resembles a thresholding function.\n",
    "Since early artificial neural networks were inspired\n",
    "by biological neural networks,\n",
    "the idea of neurons that fire either *fully* or *not at all*\n",
    "(like biological neurons) seemed appealing.\n",
    "Let's take a closer look at the sigmoid\n",
    "to see why it can cause vanishing gradients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61ef21bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"500\" height=\"250\" viewBox=\"0 0 2000 1000\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip630\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2000\" height=\"1000\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip630)\" d=\"\n",
       "M0 1000 L2000 1000 L2000 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip631\">\n",
       "    <rect x=\"400\" y=\"0\" width=\"1401\" height=\"1000\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip630)\" d=\"\n",
       "M182.274 901.088 L1952.76 901.088 L1952.76 47.2441 L182.274 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip632\">\n",
       "    <rect x=\"182\" y=\"47\" width=\"1771\" height=\"855\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip632)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  399.409,901.088 399.409,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip632)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  622.111,901.088 622.111,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip632)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  844.813,901.088 844.813,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip632)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1067.51,901.088 1067.51,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip632)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1290.22,901.088 1290.22,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip632)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1512.92,901.088 1512.92,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip632)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1735.62,901.088 1735.62,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip630)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  182.274,901.088 1952.76,901.088 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip630)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  399.409,901.088 399.409,882.19 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip630)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  622.111,901.088 622.111,882.19 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip630)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  844.813,901.088 844.813,882.19 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip630)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1067.51,901.088 1067.51,882.19 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip630)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1290.22,901.088 1290.22,882.19 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip630)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1512.92,901.088 1512.92,882.19 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip630)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1735.62,901.088 1735.62,882.19 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip630)\" d=\"M368.471 943.459 L398.147 943.459 L398.147 947.394 L368.471 947.394 L368.471 943.459 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M418.818 941.144 Q415.67 941.144 413.818 943.297 Q411.99 945.45 411.99 949.2 Q411.99 952.927 413.818 955.103 Q415.67 957.255 418.818 957.255 Q421.966 957.255 423.795 955.103 Q425.647 952.927 425.647 949.2 Q425.647 945.45 423.795 943.297 Q421.966 941.144 418.818 941.144 M428.101 926.492 L428.101 930.751 Q426.341 929.918 424.536 929.478 Q422.753 929.038 420.994 929.038 Q416.365 929.038 413.911 932.163 Q411.48 935.288 411.133 941.607 Q412.499 939.594 414.559 938.529 Q416.619 937.441 419.096 937.441 Q424.304 937.441 427.314 940.612 Q430.346 943.76 430.346 949.2 Q430.346 954.524 427.198 957.742 Q424.05 960.959 418.818 960.959 Q412.823 960.959 409.652 956.376 Q406.48 951.769 406.48 943.043 Q406.48 934.848 410.369 929.987 Q414.258 925.103 420.809 925.103 Q422.568 925.103 424.351 925.45 Q426.156 925.797 428.101 926.492 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M591.011 943.459 L620.687 943.459 L620.687 947.394 L591.011 947.394 L591.011 943.459 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M643.627 929.802 L631.821 948.251 L643.627 948.251 L643.627 929.802 M642.4 925.728 L648.28 925.728 L648.28 948.251 L653.21 948.251 L653.21 952.14 L648.28 952.14 L648.28 960.288 L643.627 960.288 L643.627 952.14 L628.025 952.14 L628.025 947.626 L642.4 925.728 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M814.755 943.459 L844.431 943.459 L844.431 947.394 L814.755 947.394 L814.755 943.459 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M858.551 956.353 L874.871 956.353 L874.871 960.288 L852.926 960.288 L852.926 956.353 Q855.588 953.598 860.172 948.968 Q864.778 944.316 865.959 942.973 Q868.204 940.45 869.084 938.714 Q869.986 936.955 869.986 935.265 Q869.986 932.51 868.042 930.774 Q866.121 929.038 863.019 929.038 Q860.82 929.038 858.366 929.802 Q855.935 930.566 853.158 932.117 L853.158 927.395 Q855.982 926.26 858.435 925.682 Q860.889 925.103 862.926 925.103 Q868.296 925.103 871.491 927.788 Q874.685 930.473 874.685 934.964 Q874.685 937.094 873.875 939.015 Q873.088 940.913 870.982 943.506 Q870.403 944.177 867.301 947.394 Q864.199 950.589 858.551 956.353 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M1067.51 928.807 Q1063.9 928.807 1062.08 932.371 Q1060.27 935.913 1060.27 943.043 Q1060.27 950.149 1062.08 953.714 Q1063.9 957.255 1067.51 957.255 Q1071.15 957.255 1072.95 953.714 Q1074.78 950.149 1074.78 943.043 Q1074.78 935.913 1072.95 932.371 Q1071.15 928.807 1067.51 928.807 M1067.51 925.103 Q1073.33 925.103 1076.38 929.709 Q1079.46 934.293 1079.46 943.043 Q1079.46 951.769 1076.38 956.376 Q1073.33 960.959 1067.51 960.959 Q1061.7 960.959 1058.63 956.376 Q1055.57 951.769 1055.57 943.043 Q1055.57 934.293 1058.63 929.709 Q1061.7 925.103 1067.51 925.103 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M1284.87 956.353 L1301.19 956.353 L1301.19 960.288 L1279.24 960.288 L1279.24 956.353 Q1281.91 953.598 1286.49 948.968 Q1291.1 944.316 1292.28 942.973 Q1294.52 940.45 1295.4 938.714 Q1296.31 936.955 1296.31 935.265 Q1296.31 932.51 1294.36 930.774 Q1292.44 929.038 1289.34 929.038 Q1287.14 929.038 1284.68 929.802 Q1282.25 930.566 1279.48 932.117 L1279.48 927.395 Q1282.3 926.26 1284.75 925.682 Q1287.21 925.103 1289.24 925.103 Q1294.62 925.103 1297.81 927.788 Q1301 930.473 1301 934.964 Q1301 937.094 1300.19 939.015 Q1299.41 940.913 1297.3 943.506 Q1296.72 944.177 1293.62 947.394 Q1290.52 950.589 1284.87 956.353 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M1515.93 929.802 L1504.12 948.251 L1515.93 948.251 L1515.93 929.802 M1514.7 925.728 L1520.58 925.728 L1520.58 948.251 L1525.51 948.251 L1525.51 952.14 L1520.58 952.14 L1520.58 960.288 L1515.93 960.288 L1515.93 952.14 L1500.33 952.14 L1500.33 947.626 L1514.7 925.728 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M1736.03 941.144 Q1732.88 941.144 1731.03 943.297 Q1729.2 945.45 1729.2 949.2 Q1729.2 952.927 1731.03 955.103 Q1732.88 957.255 1736.03 957.255 Q1739.17 957.255 1741 955.103 Q1742.86 952.927 1742.86 949.2 Q1742.86 945.45 1741 943.297 Q1739.17 941.144 1736.03 941.144 M1745.31 926.492 L1745.31 930.751 Q1743.55 929.918 1741.74 929.478 Q1739.96 929.038 1738.2 929.038 Q1733.57 929.038 1731.12 932.163 Q1728.69 935.288 1728.34 941.607 Q1729.71 939.594 1731.77 938.529 Q1733.83 937.441 1736.3 937.441 Q1741.51 937.441 1744.52 940.612 Q1747.55 943.76 1747.55 949.2 Q1747.55 954.524 1744.41 957.742 Q1741.26 960.959 1736.03 960.959 Q1730.03 960.959 1726.86 956.376 Q1723.69 951.769 1723.69 943.043 Q1723.69 934.848 1727.58 929.987 Q1731.47 925.103 1738.02 925.103 Q1739.78 925.103 1741.56 925.45 Q1743.36 925.797 1745.31 926.492 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip632)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  182.274,877.368 1952.76,877.368 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip632)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  182.274,675.767 1952.76,675.767 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip632)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  182.274,474.166 1952.76,474.166 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip632)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  182.274,272.565 1952.76,272.565 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip632)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  182.274,70.9637 1952.76,70.9637 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip630)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  182.274,901.088 182.274,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip630)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  182.274,877.368 201.172,877.368 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip630)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  182.274,675.767 201.172,675.767 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip630)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  182.274,474.166 201.172,474.166 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip630)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  182.274,272.565 201.172,272.565 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip630)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  182.274,70.9637 201.172,70.9637 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip630)\" d=\"M64.9365 863.167 Q61.3254 863.167 59.4967 866.731 Q57.6912 870.273 57.6912 877.403 Q57.6912 884.509 59.4967 888.074 Q61.3254 891.616 64.9365 891.616 Q68.5707 891.616 70.3763 888.074 Q72.205 884.509 72.205 877.403 Q72.205 870.273 70.3763 866.731 Q68.5707 863.167 64.9365 863.167 M64.9365 859.463 Q70.7467 859.463 73.8022 864.069 Q76.8809 868.653 76.8809 877.403 Q76.8809 886.13 73.8022 890.736 Q70.7467 895.319 64.9365 895.319 Q59.1264 895.319 56.0477 890.736 Q52.9921 886.13 52.9921 877.403 Q52.9921 868.653 56.0477 864.069 Q59.1264 859.463 64.9365 859.463 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M85.0984 888.768 L89.9827 888.768 L89.9827 894.648 L85.0984 894.648 L85.0984 888.768 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M110.168 863.167 Q106.557 863.167 104.728 866.731 Q102.922 870.273 102.922 877.403 Q102.922 884.509 104.728 888.074 Q106.557 891.616 110.168 891.616 Q113.802 891.616 115.608 888.074 Q117.436 884.509 117.436 877.403 Q117.436 870.273 115.608 866.731 Q113.802 863.167 110.168 863.167 M110.168 859.463 Q115.978 859.463 119.033 864.069 Q122.112 868.653 122.112 877.403 Q122.112 886.13 119.033 890.736 Q115.978 895.319 110.168 895.319 Q104.358 895.319 101.279 890.736 Q98.2234 886.13 98.2234 877.403 Q98.2234 868.653 101.279 864.069 Q104.358 859.463 110.168 859.463 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M140.33 863.167 Q136.719 863.167 134.89 866.731 Q133.084 870.273 133.084 877.403 Q133.084 884.509 134.89 888.074 Q136.719 891.616 140.33 891.616 Q143.964 891.616 145.769 888.074 Q147.598 884.509 147.598 877.403 Q147.598 870.273 145.769 866.731 Q143.964 863.167 140.33 863.167 M140.33 859.463 Q146.14 859.463 149.195 864.069 Q152.274 868.653 152.274 877.403 Q152.274 886.13 149.195 890.736 Q146.14 895.319 140.33 895.319 Q134.519 895.319 131.441 890.736 Q128.385 886.13 128.385 877.403 Q128.385 868.653 131.441 864.069 Q134.519 859.463 140.33 859.463 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M65.9319 661.566 Q62.3208 661.566 60.4921 665.13 Q58.6865 668.672 58.6865 675.802 Q58.6865 682.908 60.4921 686.473 Q62.3208 690.015 65.9319 690.015 Q69.5661 690.015 71.3717 686.473 Q73.2004 682.908 73.2004 675.802 Q73.2004 668.672 71.3717 665.13 Q69.5661 661.566 65.9319 661.566 M65.9319 657.862 Q71.742 657.862 74.7976 662.468 Q77.8763 667.052 77.8763 675.802 Q77.8763 684.528 74.7976 689.135 Q71.742 693.718 65.9319 693.718 Q60.1217 693.718 57.043 689.135 Q53.9875 684.528 53.9875 675.802 Q53.9875 667.052 57.043 662.468 Q60.1217 657.862 65.9319 657.862 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M86.0938 687.167 L90.978 687.167 L90.978 693.047 L86.0938 693.047 L86.0938 687.167 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M105.191 689.112 L121.51 689.112 L121.51 693.047 L99.566 693.047 L99.566 689.112 Q102.228 686.357 106.811 681.728 Q111.418 677.075 112.598 675.732 Q114.844 673.209 115.723 671.473 Q116.626 669.714 116.626 668.024 Q116.626 665.269 114.682 663.533 Q112.76 661.797 109.658 661.797 Q107.459 661.797 105.006 662.561 Q102.575 663.325 99.7974 664.876 L99.7974 660.154 Q102.621 659.019 105.075 658.441 Q107.529 657.862 109.566 657.862 Q114.936 657.862 118.131 660.547 Q121.325 663.232 121.325 667.723 Q121.325 669.853 120.515 671.774 Q119.728 673.672 117.621 676.265 Q117.043 676.936 113.941 680.153 Q110.839 683.348 105.191 689.112 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M131.371 658.487 L149.728 658.487 L149.728 662.422 L135.654 662.422 L135.654 670.894 Q136.672 670.547 137.691 670.385 Q138.709 670.2 139.728 670.2 Q145.515 670.2 148.894 673.371 Q152.274 676.542 152.274 681.959 Q152.274 687.538 148.802 690.64 Q145.33 693.718 139.01 693.718 Q136.834 693.718 134.566 693.348 Q132.32 692.977 129.913 692.237 L129.913 687.538 Q131.996 688.672 134.219 689.228 Q136.441 689.783 138.918 689.783 Q142.922 689.783 145.26 687.677 Q147.598 685.57 147.598 681.959 Q147.598 678.348 145.26 676.241 Q142.922 674.135 138.918 674.135 Q137.043 674.135 135.168 674.552 Q133.316 674.968 131.371 675.848 L131.371 658.487 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M64.9365 459.965 Q61.3254 459.965 59.4967 463.529 Q57.6912 467.071 57.6912 474.201 Q57.6912 481.307 59.4967 484.872 Q61.3254 488.413 64.9365 488.413 Q68.5707 488.413 70.3763 484.872 Q72.205 481.307 72.205 474.201 Q72.205 467.071 70.3763 463.529 Q68.5707 459.965 64.9365 459.965 M64.9365 456.261 Q70.7467 456.261 73.8022 460.867 Q76.8809 465.451 76.8809 474.201 Q76.8809 482.927 73.8022 487.534 Q70.7467 492.117 64.9365 492.117 Q59.1264 492.117 56.0477 487.534 Q52.9921 482.927 52.9921 474.201 Q52.9921 465.451 56.0477 460.867 Q59.1264 456.261 64.9365 456.261 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M85.0984 485.566 L89.9827 485.566 L89.9827 491.446 L85.0984 491.446 L85.0984 485.566 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M100.214 456.886 L118.57 456.886 L118.57 460.821 L104.496 460.821 L104.496 469.293 Q105.515 468.946 106.534 468.784 Q107.552 468.599 108.571 468.599 Q114.358 468.599 117.737 471.77 Q121.117 474.941 121.117 480.358 Q121.117 485.937 117.645 489.038 Q114.172 492.117 107.853 492.117 Q105.677 492.117 103.409 491.747 Q101.163 491.376 98.7558 490.636 L98.7558 485.937 Q100.839 487.071 103.061 487.626 Q105.284 488.182 107.76 488.182 Q111.765 488.182 114.103 486.076 Q116.441 483.969 116.441 480.358 Q116.441 476.747 114.103 474.64 Q111.765 472.534 107.76 472.534 Q105.885 472.534 104.01 472.951 Q102.159 473.367 100.214 474.247 L100.214 456.886 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M140.33 459.965 Q136.719 459.965 134.89 463.529 Q133.084 467.071 133.084 474.201 Q133.084 481.307 134.89 484.872 Q136.719 488.413 140.33 488.413 Q143.964 488.413 145.769 484.872 Q147.598 481.307 147.598 474.201 Q147.598 467.071 145.769 463.529 Q143.964 459.965 140.33 459.965 M140.33 456.261 Q146.14 456.261 149.195 460.867 Q152.274 465.451 152.274 474.201 Q152.274 482.927 149.195 487.534 Q146.14 492.117 140.33 492.117 Q134.519 492.117 131.441 487.534 Q128.385 482.927 128.385 474.201 Q128.385 465.451 131.441 460.867 Q134.519 456.261 140.33 456.261 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M65.9319 258.363 Q62.3208 258.363 60.4921 261.928 Q58.6865 265.47 58.6865 272.6 Q58.6865 279.706 60.4921 283.271 Q62.3208 286.812 65.9319 286.812 Q69.5661 286.812 71.3717 283.271 Q73.2004 279.706 73.2004 272.6 Q73.2004 265.47 71.3717 261.928 Q69.5661 258.363 65.9319 258.363 M65.9319 254.66 Q71.742 254.66 74.7976 259.266 Q77.8763 263.85 77.8763 272.6 Q77.8763 281.326 74.7976 285.933 Q71.742 290.516 65.9319 290.516 Q60.1217 290.516 57.043 285.933 Q53.9875 281.326 53.9875 272.6 Q53.9875 263.85 57.043 259.266 Q60.1217 254.66 65.9319 254.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M86.0938 283.965 L90.978 283.965 L90.978 289.845 L86.0938 289.845 L86.0938 283.965 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M99.9826 255.285 L122.205 255.285 L122.205 257.276 L109.658 289.845 L104.774 289.845 L116.58 259.22 L99.9826 259.22 L99.9826 255.285 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M131.371 255.285 L149.728 255.285 L149.728 259.22 L135.654 259.22 L135.654 267.692 Q136.672 267.345 137.691 267.183 Q138.709 266.998 139.728 266.998 Q145.515 266.998 148.894 270.169 Q152.274 273.34 152.274 278.757 Q152.274 284.336 148.802 287.437 Q145.33 290.516 139.01 290.516 Q136.834 290.516 134.566 290.146 Q132.32 289.775 129.913 289.035 L129.913 284.336 Q131.996 285.47 134.219 286.025 Q136.441 286.581 138.918 286.581 Q142.922 286.581 145.26 284.474 Q147.598 282.368 147.598 278.757 Q147.598 275.146 145.26 273.039 Q142.922 270.933 138.918 270.933 Q137.043 270.933 135.168 271.35 Q133.316 271.766 131.371 272.646 L131.371 255.285 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M55.7467 84.3086 L63.3856 84.3086 L63.3856 57.943 L55.0754 59.6096 L55.0754 55.3504 L63.3393 53.6837 L68.0152 53.6837 L68.0152 84.3086 L75.654 84.3086 L75.654 88.2437 L55.7467 88.2437 L55.7467 84.3086 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M85.0984 82.3641 L89.9827 82.3641 L89.9827 88.2437 L85.0984 88.2437 L85.0984 82.3641 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M110.168 56.7624 Q106.557 56.7624 104.728 60.3272 Q102.922 63.8689 102.922 70.9984 Q102.922 78.1049 104.728 81.6697 Q106.557 85.2113 110.168 85.2113 Q113.802 85.2113 115.608 81.6697 Q117.436 78.1049 117.436 70.9984 Q117.436 63.8689 115.608 60.3272 Q113.802 56.7624 110.168 56.7624 M110.168 53.0587 Q115.978 53.0587 119.033 57.6652 Q122.112 62.2485 122.112 70.9984 Q122.112 79.7252 119.033 84.3317 Q115.978 88.915 110.168 88.915 Q104.358 88.915 101.279 84.3317 Q98.2234 79.7252 98.2234 70.9984 Q98.2234 62.2485 101.279 57.6652 Q104.358 53.0587 110.168 53.0587 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M140.33 56.7624 Q136.719 56.7624 134.89 60.3272 Q133.084 63.8689 133.084 70.9984 Q133.084 78.1049 134.89 81.6697 Q136.719 85.2113 140.33 85.2113 Q143.964 85.2113 145.769 81.6697 Q147.598 78.1049 147.598 70.9984 Q147.598 63.8689 145.769 60.3272 Q143.964 56.7624 140.33 56.7624 M140.33 53.0587 Q146.14 53.0587 149.195 57.6652 Q152.274 62.2485 152.274 70.9984 Q152.274 79.7252 149.195 84.3317 Q146.14 88.915 140.33 88.915 Q134.519 88.915 131.441 84.3317 Q128.385 79.7252 128.385 70.9984 Q128.385 62.2485 131.441 57.6652 Q134.519 53.0587 140.33 53.0587 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip632)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  232.382,876.922 243.517,876.875 254.652,876.824 265.787,876.766 276.922,876.703 288.058,876.633 299.193,876.556 310.328,876.471 321.463,876.377 332.598,876.272 \n",
       "  343.733,876.157 354.868,876.03 366.003,875.89 377.138,875.735 388.273,875.563 399.409,875.374 410.544,875.165 421.679,874.934 432.814,874.679 443.949,874.397 \n",
       "  455.084,874.086 466.219,873.742 477.354,873.363 488.489,872.944 499.625,872.481 510.76,871.971 521.895,871.407 533.03,870.786 544.165,870.1 555.3,869.343 \n",
       "  566.435,868.508 577.57,867.588 588.705,866.573 599.84,865.454 610.976,864.222 622.111,862.864 633.246,861.369 644.381,859.723 655.516,857.912 666.651,855.92 \n",
       "  677.786,853.73 688.921,851.325 700.056,848.683 711.192,845.785 722.327,842.606 733.462,839.124 744.597,835.311 755.732,831.142 766.867,826.586 778.002,821.614 \n",
       "  789.137,816.196 800.272,810.297 811.408,803.886 822.543,796.929 833.678,789.392 844.813,781.242 855.948,772.448 867.083,762.979 878.218,752.807 889.353,741.907 \n",
       "  900.488,730.259 911.623,717.848 922.759,704.664 933.894,690.705 945.029,675.977 956.164,660.492 967.299,644.276 978.434,627.362 989.569,609.793 1000.7,591.624 \n",
       "  1011.84,572.918 1022.97,553.748 1034.11,534.197 1045.24,514.352 1056.38,494.309 1067.51,474.166 1078.65,454.023 1089.79,433.98 1100.92,414.135 1112.06,394.584 \n",
       "  1123.19,375.414 1134.33,356.708 1145.46,338.539 1156.6,320.97 1167.73,304.055 1178.87,287.839 1190,272.355 1201.14,257.626 1212.27,243.667 1223.41,230.483 \n",
       "  1234.54,218.072 1245.68,206.425 1256.81,195.525 1267.95,185.353 1279.08,175.884 1290.22,167.089 1301.35,158.94 1312.49,151.403 1323.62,144.446 1334.76,138.035 \n",
       "  1345.89,132.136 1357.03,126.717 1368.16,121.746 1379.3,117.19 1390.43,113.021 1401.57,109.208 1412.7,105.726 1423.84,102.547 1434.97,99.6485 1446.11,97.0069 \n",
       "  1457.24,94.6012 1468.38,92.4117 1479.51,90.4199 1490.65,88.6089 1501.78,86.963 1512.92,85.4679 1524.05,84.1101 1535.19,82.8776 1546.32,81.759 1557.46,80.7441 \n",
       "  1568.59,79.8236 1579.73,78.9889 1590.86,78.2321 1602,77.5461 1613.14,76.9243 1624.27,76.3609 1635.41,75.8504 1646.54,75.3879 1657.68,74.969 1668.81,74.5895 \n",
       "  1679.95,74.2459 1691.08,73.9347 1702.22,73.6529 1713.35,73.3978 1724.49,73.1668 1735.62,72.9577 1746.76,72.7683 1757.89,72.5969 1769.03,72.4418 1780.16,72.3014 \n",
       "  1791.3,72.1743 1802.43,72.0592 1813.57,71.9551 1824.7,71.8609 1835.84,71.7756 1846.97,71.6984 1858.11,71.6285 1869.24,71.5653 1880.38,71.5081 1891.51,71.4563 \n",
       "  1902.65,71.4095 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip632)\" style=\"stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  232.382,876.922 243.517,876.876 254.652,876.824 265.787,876.767 276.922,876.704 288.058,876.634 299.193,876.557 310.328,876.472 321.463,876.378 332.598,876.274 \n",
       "  343.733,876.159 354.868,876.033 366.003,875.893 377.138,875.738 388.273,875.567 399.409,875.379 410.544,875.171 421.679,874.941 432.814,874.688 443.949,874.408 \n",
       "  455.084,874.099 466.219,873.758 477.354,873.383 488.489,872.968 499.625,872.511 510.76,872.007 521.895,871.451 533.03,870.839 544.165,870.165 555.3,869.423 \n",
       "  566.435,868.605 577.57,867.706 588.705,866.717 599.84,865.63 610.976,864.436 622.111,863.125 633.246,861.686 644.381,860.109 655.516,858.381 666.651,856.491 \n",
       "  677.786,854.423 688.921,852.166 700.056,849.704 711.192,847.022 722.327,844.105 733.462,840.937 744.597,837.505 755.732,833.791 766.867,829.784 778.002,825.469 \n",
       "  789.137,820.836 800.272,815.876 811.408,810.582 822.543,804.953 833.678,798.99 844.813,792.701 855.948,786.099 867.083,779.205 878.218,772.047 889.353,764.662 \n",
       "  900.488,757.096 911.623,749.404 922.759,741.651 933.894,733.913 945.029,726.272 956.164,718.819 967.299,711.652 978.434,704.87 989.569,698.578 1000.7,692.875 \n",
       "  1011.84,687.86 1022.97,683.621 1034.11,680.236 1045.24,677.77 1056.38,676.27 1067.51,675.767 1078.65,676.27 1089.79,677.77 1100.92,680.236 1112.06,683.621 \n",
       "  1123.19,687.86 1134.33,692.875 1145.46,698.578 1156.6,704.87 1167.73,711.652 1178.87,718.819 1190,726.272 1201.14,733.913 1212.27,741.651 1223.41,749.404 \n",
       "  1234.54,757.096 1245.68,764.662 1256.81,772.047 1267.95,779.205 1279.08,786.099 1290.22,792.701 1301.35,798.99 1312.49,804.953 1323.62,810.582 1334.76,815.876 \n",
       "  1345.89,820.836 1357.03,825.469 1368.16,829.784 1379.3,833.791 1390.43,837.505 1401.57,840.937 1412.7,844.105 1423.84,847.022 1434.97,849.704 1446.11,852.166 \n",
       "  1457.24,854.423 1468.38,856.491 1479.51,858.381 1490.65,860.109 1501.78,861.686 1512.92,863.125 1524.05,864.436 1535.19,865.63 1546.32,866.717 1557.46,867.706 \n",
       "  1568.59,868.605 1579.73,869.423 1590.86,870.165 1602,870.839 1613.14,871.451 1624.27,872.007 1635.41,872.511 1646.54,872.968 1657.68,873.383 1668.81,873.758 \n",
       "  1679.95,874.099 1691.08,874.408 1702.22,874.688 1713.35,874.941 1724.49,875.171 1735.62,875.379 1746.76,875.567 1757.89,875.738 1769.03,875.893 1780.16,876.033 \n",
       "  1791.3,876.159 1802.43,876.274 1813.57,876.378 1824.7,876.472 1835.84,876.557 1846.97,876.634 1858.11,876.704 1869.24,876.767 1880.38,876.824 1891.51,876.876 \n",
       "  1902.65,876.922 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip630)\" d=\"\n",
       "M241.29 231.226 L802.059 231.226 L802.059 75.7056 L241.29 75.7056  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip630)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  241.29,231.226 802.059,231.226 802.059,75.7056 241.29,75.7056 241.29,231.226 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip630)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  260.962,127.546 378.994,127.546 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip630)\" d=\"M417.092 119.664 L417.092 123.691 Q415.287 122.765 413.342 122.303 Q411.398 121.84 409.314 121.84 Q406.143 121.84 404.546 122.812 Q402.972 123.784 402.972 125.728 Q402.972 127.21 404.106 128.066 Q405.24 128.9 408.666 129.664 L410.125 129.988 Q414.662 130.96 416.56 132.742 Q418.481 134.502 418.481 137.673 Q418.481 141.284 415.611 143.39 Q412.763 145.497 407.763 145.497 Q405.68 145.497 403.412 145.08 Q401.166 144.687 398.666 143.876 L398.666 139.478 Q401.027 140.705 403.319 141.33 Q405.611 141.932 407.856 141.932 Q410.865 141.932 412.486 140.914 Q414.106 139.872 414.106 137.997 Q414.106 136.261 412.925 135.335 Q411.768 134.409 407.81 133.552 L406.328 133.205 Q402.37 132.372 400.611 130.659 Q398.851 128.923 398.851 125.914 Q398.851 122.256 401.444 120.266 Q404.037 118.275 408.805 118.275 Q411.166 118.275 413.249 118.622 Q415.333 118.969 417.092 119.664 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M425.263 118.9 L429.523 118.9 L429.523 144.826 L425.263 144.826 L425.263 118.9 M425.263 108.807 L429.523 108.807 L429.523 114.201 L425.263 114.201 L425.263 108.807 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M455.495 131.562 Q455.495 126.932 453.573 124.386 Q451.675 121.84 448.226 121.84 Q444.8 121.84 442.879 124.386 Q440.981 126.932 440.981 131.562 Q440.981 136.168 442.879 138.714 Q444.8 141.261 448.226 141.261 Q451.675 141.261 453.573 138.714 Q455.495 136.168 455.495 131.562 M459.754 141.608 Q459.754 148.228 456.814 151.446 Q453.874 154.687 447.809 154.687 Q445.564 154.687 443.573 154.339 Q441.583 154.015 439.708 153.321 L439.708 149.177 Q441.583 150.196 443.411 150.682 Q445.24 151.168 447.138 151.168 Q451.328 151.168 453.411 148.969 Q455.495 146.793 455.495 142.372 L455.495 140.265 Q454.175 142.557 452.115 143.691 Q450.055 144.826 447.184 144.826 Q442.416 144.826 439.499 141.191 Q436.583 137.557 436.583 131.562 Q436.583 125.543 439.499 121.909 Q442.416 118.275 447.184 118.275 Q450.055 118.275 452.115 119.409 Q454.175 120.543 455.495 122.835 L455.495 118.9 L459.754 118.9 L459.754 141.608 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M488.712 123.877 Q490.309 121.006 492.531 119.641 Q494.754 118.275 497.763 118.275 Q501.814 118.275 504.013 121.122 Q506.212 123.946 506.212 129.177 L506.212 144.826 L501.93 144.826 L501.93 129.316 Q501.93 125.59 500.61 123.784 Q499.291 121.978 496.582 121.978 Q493.272 121.978 491.351 124.178 Q489.43 126.377 489.43 130.173 L489.43 144.826 L485.147 144.826 L485.147 129.316 Q485.147 125.566 483.828 123.784 Q482.508 121.978 479.754 121.978 Q476.49 121.978 474.569 124.201 Q472.647 126.4 472.647 130.173 L472.647 144.826 L468.365 144.826 L468.365 118.9 L472.647 118.9 L472.647 122.928 Q474.106 120.543 476.143 119.409 Q478.18 118.275 480.981 118.275 Q483.805 118.275 485.772 119.71 Q487.763 121.145 488.712 123.877 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M524.754 121.886 Q521.328 121.886 519.337 124.571 Q517.346 127.233 517.346 131.886 Q517.346 136.539 519.314 139.224 Q521.304 141.886 524.754 141.886 Q528.156 141.886 530.147 139.201 Q532.138 136.515 532.138 131.886 Q532.138 127.279 530.147 124.594 Q528.156 121.886 524.754 121.886 M524.754 118.275 Q530.309 118.275 533.48 121.886 Q536.652 125.497 536.652 131.886 Q536.652 138.252 533.48 141.886 Q530.309 145.497 524.754 145.497 Q519.175 145.497 516.004 141.886 Q512.855 138.252 512.855 131.886 Q512.855 125.497 516.004 121.886 Q519.175 118.275 524.754 118.275 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M543.712 118.9 L547.971 118.9 L547.971 144.826 L543.712 144.826 L543.712 118.9 M543.712 108.807 L547.971 108.807 L547.971 114.201 L543.712 114.201 L543.712 108.807 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M573.943 122.835 L573.943 108.807 L578.202 108.807 L578.202 144.826 L573.943 144.826 L573.943 140.937 Q572.6 143.251 570.54 144.386 Q568.503 145.497 565.633 145.497 Q560.934 145.497 557.971 141.747 Q555.031 137.997 555.031 131.886 Q555.031 125.775 557.971 122.025 Q560.934 118.275 565.633 118.275 Q568.503 118.275 570.54 119.409 Q572.6 120.52 573.943 122.835 M559.429 131.886 Q559.429 136.585 561.351 139.27 Q563.295 141.932 566.675 141.932 Q570.054 141.932 571.999 139.27 Q573.943 136.585 573.943 131.886 Q573.943 127.187 571.999 124.525 Q570.054 121.84 566.675 121.84 Q563.295 121.84 561.351 124.525 Q559.429 127.187 559.429 131.886 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M597.207 108.854 Q594.105 114.178 592.6 119.386 Q591.096 124.594 591.096 129.941 Q591.096 135.289 592.6 140.543 Q594.128 145.775 597.207 151.076 L593.503 151.076 Q590.031 145.636 588.295 140.381 Q586.582 135.127 586.582 129.941 Q586.582 124.779 588.295 119.548 Q590.008 114.316 593.503 108.854 L597.207 108.854 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M627.021 118.9 L617.647 131.515 L627.508 144.826 L622.484 144.826 L614.938 134.64 L607.392 144.826 L602.369 144.826 L612.438 131.261 L603.225 118.9 L608.248 118.9 L615.123 128.136 L621.998 118.9 L627.021 118.9 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M632.855 108.854 L636.558 108.854 Q640.031 114.316 641.744 119.548 Q643.48 124.779 643.48 129.941 Q643.48 135.127 641.744 140.381 Q640.031 145.636 636.558 151.076 L632.855 151.076 Q635.933 145.775 637.438 140.543 Q638.966 135.289 638.966 129.941 Q638.966 124.594 637.438 119.386 Q635.933 114.178 632.855 108.854 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip630)\" style=\"stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  260.962,179.386 378.994,179.386 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip630)\" d=\"M417.578 183.402 Q417.578 178.772 415.657 176.226 Q413.759 173.68 410.31 173.68 Q406.884 173.68 404.962 176.226 Q403.064 178.772 403.064 183.402 Q403.064 188.008 404.962 190.554 Q406.884 193.101 410.31 193.101 Q413.759 193.101 415.657 190.554 Q417.578 188.008 417.578 183.402 M421.837 193.448 Q421.837 200.068 418.898 203.286 Q415.958 206.527 409.893 206.527 Q407.648 206.527 405.657 206.179 Q403.666 205.855 401.791 205.161 L401.791 201.017 Q403.666 202.036 405.495 202.522 Q407.324 203.008 409.222 203.008 Q413.412 203.008 415.495 200.809 Q417.578 198.633 417.578 194.212 L417.578 192.105 Q416.259 194.397 414.199 195.531 Q412.138 196.666 409.268 196.666 Q404.5 196.666 401.583 193.031 Q398.666 189.397 398.666 183.402 Q398.666 177.383 401.583 173.749 Q404.5 170.115 409.268 170.115 Q412.138 170.115 414.199 171.249 Q416.259 172.383 417.578 174.675 L417.578 170.74 L421.837 170.74 L421.837 193.448 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M445.634 174.721 Q444.916 174.305 444.06 174.119 Q443.226 173.911 442.208 173.911 Q438.597 173.911 436.652 176.272 Q434.731 178.61 434.731 183.008 L434.731 196.666 L430.448 196.666 L430.448 170.74 L434.731 170.74 L434.731 174.768 Q436.073 172.406 438.226 171.272 Q440.379 170.115 443.458 170.115 Q443.897 170.115 444.43 170.184 Q444.962 170.231 445.61 170.346 L445.634 174.721 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M461.883 183.633 Q456.721 183.633 454.731 184.814 Q452.74 185.994 452.74 188.842 Q452.74 191.11 454.221 192.453 Q455.726 193.772 458.296 193.772 Q461.837 193.772 463.967 191.272 Q466.12 188.749 466.12 184.582 L466.12 183.633 L461.883 183.633 M470.379 181.874 L470.379 196.666 L466.12 196.666 L466.12 192.73 Q464.661 195.091 462.485 196.226 Q460.309 197.337 457.161 197.337 Q453.18 197.337 450.819 195.115 Q448.481 192.869 448.481 189.119 Q448.481 184.744 451.397 182.522 Q454.337 180.3 460.147 180.3 L466.12 180.3 L466.12 179.883 Q466.12 176.943 464.175 175.346 Q462.254 173.726 458.759 173.726 Q456.536 173.726 454.43 174.258 Q452.323 174.791 450.379 175.855 L450.379 171.92 Q452.717 171.018 454.916 170.578 Q457.115 170.115 459.198 170.115 Q464.823 170.115 467.601 173.031 Q470.379 175.948 470.379 181.874 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M496.212 174.675 L496.212 160.647 L500.471 160.647 L500.471 196.666 L496.212 196.666 L496.212 192.777 Q494.869 195.091 492.809 196.226 Q490.772 197.337 487.902 197.337 Q483.203 197.337 480.24 193.587 Q477.3 189.837 477.3 183.726 Q477.3 177.615 480.24 173.865 Q483.203 170.115 487.902 170.115 Q490.772 170.115 492.809 171.249 Q494.869 172.36 496.212 174.675 M481.698 183.726 Q481.698 188.425 483.619 191.11 Q485.564 193.772 488.944 193.772 Q492.323 193.772 494.268 191.11 Q496.212 188.425 496.212 183.726 Q496.212 179.027 494.268 176.365 Q492.323 173.68 488.944 173.68 Q485.564 173.68 483.619 176.365 Q481.698 179.027 481.698 183.726 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M534.36 173.726 Q530.934 173.726 528.943 176.411 Q526.953 179.073 526.953 183.726 Q526.953 188.379 528.92 191.064 Q530.911 193.726 534.36 193.726 Q537.763 193.726 539.753 191.041 Q541.744 188.355 541.744 183.726 Q541.744 179.119 539.753 176.434 Q537.763 173.726 534.36 173.726 M534.36 170.115 Q539.915 170.115 543.087 173.726 Q546.258 177.337 546.258 183.726 Q546.258 190.092 543.087 193.726 Q539.915 197.337 534.36 197.337 Q528.781 197.337 525.61 193.726 Q522.462 190.092 522.462 183.726 Q522.462 177.337 525.61 173.726 Q528.781 170.115 534.36 170.115 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M566.443 160.647 L566.443 164.189 L562.369 164.189 Q560.077 164.189 559.175 165.115 Q558.295 166.041 558.295 168.448 L558.295 170.74 L565.309 170.74 L565.309 174.05 L558.295 174.05 L558.295 196.666 L554.013 196.666 L554.013 174.05 L549.939 174.05 L549.939 170.74 L554.013 170.74 L554.013 168.934 Q554.013 164.606 556.027 162.638 Q558.04 160.647 562.415 160.647 L566.443 160.647 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M601.605 171.504 L601.605 175.531 Q599.799 174.605 597.855 174.143 Q595.911 173.68 593.827 173.68 Q590.656 173.68 589.059 174.652 Q587.485 175.624 587.485 177.568 Q587.485 179.05 588.619 179.906 Q589.753 180.74 593.179 181.504 L594.637 181.828 Q599.174 182.8 601.073 184.582 Q602.994 186.342 602.994 189.513 Q602.994 193.124 600.123 195.23 Q597.276 197.337 592.276 197.337 Q590.193 197.337 587.924 196.92 Q585.679 196.527 583.179 195.716 L583.179 191.318 Q585.54 192.545 587.832 193.17 Q590.124 193.772 592.369 193.772 Q595.378 193.772 596.999 192.754 Q598.619 191.712 598.619 189.837 Q598.619 188.101 597.438 187.175 Q596.281 186.249 592.323 185.392 L590.841 185.045 Q586.883 184.212 585.124 182.499 Q583.364 180.763 583.364 177.754 Q583.364 174.096 585.957 172.106 Q588.549 170.115 593.318 170.115 Q595.679 170.115 597.762 170.462 Q599.846 170.809 601.605 171.504 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M609.776 170.74 L614.035 170.74 L614.035 196.666 L609.776 196.666 L609.776 170.74 M609.776 160.647 L614.035 160.647 L614.035 166.041 L609.776 166.041 L609.776 160.647 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M640.008 183.402 Q640.008 178.772 638.086 176.226 Q636.188 173.68 632.739 173.68 Q629.313 173.68 627.392 176.226 Q625.494 178.772 625.494 183.402 Q625.494 188.008 627.392 190.554 Q629.313 193.101 632.739 193.101 Q636.188 193.101 638.086 190.554 Q640.008 188.008 640.008 183.402 M644.267 193.448 Q644.267 200.068 641.327 203.286 Q638.387 206.527 632.322 206.527 Q630.077 206.527 628.086 206.179 Q626.096 205.855 624.221 205.161 L624.221 201.017 Q626.096 202.036 627.924 202.522 Q629.753 203.008 631.651 203.008 Q635.841 203.008 637.924 200.809 Q640.008 198.633 640.008 194.212 L640.008 192.105 Q638.688 194.397 636.628 195.531 Q634.568 196.666 631.697 196.666 Q626.929 196.666 624.012 193.031 Q621.096 189.397 621.096 183.402 Q621.096 177.383 624.012 173.749 Q626.929 170.115 631.697 170.115 Q634.568 170.115 636.628 171.249 Q638.688 172.383 640.008 174.675 L640.008 170.74 L644.267 170.74 L644.267 193.448 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M673.225 175.717 Q674.822 172.846 677.044 171.481 Q679.267 170.115 682.276 170.115 Q686.327 170.115 688.526 172.962 Q690.725 175.786 690.725 181.017 L690.725 196.666 L686.442 196.666 L686.442 181.156 Q686.442 177.43 685.123 175.624 Q683.804 173.818 681.095 173.818 Q677.785 173.818 675.864 176.018 Q673.943 178.217 673.943 182.013 L673.943 196.666 L669.66 196.666 L669.66 181.156 Q669.66 177.406 668.341 175.624 Q667.021 173.818 664.267 173.818 Q661.003 173.818 659.082 176.041 Q657.16 178.24 657.16 182.013 L657.16 196.666 L652.878 196.666 L652.878 170.74 L657.16 170.74 L657.16 174.768 Q658.619 172.383 660.656 171.249 Q662.693 170.115 665.494 170.115 Q668.318 170.115 670.285 171.55 Q672.276 172.985 673.225 175.717 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M709.266 173.726 Q705.841 173.726 703.85 176.411 Q701.859 179.073 701.859 183.726 Q701.859 188.379 703.827 191.064 Q705.817 193.726 709.266 193.726 Q712.669 193.726 714.66 191.041 Q716.651 188.355 716.651 183.726 Q716.651 179.119 714.66 176.434 Q712.669 173.726 709.266 173.726 M709.266 170.115 Q714.822 170.115 717.993 173.726 Q721.165 177.337 721.165 183.726 Q721.165 190.092 717.993 193.726 Q714.822 197.337 709.266 197.337 Q703.688 197.337 700.516 193.726 Q697.368 190.092 697.368 183.726 Q697.368 177.337 700.516 173.726 Q703.688 170.115 709.266 170.115 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M728.225 170.74 L732.484 170.74 L732.484 196.666 L728.225 196.666 L728.225 170.74 M728.225 160.647 L732.484 160.647 L732.484 166.041 L728.225 166.041 L728.225 160.647 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip630)\" d=\"M758.456 174.675 L758.456 160.647 L762.715 160.647 L762.715 196.666 L758.456 196.666 L758.456 192.777 Q757.113 195.091 755.053 196.226 Q753.016 197.337 750.146 197.337 Q745.447 197.337 742.484 193.587 Q739.544 189.837 739.544 183.726 Q739.544 177.615 742.484 173.865 Q745.447 170.115 750.146 170.115 Q753.016 170.115 755.053 171.249 Q757.113 172.36 758.456 174.675 M743.942 183.726 Q743.942 188.425 745.863 191.11 Q747.808 193.772 751.187 193.772 Q754.567 193.772 756.512 191.11 Q758.456 188.425 758.456 183.726 Q758.456 179.027 756.512 176.365 Q754.567 173.68 751.187 173.68 Q747.808 173.68 745.863 176.365 Q743.942 179.027 743.942 183.726 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = range(-7.5, 7.5, step=0.1)\n",
    "y = Flux.sigmoid.(x)\n",
    "grad = Zygote.gradient(x -> sum(Flux.sigmoid.(x)), x)\n",
    "\n",
    "p = Plots.plot(x, y, label=\"sigmoid(x)\", legend=:topleft, size=(500, 250))\n",
    "Plots.plot!(x, grad, label=\"grad of sigmoid\", legend=:topleft, size=(500, 250))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fcc292",
   "metadata": {},
   "source": [
    "As you can see, (**the sigmoid's gradient vanishes\n",
    "both when its inputs are large and when they are small**).\n",
    "Moreover, when backpropagating through many layers,\n",
    "unless we are in the Goldilocks zone, where\n",
    "the inputs to many of the sigmoids are close to zero,\n",
    "the gradients of the overall product may vanish.\n",
    "When our network boasts many layers,\n",
    "unless we are careful, the gradient\n",
    "will likely be cut off at some layer.\n",
    "Indeed, this problem used to plague deep network training.\n",
    "Consequently, ReLUs, which are more stable\n",
    "(but less neurally plausible),\n",
    "have emerged as the default choice for practitioners.\n",
    "\n",
    "\n",
    "### [**Exploding Gradients**]\n",
    "\n",
    "The opposite problem, when gradients explode,\n",
    "can be similarly vexing.\n",
    "To illustrate this a bit better,\n",
    "we draw 100 Gaussian random matrices\n",
    "and multiply them with some initial matrix.\n",
    "For the scale that we picked\n",
    "(the choice of the variance $\\sigma^2=1$),\n",
    "the matrix product explodes.\n",
    "When this happens due to the initialization\n",
    "of a deep network, we have no chance of getting\n",
    "a gradient descent optimizer to converge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "239966de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a single matrix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "44 Matrix{Float64}:\n",
       " -0.0427301  -2.09471   -1.82985     2.19672\n",
       "  0.540102    0.51016    0.0233455  -0.251296\n",
       " -0.500163    0.939827  -0.819848   -0.713231\n",
       " -1.54253    -0.975876   0.150157   -0.512266"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after multiplying 100 matrices\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "44 Matrix{Float64}:\n",
       "  2.13151e25   3.62394e25  -1.5671e25    2.33683e25\n",
       " -1.03617e24  -1.76166e24   7.61797e23  -1.13597e24\n",
       " -1.84137e24  -3.13064e24   1.35379e24  -2.01874e24\n",
       " -4.61792e24  -7.85126e24   3.39513e24  -5.06273e24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "M = randn(4, 4)\n",
    "println(\"a single matrix\")\n",
    "display(M)\n",
    "\n",
    "for i in 1:100\n",
    "    M = M * randn(4, 4)\n",
    "end\n",
    "\n",
    "println(\"after multiplying 100 matrices\")\n",
    "display(M)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be554f1",
   "metadata": {},
   "source": [
    "### Breaking the Symmetry\n",
    "\n",
    "Another problem in neural network design\n",
    "is the symmetry inherent in their parametrization.\n",
    "Assume that we have a simple MLP\n",
    "with one hidden layer and two units.\n",
    "In this case, we could permute the weights $\\mathbf{W}^{(1)}$\n",
    "of the first layer and likewise permute\n",
    "the weights of the output layer\n",
    "to obtain the same function.\n",
    "There is nothing special differentiating\n",
    "the first hidden unit vs. the second hidden unit.\n",
    "In other words, we have permutation symmetry\n",
    "among the hidden units of each layer.\n",
    "\n",
    "This is more than just a theoretical nuisance.\n",
    "Consider the aforementioned one-hidden-layer MLP\n",
    "with two hidden units.\n",
    "For illustration,\n",
    "suppose that the output layer transforms the two hidden units into only one output unit.\n",
    "Imagine what would happen if we initialized\n",
    "all of the parameters of the hidden layer\n",
    "as $\\mathbf{W}^{(1)} = c$ for some constant $c$.\n",
    "In this case, during forward propagation\n",
    "either hidden unit takes the same inputs and parameters,\n",
    "producing the same activation,\n",
    "which is fed to the output unit.\n",
    "During backpropagation,\n",
    "differentiating the output unit with respect to parameters $\\mathbf{W}^{(1)}$ gives a gradient whose elements all take the same value.\n",
    "Thus, after gradient-based iteration (e.g., minibatch stochastic gradient descent),\n",
    "all the elements of $\\mathbf{W}^{(1)}$ still take the same value.\n",
    "Such iterations would\n",
    "never *break the symmetry* on its own\n",
    "and we might never be able to realize\n",
    "the network's expressive power.\n",
    "The hidden layer would behave\n",
    "as if it had only a single unit.\n",
    "Note that while minibatch stochastic gradient descent would not break this symmetry,\n",
    "dropout regularization (to be introduced later) would!\n",
    "\n",
    "\n",
    "## Parameter Initialization\n",
    "\n",
    "One way of addressing---or at least mitigating---the\n",
    "issues raised above is through careful initialization.\n",
    "As we will see later,\n",
    "additional care during optimization\n",
    "and suitable regularization can further enhance stability.\n",
    "\n",
    "\n",
    "### Default Initialization\n",
    "\n",
    "In the previous sections, e.g., in :numref:`sec_linear_concise`,\n",
    "we used a normal distribution\n",
    "to initialize the values of our weights.\n",
    "If we do not specify the initialization method, the framework will\n",
    "use a default random initialization method, which often works well in practice\n",
    "for moderate problem sizes.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Xavier Initialization\n",
    ":label:`subsec_xavier`\n",
    "\n",
    "Let's look at the scale distribution of\n",
    "an output $o_{i}$ for some fully connected layer\n",
    "*without nonlinearities*.\n",
    "With $n_\\mathrm{in}$ inputs $x_j$\n",
    "and their associated weights $w_{ij}$ for this layer,\n",
    "an output is given by\n",
    "\n",
    "$$o_{i} = \\sum_{j=1}^{n_\\mathrm{in}} w_{ij} x_j.$$\n",
    "\n",
    "The weights $w_{ij}$ are all drawn\n",
    "independently from the same distribution.\n",
    "Furthermore, let's assume that this distribution\n",
    "has zero mean and variance $\\sigma^2$.\n",
    "Note that this does not mean that the distribution has to be Gaussian,\n",
    "just that the mean and variance need to exist.\n",
    "For now, let's assume that the inputs to the layer $x_j$\n",
    "also have zero mean and variance $\\gamma^2$\n",
    "and that they are independent of $w_{ij}$ and independent of each other.\n",
    "In this case, we can compute the mean and variance of $o_i$ as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    E[o_i] & = \\sum_{j=1}^{n_\\mathrm{in}} E[w_{ij} x_j] \\\\&= \\sum_{j=1}^{n_\\mathrm{in}} E[w_{ij}] E[x_j] \\\\&= 0, \\\\\n",
    "    \\mathrm{Var}[o_i] & = E[o_i^2] - (E[o_i])^2 \\\\\n",
    "        & = \\sum_{j=1}^{n_\\mathrm{in}} E[w^2_{ij} x^2_j] - 0 \\\\\n",
    "        & = \\sum_{j=1}^{n_\\mathrm{in}} E[w^2_{ij}] E[x^2_j] \\\\\n",
    "        & = n_\\mathrm{in} \\sigma^2 \\gamma^2.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "One way to keep the variance fixed\n",
    "is to set $n_\\mathrm{in} \\sigma^2 = 1$.\n",
    "Now consider backpropagation.\n",
    "There we face a similar problem,\n",
    "albeit with gradients being propagated from the layers closer to the output.\n",
    "Using the same reasoning as for forward propagation,\n",
    "we see that the gradients' variance can blow up\n",
    "unless $n_\\mathrm{out} \\sigma^2 = 1$,\n",
    "where $n_\\mathrm{out}$ is the number of outputs of this layer.\n",
    "This leaves us in a dilemma:\n",
    "we cannot possibly satisfy both conditions simultaneously.\n",
    "Instead, we simply try to satisfy:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{1}{2} (n_\\mathrm{in} + n_\\mathrm{out}) \\sigma^2 = 1 \\text{ or equivalently }\n",
    "\\sigma = \\sqrt{\\frac{2}{n_\\mathrm{in} + n_\\mathrm{out}}}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This is the reasoning underlying the now-standard\n",
    "and practically beneficial *Xavier initialization*,\n",
    "named after the first author of its creators :cite:`Glorot.Bengio.2010`.\n",
    "Typically, the Xavier initialization\n",
    "samples weights from a Gaussian distribution\n",
    "with zero mean and variance\n",
    "$\\sigma^2 = \\frac{2}{n_\\mathrm{in} + n_\\mathrm{out}}$.\n",
    "We can also adapt this to\n",
    "choose the variance when sampling weights\n",
    "from a uniform distribution.\n",
    "Note that the uniform distribution $U(-a, a)$ has variance $\\frac{a^2}{3}$.\n",
    "Plugging $\\frac{a^2}{3}$ into our condition on $\\sigma^2$\n",
    "yields the suggestion to initialize according to\n",
    "\n",
    "$$U\\left(-\\sqrt{\\frac{6}{n_\\mathrm{in} + n_\\mathrm{out}}}, \\sqrt{\\frac{6}{n_\\mathrm{in} + n_\\mathrm{out}}}\\right).$$\n",
    "\n",
    "Though the assumption for nonexistence of nonlinearities\n",
    "in the above mathematical reasoning\n",
    "can be easily violated in neural networks,\n",
    "the Xavier initialization method\n",
    "turns out to work well in practice.\n",
    "\n",
    "\n",
    "### Beyond\n",
    "\n",
    "The reasoning above barely scratches the surface\n",
    "of modern approaches to parameter initialization.\n",
    "A deep learning framework often implements over a dozen different heuristics.\n",
    "Moreover, parameter initialization continues to be\n",
    "a hot area of fundamental research in deep learning.\n",
    "Among these are heuristics specialized for\n",
    "tied (shared) parameters, super-resolution,\n",
    "sequence models, and other situations.\n",
    "For instance,\n",
    ":citet:`Xiao.Bahri.Sohl-Dickstein.ea.2018` demonstrated the possibility of training\n",
    "10000-layer neural networks without architectural tricks\n",
    "by using a carefully-designed initialization method.\n",
    "\n",
    "If the topic interests you we suggest\n",
    "a deep dive into this module's offerings,\n",
    "reading the papers that proposed and analyzed each heuristic,\n",
    "and then exploring the latest publications on the topic.\n",
    "Perhaps you will stumble across or even invent\n",
    "a clever idea and contribute an implementation to deep learning frameworks.\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "Vanishing and exploding gradients are common issues in deep networks. Great care in parameter initialization is required to ensure that gradients and parameters remain well controlled.\n",
    "Initialization heuristics are needed to ensure that the initial gradients are neither too large nor too small.\n",
    "Random initialization is key to ensure that symmetry is broken before optimization.\n",
    "Xavier initialization suggests that, for each layer, variance of any output is not affected by the number of inputs, and variance of any gradient is not affected by the number of outputs.\n",
    "ReLU activation functions mitigate the vanishing gradient problem. This can accelerate convergence.\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Can you design other cases where a neural network might exhibit symmetry requiring breaking besides the permutation symmetry in an MLP's layers?\n",
    "1. Can we initialize all weight parameters in linear regression or in softmax regression to the same value?\n",
    "1. Look up analytic bounds on the eigenvalues of the product of two matrices. What does this tell you about ensuring that gradients are well conditioned?\n",
    "1. If we know that some terms diverge, can we fix this after the fact? Look at the paper on layerwise adaptive rate scaling  for inspiration :cite:`You.Gitman.Ginsburg.2017`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4042ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ffca4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6009c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a71392",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad119388",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
